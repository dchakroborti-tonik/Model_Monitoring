{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "180e57d0-a0a4-407a-af36-27664e0e8428",
   "metadata": {},
   "source": [
    "- In this Notebook, I am trying to integrate the training data from the backscore and rest of the period from prj-prod-dataplatform.audit_balance.ml_model_run_details table.\n",
    "- In this I will compare the training period with each month of test period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53afdaae-de4e-43b5-a407-93c959ed76ca",
   "metadata": {},
   "source": [
    "**Steps to Follow**:\n",
    "\n",
    "* Read the specific model data from prj-prod-dataplatform.audit_balance.ml_model_run_details table\n",
    "* Expand the calcFeature column to extract all the features for the model\n",
    "* Read the data from specific backscore table for the training data\n",
    "* Identify the features and create a list\n",
    "* Use transform_data function to create the same structure as ml_model_run_details table\n",
    "* Insert the data to a similar training table - prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "* Read the specific model data from prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "* expand the training set from the calcFeature column\n",
    "* Concatenate both the test and train datasets\n",
    "* Calculate the PSI using the PSI function comparing it with the train set\n",
    "* Insert the result to a PSI table prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cic_sil_model_psi_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "250a297c-02ae-4436-adde-fe781991f45c",
   "metadata": {},
   "source": [
    "# **PSI - CSI Calculation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e35efe5-2336-4f40-b428-e33c5f7e7987",
   "metadata": {},
   "source": [
    "## Define Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c89aead0-01af-4cd1-a347-a0dda0dccd9a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "import gcsfs\n",
    "import duckdb as dd\n",
    "import pickle\n",
    "import joblib\n",
    "from typing import Union\n",
    "import io\n",
    "path = r'C:\\Users\\Dwaipayan\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"prj-prod-dataplatform\"\n",
    "\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"Display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc238785-761c-4442-9b4e-6bc45bcf1047",
   "metadata": {},
   "source": [
    "### Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100886af-a612-44e9-870c-dc65cea82004",
   "metadata": {
    "id": "KYE0bNRQylvI"
   },
   "source": [
    "#### expand_calc_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "23c752fb-6255-4a45-a488-a495dec4daa4",
   "metadata": {
    "id": "ZZAarPnPyVnu",
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def expand_calc_features(df):\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns and return the complete DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with calcFeatures column containing JSON data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Expanded DataFrame with all original columns plus JSON features as separate columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_expanded = df.copy()\n",
    "\n",
    "    # Parse the calcFeatures JSON column\n",
    "    calc_features_list = []\n",
    "\n",
    "    for idx, calc_features_str in enumerate(df['calcFeatures']):\n",
    "        try:\n",
    "            # Parse the JSON string\n",
    "            features_dict = json.loads(calc_features_str.replace(\"'\", '\"'))  # Replace single quotes with double quotes for valid JSON\n",
    "            calc_features_list.append(features_dict)\n",
    "        except (json.JSONDecodeError, AttributeError) as e:\n",
    "            # If parsing fails, create an empty dict and print warning\n",
    "            print(f\"Warning: Could not parse calcFeatures at index {idx}: {e}\")\n",
    "            calc_features_list.append({})\n",
    "\n",
    "    # Create DataFrame from the parsed JSON data\n",
    "    calc_features_df = pd.DataFrame(calc_features_list)\n",
    "\n",
    "    # Add prefix to JSON-derived columns to avoid conflicts\n",
    "    calc_features_df = calc_features_df.add_prefix('calc_')\n",
    "\n",
    "    # Reset index to ensure proper alignment\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    calc_features_df = calc_features_df.reset_index(drop=True)\n",
    "\n",
    "    # Combine original DataFrame with expanded calcFeatures\n",
    "    result_df = pd.concat([df_expanded, calc_features_df], axis=1)\n",
    "\n",
    "    return result_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e028cc-a315-47fc-8510-01774c81d9f7",
   "metadata": {
    "id": "2OKhEBCoysON"
   },
   "source": [
    "#### expand_calc_features_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "386b1233-fe21-4364-bd2a-7398565fdc50",
   "metadata": {
    "id": "6q23h135yqAP",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "\n",
    "# def expand_calc_features_robust(df):\n",
    "#     \"\"\"\n",
    "#     Expand the calcFeatures JSON column into separate columns with better error handling.\n",
    "\n",
    "#     Parameters:\n",
    "#     df (pd.DataFrame): Input DataFrame with calcFeatures column containing JSON data\n",
    "\n",
    "#     Returns:\n",
    "#     pd.DataFrame: Expanded DataFrame with all original columns plus JSON features as separate columns\n",
    "#     \"\"\"\n",
    "\n",
    "#     # Make a copy to avoid modifying the original DataFrame\n",
    "#     df_expanded = df.copy()\n",
    "\n",
    "#     # Parse the calcFeatures JSON column\n",
    "#     calc_features_data = []\n",
    "\n",
    "#     for idx, row in df.iterrows():\n",
    "#         calc_features_str = row['calcFeatures']\n",
    "\n",
    "#         if pd.isna(calc_features_str) or calc_features_str == '':\n",
    "#             calc_features_data.append({})\n",
    "#             continue\n",
    "\n",
    "#         try:\n",
    "#             # Clean the string and parse JSON\n",
    "#             cleaned_str = calc_features_str.replace(\"'\", '\"').replace('None', 'null').replace('True', 'true').replace('False', 'false')\n",
    "#             features_dict = json.loads(cleaned_str)\n",
    "#             calc_features_data.append(features_dict)\n",
    "#         except Exception as e:\n",
    "#             print(f\"Warning: Could not parse calcFeatures at index {idx}: {e}\")\n",
    "#             print(f\"Problematic string: {calc_features_str[:100]}...\")  # Print first 100 chars\n",
    "#             calc_features_data.append({})\n",
    "\n",
    "#     # Create DataFrame from the parsed JSON data\n",
    "#     calc_features_df = pd.DataFrame(calc_features_data)\n",
    "\n",
    "#     # Add prefix to JSON-derived columns to avoid conflicts with existing columns\n",
    "#     calc_features_df = calc_features_df.add_prefix('feat_')\n",
    "\n",
    "#     # Combine DataFrames\n",
    "#     result_df = pd.concat([df_expanded, calc_features_df], axis=1)\n",
    "\n",
    "#     print(f\"Original DataFrame shape: {df.shape}\")\n",
    "#     print(f\"Expanded DataFrame shape: {result_df.shape}\")\n",
    "#     print(f\"Added {len(calc_features_df.columns)} new columns from calcFeatures\")\n",
    "\n",
    "#     return result_df\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def expand_calc_features_robust(df):\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns and return the complete DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with calcFeatures column containing JSON data\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: Expanded DataFrame with all original columns plus JSON features as separate columns\n",
    "    \"\"\"\n",
    "    \n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_expanded = df.copy()\n",
    "    \n",
    "    # Parse the calcFeatures JSON column\n",
    "    calc_features_data = []\n",
    "    \n",
    "    for idx, calc_features_str in enumerate(df['calcFeatures']):\n",
    "        try:\n",
    "            # Handle None or NaN values\n",
    "            if pd.isna(calc_features_str):\n",
    "                calc_features_data.append({})\n",
    "                continue\n",
    "            \n",
    "            # Convert to string if not already\n",
    "            calc_features_str = str(calc_features_str)\n",
    "            \n",
    "            # Parse the JSON string\n",
    "            features_dict = json.loads(calc_features_str.replace(\"'\", '\"'))\n",
    "            \n",
    "            # Ensure it's a dictionary, not a list\n",
    "            if isinstance(features_dict, dict):\n",
    "                calc_features_data.append(features_dict)\n",
    "            elif isinstance(features_dict, list):\n",
    "                # If it's a list, convert to dict with index keys or skip\n",
    "                print(f\"Warning: calcFeatures at index {idx} is a list, converting to dict\")\n",
    "                calc_features_data.append({'raw_list': features_dict})\n",
    "            else:\n",
    "                print(f\"Warning: calcFeatures at index {idx} is neither dict nor list: {type(features_dict)}\")\n",
    "                calc_features_data.append({})\n",
    "                \n",
    "        except (json.JSONDecodeError, AttributeError, TypeError) as e:\n",
    "            # If parsing fails, create an empty dict and print warning\n",
    "            print(f\"Warning: Could not parse calcFeatures at index {idx}: {e}\")\n",
    "            print(f\"  Value: {calc_features_str}\")\n",
    "            calc_features_data.append({})\n",
    "    \n",
    "    # Create DataFrame from the parsed JSON data\n",
    "    calc_features_df = pd.DataFrame(calc_features_data)\n",
    "    \n",
    "    # Add prefix to JSON-derived columns to avoid conflicts\n",
    "    calc_features_df = calc_features_df.add_prefix('calc_')\n",
    "    \n",
    "    # Reset index to ensure proper alignment\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    calc_features_df = calc_features_df.reset_index(drop=True)\n",
    "    \n",
    "    # Combine original DataFrame with expanded calcFeatures\n",
    "    result_df = pd.concat([df_expanded, calc_features_df], axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d4dbf52",
   "metadata": {},
   "source": [
    "# expand_calc_features_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dbffced9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "\n",
    "def expand_calc_features_fixed(df):\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns and return the complete DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with calcFeatures column containing JSON data\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Expanded DataFrame with all original columns plus JSON features as separate columns\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy to avoid modifying the original DataFrame\n",
    "    df_expanded = df.copy()\n",
    "\n",
    "    # Parse the calcFeatures JSON column\n",
    "    calc_features_list = []\n",
    "\n",
    "    for idx, calc_features_str in enumerate(df['calcFeatures']):\n",
    "        try:\n",
    "            # Handle None/null values\n",
    "            if pd.isna(calc_features_str) or calc_features_str is None:\n",
    "                calc_features_list.append({})\n",
    "                continue\n",
    "                \n",
    "            # Parse the JSON string\n",
    "            features_data = json.loads(calc_features_str.replace(\"'\", '\"'))  # Replace single quotes with double quotes for valid JSON\n",
    "            \n",
    "            # Handle both list and dictionary formats\n",
    "            if isinstance(features_data, list):\n",
    "                # If it's a list, take the first element (assuming list of one dictionary)\n",
    "                if len(features_data) > 0:\n",
    "                    calc_features_list.append(features_data[0])\n",
    "                else:\n",
    "                    calc_features_list.append({})\n",
    "            elif isinstance(features_data, dict):\n",
    "                calc_features_list.append(features_data)\n",
    "            else:\n",
    "                print(f\"Warning: Unexpected data type at index {idx}: {type(features_data)}\")\n",
    "                calc_features_list.append({})\n",
    "                \n",
    "        except (json.JSONDecodeError, AttributeError, TypeError) as e:\n",
    "            # If parsing fails, create an empty dict and print warning\n",
    "            print(f\"Warning: Could not parse calcFeatures at index {idx}: {e}\")\n",
    "            calc_features_list.append({})\n",
    "\n",
    "    # Create DataFrame from the parsed JSON data\n",
    "    calc_features_df = pd.DataFrame(calc_features_list)\n",
    "\n",
    "    # Add prefix to JSON-derived columns to avoid conflicts\n",
    "    calc_features_df = calc_features_df.add_prefix('calc_')\n",
    "\n",
    "    # Reset index to ensure proper alignment\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    calc_features_df = calc_features_df.reset_index(drop=True)\n",
    "\n",
    "    # Combine original DataFrame with expanded calcFeatures\n",
    "    result_df = pd.concat([df_expanded, calc_features_df], axis=1)\n",
    "\n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583c72b5-862d-455e-b015-13e3085c9bd8",
   "metadata": {},
   "source": [
    "#### transform_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6f06a7f2-d7b8-4859-9c81-49964583c4c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# import uuid\n",
    "# from datetime import datetime\n",
    "# from typing import List\n",
    "\n",
    "# def transform_data(d1: pd.DataFrame, feature_column: List[str], a='demo_score', modelDisplayName = 'Cash_beta_trench1_Demo_backscore', subscription_name = 'sil_march 25 models'):\n",
    "#     # Read the input CSV file\n",
    "#     df = d1.copy()\n",
    "    \n",
    "#     # Create the output DataFrame with the required structure\n",
    "#     output_data = []\n",
    "    \n",
    "#     for _, row in df.iterrows():\n",
    "#         # Create the calcFeature JSON with all the feature columns\n",
    "#         feature_columns = feature_column\n",
    "        \n",
    "#         calc_feature = {}\n",
    "#         for col in feature_columns:\n",
    "#             if col in row and pd.notna(row[col]):\n",
    "#                 # Convert Timestamp objects to string\n",
    "#                 if isinstance(row[col], pd.Timestamp):\n",
    "#                     calc_feature[col] = row[col].isoformat()\n",
    "#                 else:\n",
    "#                     calc_feature[col] = row[col]\n",
    "        \n",
    "       \n",
    "#         # Get current timestamp\n",
    "#         current_time = datetime.now().isoformat()\n",
    "        \n",
    "#         # Create the output row\n",
    "#         output_row = {\n",
    "#             \"customerId\": row['customer_id'],\n",
    "#             \"digitalLoanAccountId\": row['digitalLoanAccountId'],\n",
    "#             \"crifApplicationId\": str(uuid.uuid4()),  # Generate random UUID\n",
    "#             \"prediction\": row.get(a, 0),\n",
    "#             \"start_time\": current_time,\n",
    "#             \"end_time\": current_time,\n",
    "#             \"modelDisplayName\":modelDisplayName,\n",
    "#             \"modelVersionId\":\"v1\",\n",
    "#             \"subscription_name\": subscription_name,\n",
    "#             \"message_id\": str(uuid.uuid4()),  # Generate random UUID\n",
    "#             \"publish_time\": current_time,\n",
    "#             \"attributes\": \"{}\",  # Empty JSON object\n",
    "#             \"calcFeature\": json.dumps(calc_feature, default=str)  # Use default=str to handle non-serializable objects\n",
    "            \n",
    "#         }\n",
    "        \n",
    "#         output_data.append(output_row)\n",
    "    \n",
    "#     # Create DataFrame from the output data\n",
    "#     output_df = pd.DataFrame(output_data)\n",
    "    \n",
    "#     return output_df\n",
    "\n",
    "# # Example usage:\n",
    "# # transformeddata = 'cash_beta_trench1_applied_loans_backscored_20241001_20250831'\n",
    "# # transform_data(f'{LOCALPATH}/{transformeddata}.csv')\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import uuid\n",
    "from datetime import datetime\n",
    "from typing import List\n",
    "\n",
    "def transform_data(\n",
    "    d1: pd.DataFrame, \n",
    "    feature_column: List[str], \n",
    "    a: str = 'demo_score', \n",
    "    modelDisplayName: str = 'Cash_beta_trench1_Demo_backscore', \n",
    "    tc: str = \"\", \n",
    "    subscription_name: str = 'sil_march 25 models'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Transforms input data into a structured format suitable for model scoring output.\n",
    "\n",
    "    Parameters:\n",
    "    - d1 (pd.DataFrame): Input DataFrame containing raw data.\n",
    "    - feature_column (List[str]): List of column names to include in the 'calcFeature' JSON.\n",
    "    - a (str): Column name containing the prediction score. Default is 'demo_score'.\n",
    "    - modelDisplayName (str): Name of the model used for scoring.\n",
    "    - tc (str): Trench category (optional).\n",
    "    - do (str): Device operating system. Default is 'android'.\n",
    "    - subscription_name (str): Name of the subscription or model group.\n",
    "\n",
    "    Returns:\n",
    "    - pd.DataFrame: Transformed DataFrame with structured output.\n",
    "    \"\"\"\n",
    "\n",
    "    # Make a copy of the input DataFrame to avoid modifying the original\n",
    "    df = d1.copy()\n",
    "    \n",
    "    # Initialize an empty list to store transformed rows\n",
    "    output_data = []\n",
    "    \n",
    "    # Iterate over each row in the DataFrame\n",
    "    for _, row in df.iterrows():\n",
    "        # Initialize dictionary to hold feature values\n",
    "        calc_feature = {}\n",
    "        \n",
    "        # Loop through each feature column and extract its value from the row\n",
    "        for col in feature_column:\n",
    "            if col in row and pd.notna(row[col]):\n",
    "                # Convert datetime values to ISO format strings\n",
    "                if isinstance(row[col], pd.Timestamp):\n",
    "                    calc_feature[col] = row[col].isoformat()\n",
    "                else:\n",
    "                    calc_feature[col] = row[col]\n",
    "        \n",
    "        # Get the current timestamp for start_time, end_time, and publish_time\n",
    "        current_time = datetime.now().isoformat()\n",
    "        \n",
    "        # Construct the output row dictionary with required fields\n",
    "        output_row = {\n",
    "            \"customerId\": row['customer_id'],  # Unique customer identifier\n",
    "            \"digitalLoanAccountId\": row['digitalLoanAccountId'],  # Loan account ID\n",
    "            \"crifApplicationId\": str(uuid.uuid4()),  # Random UUID for application ID\n",
    "            \"prediction\": row.get(a, 0),  # Prediction score from specified column\n",
    "            \"start_time\": current_time,  # Timestamp when processing starts\n",
    "            \"end_time\": current_time,    # Timestamp when processing ends\n",
    "            \"modelDisplayName\": modelDisplayName,  # Name of the model used\n",
    "            \"modelVersionId\": \"v1\",  # Static model version\n",
    "            \"calcFeature\": json.dumps(calc_feature, default=str),  # Features as JSON string\n",
    "            \"subscription_name\": subscription_name,  # Subscription name\n",
    "            \"message_id\": str(uuid.uuid4()),  # Random UUID for message ID\n",
    "            \"publish_time\": current_time,  # Timestamp when message is published\n",
    "            \"attributes\": \"{}\",  # Placeholder for additional attributes\n",
    "            \"trenchCategory\": tc,  # Optional trench category\n",
    "            \"deviceOs\": row['osType'],  # Device operating system\n",
    "        }\n",
    "        \n",
    "        # Append the transformed row to the output list\n",
    "        output_data.append(output_row)\n",
    "    \n",
    "    # Convert the list of dictionaries to a DataFrame\n",
    "    output_df = pd.DataFrame(output_data)\n",
    "    \n",
    "    # Return the transformed DataFrame\n",
    "    return output_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da72812e-83ad-4fcc-bd4b-ca04df18e98b",
   "metadata": {
    "id": "4PxokaKjyxV3"
   },
   "source": [
    "#### PSI Functions new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e06ed879-ce59-4b8b-9632-f91e8f0222ed",
   "metadata": {
    "id": "mp3klsmZV3Gz",
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Updated on 27-10-2025 - Modified for Training Period Baseline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Identify categorical and numerical features from the feature list.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    feature_list : List[str]\n",
    "        List of features to classify\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Dict with 'categorical' and 'numerical' keys containing respective feature lists\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "\n",
    "    for feature in feature_list:\n",
    "        if feature not in df.columns:\n",
    "            print(f\"Warning: Feature '{feature}' not found in dataframe\")\n",
    "            continue\n",
    "\n",
    "        # Check if feature is numeric\n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            # If unique values are less than 15 and all integers, treat as categorical\n",
    "            unique_vals = df[feature].nunique()\n",
    "            if unique_vals < 15 and df[feature].dropna().apply(lambda x: x == int(x) if isinstance(x, (int, float)) else False).all():\n",
    "                categorical_features.append(feature)\n",
    "            else:\n",
    "                numerical_features.append(feature)\n",
    "        else:\n",
    "            categorical_features.append(feature)\n",
    "\n",
    "    return {\n",
    "        'categorical': categorical_features,\n",
    "        'numerical': numerical_features\n",
    "    }\n",
    "\n",
    "\n",
    "def create_bins_for_features(df: pd.DataFrame,\n",
    "                             numerical_features: List[str],\n",
    "                             categorical_features: List[str],\n",
    "                             train_period_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create bins for numerical features (deciles with fallback) and categorical features (top 6 + others)\n",
    "    based on the entire training period data.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Full input dataframe\n",
    "    numerical_features : List[str]\n",
    "        List of numerical features\n",
    "    categorical_features : List[str]\n",
    "        List of categorical features\n",
    "    train_period_df : pd.DataFrame\n",
    "        Training period dataframe (June 2024 to March 2025)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    Dictionary containing binning information for each feature\n",
    "    \"\"\"\n",
    "    binning_info = {}\n",
    "\n",
    "    # Create bins for numerical features with fallback strategy\n",
    "    for feature in numerical_features:\n",
    "        valid_data = train_period_df[feature].dropna()\n",
    "\n",
    "        if len(valid_data) == 0:\n",
    "            binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "            continue\n",
    "\n",
    "        bins = None\n",
    "        bin_count = None\n",
    "\n",
    "        # Try 10 bins (deciles)\n",
    "        try:\n",
    "            test_bins = np.percentile(valid_data, np.arange(0, 101, 10))\n",
    "            test_bins = np.unique(test_bins)\n",
    "            if len(test_bins) >= 11:  # 11 edges = 10 bins\n",
    "                bins = test_bins\n",
    "                bin_count = 10\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        # If 10 bins not possible, try 5 bins\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, np.arange(0, 101, 20))\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 6:  # 6 edges = 5 bins\n",
    "                    bins = test_bins\n",
    "                    bin_count = 5\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # If 5 bins not possible, try 3 bins\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, [0, 33.33, 66.67, 100])\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 4:  # 4 edges = 3 bins\n",
    "                    bins = test_bins\n",
    "                    bin_count = 3\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # If still no bins possible, use equal distance bins of 5\n",
    "        if bins is None:\n",
    "            print(f\"Warning: Feature '{feature}' has insufficient variance - cannot create standard bins\")\n",
    "            print(f\"Feature '{feature}': Using equal distance bins of 5\")\n",
    "\n",
    "            min_val = valid_data.min()\n",
    "            max_val = valid_data.max()\n",
    "\n",
    "            # Create 5 equal distance bins\n",
    "            bins = np.linspace(min_val, max_val, 6)  # 6 edges = 5 bins\n",
    "            bins = np.unique(bins)\n",
    "            bin_count = len(bins) - 1\n",
    "\n",
    "            # If all values are the same, add slight buffer\n",
    "            if bin_count == 1:\n",
    "                bins = np.array([min_val - 0.1, min_val, min_val + 0.1])\n",
    "                bin_count = 2\n",
    "                print(f\"Feature '{feature}': Constant value ({min_val}). Created 2 equal distance bins with buffer\")\n",
    "\n",
    "        # Add infinity edges to capture all values\n",
    "        bins = bins.copy()\n",
    "        bins[0] = -np.inf\n",
    "        bins[-1] = np.inf\n",
    "\n",
    "        print(f\"Feature '{feature}': Created {bin_count} bins\")\n",
    "\n",
    "        # Create bin ranges dictionary\n",
    "        bin_ranges = {}\n",
    "        for i in range(len(bins)-1):\n",
    "            bin_name = f\"Bin_{i+1}\"\n",
    "            bin_ranges[bin_name] = {\n",
    "                'min': bins[i],\n",
    "                'max': bins[i+1],\n",
    "                'range_str': f\"[{bins[i]:.2f}, {bins[i+1]:.2f}]\" if not np.isinf(bins[i]) and not np.isinf(bins[i+1]) else f\"({bins[i]}, {bins[i+1]})\"\n",
    "            }\n",
    "\n",
    "        binning_info[feature] = {\n",
    "            'type': 'numerical',\n",
    "            'bins': bins,\n",
    "            'bin_ranges': bin_ranges,\n",
    "            'bin_count': bin_count\n",
    "        }\n",
    "\n",
    "    # Create bins for categorical features (top 6 + others) using training period\n",
    "    for feature in categorical_features:\n",
    "        value_counts = train_period_df[feature].value_counts()\n",
    "        unique_categories = value_counts.index.tolist()\n",
    "        print(f\"Unique categories: {unique_categories}\")\n",
    "\n",
    "        if len(unique_categories) <= 6:\n",
    "            # Treat each category as a separate bin\n",
    "            top_categories = unique_categories\n",
    "        else:\n",
    "            # Use top 6 categories only\n",
    "            top_categories = value_counts.nlargest(6).index.tolist()\n",
    "\n",
    "        print(f\"Top categories for feature '{feature}': {top_categories}\")\n",
    "\n",
    "        binning_info[feature] = {\n",
    "                'type': 'categorical',\n",
    "                'top_categories': top_categories,\n",
    "                'bin_ranges': {}  # No ranges for categorical\n",
    "            }\n",
    "\n",
    "    return binning_info\n",
    "\n",
    "\n",
    "def apply_binning(df: pd.DataFrame,\n",
    "                  feature: str,\n",
    "                  binning_info: Dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply binning to a feature based on binning information.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    feature : str\n",
    "        Feature name\n",
    "    binning_info : Dict\n",
    "        Binning information for the feature\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series with binned values\n",
    "    \"\"\"\n",
    "    if binning_info['type'] == 'numerical':\n",
    "        if binning_info['bins'] is None:\n",
    "            return pd.Series(['Missing'] * len(df), index=df.index)\n",
    "\n",
    "        bins = binning_info['bins']\n",
    "        labels = [f\"Bin_{i+1}\" for i in range(len(bins)-1)]\n",
    "\n",
    "        binned = pd.cut(df[feature],\n",
    "                       bins=bins,\n",
    "                       labels=labels,\n",
    "                       include_lowest=True,\n",
    "                       duplicates='drop')\n",
    "\n",
    "        # Handle nulls - convert to string and then replace\n",
    "        binned = binned.astype(str)\n",
    "        binned[df[feature].isna()] = 'Missing'\n",
    "\n",
    "        return binned\n",
    "\n",
    "    else:  # categorical\n",
    "        top_cats = binning_info['top_categories']\n",
    "\n",
    "        # Convert to string for consistent comparison\n",
    "        if pd.api.types.is_categorical_dtype(df[feature]):\n",
    "            feature_data = df[feature].astype(str)\n",
    "        else:\n",
    "            feature_data = df[feature].astype(str)\n",
    "\n",
    "        # Replace NaN string representation with 'Missing'\n",
    "        feature_data = feature_data.replace('nan', 'Missing')\n",
    "\n",
    "        # Convert top_cats to strings for comparison\n",
    "        top_cats_str = [str(cat) for cat in top_cats]\n",
    "\n",
    "        # Apply binning logic: use category name if in top_cats, else 'Others' (except for Missing)\n",
    "        binned = feature_data.apply(lambda x: x if x in top_cats_str else ('Others' if x != 'Missing' else 'Missing'))\n",
    "\n",
    "        return binned\n",
    "\n",
    "\n",
    "def calculate_psi(expected_pct: pd.Series,\n",
    "                  actual_pct: pd.Series,\n",
    "                  epsilon: float = 0.0001) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index with proper epsilon handling and renormalization.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    expected_pct : pd.Series\n",
    "        Expected (baseline) percentages\n",
    "    actual_pct : pd.Series\n",
    "        Actual percentages\n",
    "    epsilon : float\n",
    "        Small value to avoid log(0)\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    PSI value\n",
    "    \"\"\"\n",
    "    # Align indices\n",
    "    all_bins = expected_pct.index.union(actual_pct.index)\n",
    "    expected_pct = expected_pct.reindex(all_bins, fill_value=0)\n",
    "    actual_pct = actual_pct.reindex(all_bins, fill_value=0)\n",
    "\n",
    "    # Only add epsilon where values are zero\n",
    "    expected_pct = expected_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "    actual_pct = actual_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "\n",
    "    # Renormalize to ensure they sum to 1 after adding epsilon\n",
    "    expected_pct = expected_pct / expected_pct.sum()\n",
    "    actual_pct = actual_pct / actual_pct.sum()\n",
    "\n",
    "    # Calculate PSI\n",
    "    psi_value = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "\n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def calculate_month_on_month_psi(df: pd.DataFrame,\n",
    "                                 feature_list: List[str],\n",
    "                                 segment_columns: List[str],\n",
    "                                 month_col: str = 'Application_month',\n",
    "                                 data_selection_col: str = 'Data_selection',\n",
    "                                 account_id_col: str = 'digitalLoanAccountId') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate PSI for each feature comparing training period (June 2024 to March 2025)\n",
    "    vs each month after March 2025, overall and by segments.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    feature_list : List[str]\n",
    "        List of features to calculate PSI for\n",
    "    segment_columns : List[str]\n",
    "        List of segment columns\n",
    "    month_col : str\n",
    "        Name of month column\n",
    "    data_selection_col : str\n",
    "        Name of data selection column (identifies train period)\n",
    "    account_id_col : str\n",
    "        Name of account ID column for counting distinct accounts\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with PSI values with one row per feature-month-segment combination\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify training and test periods\n",
    "    train_df = df[df[data_selection_col] == 'Train'].copy()\n",
    "    test_df = df[df[data_selection_col] != 'Train'].copy()\n",
    "\n",
    "    if len(train_df) == 0:\n",
    "        raise ValueError(\"No training data found. Check Data_selection column.\")\n",
    "\n",
    "    print(f\"Training period: {train_df[month_col].min()} to {train_df[month_col].max()}\")\n",
    "    print(f\"Test period: {test_df[month_col].min()} to {test_df[month_col].max()}\")\n",
    "\n",
    "    # Identify feature types\n",
    "    feature_types = identify_feature_types(df, feature_list)\n",
    "\n",
    "    # Create binning strategy based on training period\n",
    "    binning_info = create_bins_for_features(\n",
    "        df,\n",
    "        feature_types['numerical'],\n",
    "        feature_types['categorical'],\n",
    "        train_df\n",
    "    )\n",
    "\n",
    "    # Get sorted test months\n",
    "    test_months = sorted(test_df[month_col].unique())\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # Calculate overall PSI\n",
    "    for feature in feature_list:\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Apply binning to entire dataset\n",
    "        df[f'{feature}_binned'] = apply_binning(df, feature, binning_info[feature])\n",
    "        # print(f\"Feature binned {df[f'{feature}_binned']}\")\n",
    "        # Get training period distribution (baseline)\n",
    "        train_baseline = df[df[data_selection_col] == 'Train'][f'{feature}_binned'].value_counts(normalize=True)\n",
    "\n",
    "        # Calculate PSI for each test month\n",
    "        for month in test_months:\n",
    "            actual_dist = df[df[month_col] == month][f'{feature}_binned'].value_counts(normalize=True)\n",
    "            psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "\n",
    "            # Calculate average percentages across all bins\n",
    "            expected_avg_pct = train_baseline.mean() * 100\n",
    "            actual_avg_pct = actual_dist.mean() * 100\n",
    "\n",
    "            # # Count distinct accounts for segment\n",
    "            # base_segment_count = train_segment[account_id_col].nunique()\n",
    "            # actual_segment_count = actual_segment[account_id_col].nunique()\n",
    "\n",
    "\n",
    "            results.append({\n",
    "                'Feature': feature,\n",
    "                'Feature_Type': binning_info[feature]['type'],\n",
    "                'Segment_Column': 'Overall',\n",
    "                'Segment_Value': 'All',\n",
    "                'Month': f\"{month}\",\n",
    "                'Base_Month': 'Train (Jun 2024 - Mar 2025)',\n",
    "                'Current_Month': month,\n",
    "                'Expected_Percentage': expected_avg_pct,\n",
    "                'Actual_Percentage': actual_avg_pct,\n",
    "                'PSI': psi_value\n",
    "            })\n",
    "\n",
    "    # Calculate PSI by segments\n",
    "    for segment_col in segment_columns:\n",
    "        if segment_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        segments = df[segment_col].dropna().unique()\n",
    "\n",
    "        for segment_val in segments:\n",
    "            segment_df = df[df[segment_col] == segment_val]\n",
    "\n",
    "            for feature in feature_list:\n",
    "                if feature not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Get training period distribution for segment\n",
    "                train_segment = segment_df[segment_df[data_selection_col] == 'Train']\n",
    "                if len(train_segment) == 0:\n",
    "                    continue\n",
    "\n",
    "                train_baseline = train_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "\n",
    "                # Calculate PSI for each test month\n",
    "                for month in test_months:\n",
    "                    actual_segment = segment_df[segment_df[month_col] == month]\n",
    "                    if len(actual_segment) == 0:\n",
    "                        continue\n",
    "\n",
    "                    actual_dist = actual_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                    psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "\n",
    "                    # Calculate average percentages across all bins\n",
    "                    expected_avg_pct = train_baseline.mean() * 100\n",
    "                    actual_avg_pct = actual_dist.mean() * 100\n",
    "\n",
    "                    # Count distinct accounts for segment\n",
    "                    base_segment_count = train_segment[account_id_col].nunique()\n",
    "                    actual_segment_count = actual_segment[account_id_col].nunique()\n",
    "\n",
    "                    results.append({\n",
    "                        'Feature': feature,\n",
    "                        'Feature_Type': binning_info[feature]['type'],\n",
    "                        'Segment_Column': segment_col,\n",
    "                        'Segment_Value': segment_val,\n",
    "                        'Month': f\"{month}\",\n",
    "                        'Base_Month': 'Train (Jun 2024 - Mar 2025)',\n",
    "                        'Current_Month': month,\n",
    "                        'Base_Count': base_segment_count,\n",
    "                        'Actual_Count': actual_segment_count,\n",
    "                        'Expected_Percentage': expected_avg_pct,\n",
    "                        'Actual_Percentage': actual_avg_pct,\n",
    "                        'PSI': psi_value\n",
    "                    })\n",
    "\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "def calculate_bin_level_psi(df: pd.DataFrame,\n",
    "                            feature_list: List[str],\n",
    "                            segment_columns: List[str],\n",
    "                            month_col: str = 'Application_month',\n",
    "                            data_selection_col: str = 'Data_selection',\n",
    "                            account_id_col: str = 'digitalLoanAccountId') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate bin-level PSI for each feature comparing training period\n",
    "    vs each month after March 2025, overall and by segments.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "    feature_list : List[str]\n",
    "        List of features to calculate PSI for\n",
    "    segment_columns : List[str]\n",
    "        List of segment columns\n",
    "    month_col : str\n",
    "        Name of month column\n",
    "    data_selection_col : str\n",
    "        Name of data selection column\n",
    "    account_id_col : str\n",
    "        Name of account ID column for counting distinct accounts\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with bin-level PSI details including bin ranges\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "\n",
    "    # Identify training and test periods\n",
    "    train_df = df[df[data_selection_col] == 'Train'].copy()\n",
    "    test_df = df[df[data_selection_col] != 'Train'].copy()\n",
    "\n",
    "    if len(train_df) == 0:\n",
    "        raise ValueError(\"No training data found. Check Data_selection column.\")\n",
    "\n",
    "    print(f\"Training period: {train_df[month_col].min()} to {train_df[month_col].max()}\")\n",
    "    print(f\"Test period: {test_df[month_col].min()} to {test_df[month_col].max()}\")\n",
    "\n",
    "    # Identify feature types\n",
    "    feature_types = identify_feature_types(df, feature_list)\n",
    "\n",
    "    # Create binning strategy based on training period\n",
    "    binning_info = create_bins_for_features(\n",
    "        df,\n",
    "        feature_types['numerical'],\n",
    "        feature_types['categorical'],\n",
    "        train_df\n",
    "    )\n",
    "\n",
    "    # Get sorted test months\n",
    "    test_months = sorted(test_df[month_col].unique())\n",
    "\n",
    "    results = []\n",
    "    epsilon = 0.0001\n",
    "\n",
    "    # Calculate overall bin-level PSI\n",
    "    for feature in feature_list:\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        # Apply binning to entire dataset\n",
    "        df[f'{feature}_binned'] = apply_binning(df, feature, binning_info[feature])\n",
    "        # print(df[f'{feature}_binned'])\n",
    "\n",
    "        # Get training period distribution (baseline)\n",
    "        train_baseline = df[df[data_selection_col] == 'Train'][f'{feature}_binned'].value_counts(normalize=True)\n",
    "\n",
    "        # Calculate bin-level PSI for each test month\n",
    "        for month in test_months:\n",
    "            month_data = df[df[month_col] == month]\n",
    "            actual_dist = month_data[f'{feature}_binned'].value_counts(normalize=True)\n",
    "\n",
    "            # Count distinct accounts\n",
    "            base_count = df[df[data_selection_col] == 'Train'][account_id_col].nunique()\n",
    "            actual_count = month_data[account_id_col].nunique()\n",
    "\n",
    "            # Get all bins\n",
    "            all_bins = train_baseline.index.union(actual_dist.index)\n",
    "\n",
    "            for bin_name in all_bins:\n",
    "                # Simplified epsilon logic - no redundancy\n",
    "                expected_pct = train_baseline.get(bin_name, 0)\n",
    "                actual_pct = actual_dist.get(bin_name, 0)\n",
    "\n",
    "                # Add epsilon only if zero\n",
    "                expected_pct = epsilon if expected_pct == 0 else expected_pct\n",
    "                actual_pct = epsilon if actual_pct == 0 else actual_pct\n",
    "\n",
    "                # Calculate bin-level PSI\n",
    "                bin_psi = (actual_pct - expected_pct) * np.log(actual_pct / expected_pct)\n",
    "\n",
    "                # Get bin range information\n",
    "                bin_ranges = binning_info[feature]['bin_ranges']\n",
    "                if bin_name in bin_ranges:\n",
    "                    bin_min = bin_ranges[bin_name]['min']\n",
    "                    bin_max = bin_ranges[bin_name]['max']\n",
    "                    bin_range = bin_ranges[bin_name]['range_str']\n",
    "                else:\n",
    "                    # For categorical or special bins (Missing, Others)\n",
    "                    bin_min = None\n",
    "                    bin_max = None\n",
    "                    bin_range = bin_name\n",
    "\n",
    "                results.append({\n",
    "                    'Feature': feature,\n",
    "                    'Feature_Type': binning_info[feature]['type'],\n",
    "                    'Segment_Column': 'Overall',\n",
    "                    'Segment_Value': 'All',\n",
    "                    'Month': f\"{month}\",\n",
    "                    'Base_Month': 'Train (Jun 2024 - Mar 2025)',\n",
    "                    'Current_Month': month,\n",
    "                    'Base_Count': base_count,\n",
    "                    'Actual_Count': actual_count,\n",
    "                    'Bin': bin_name,\n",
    "                    'Bin_Range': bin_range,\n",
    "                    'Bin_Min': bin_min,\n",
    "                    'Bin_Max': bin_max,\n",
    "                    'Base_Percentage': (train_baseline.get(bin_name, 0) * 100),\n",
    "                    'Actual_Percentage': (actual_dist.get(bin_name, 0) * 100),\n",
    "                    'Bin_PSI': bin_psi\n",
    "                })\n",
    "\n",
    "    # Calculate bin-level PSI by segments\n",
    "    for segment_col in segment_columns:\n",
    "        if segment_col not in df.columns:\n",
    "            continue\n",
    "\n",
    "        segments = df[segment_col].dropna().unique()\n",
    "\n",
    "        for segment_val in segments:\n",
    "            segment_df = df[df[segment_col] == segment_val]\n",
    "\n",
    "            for feature in feature_list:\n",
    "                if feature not in df.columns:\n",
    "                    continue\n",
    "\n",
    "                # Get training period distribution for segment\n",
    "                train_segment = segment_df[segment_df[data_selection_col] == 'Train']\n",
    "                if len(train_segment) == 0:\n",
    "                    continue\n",
    "\n",
    "                train_baseline = train_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "\n",
    "                # Calculate bin-level PSI for each test month\n",
    "                for month in test_months:\n",
    "                    actual_segment = segment_df[segment_df[month_col] == month]\n",
    "                    if len(actual_segment) == 0:\n",
    "                        continue\n",
    "\n",
    "                    actual_dist = actual_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "\n",
    "                    # Count distinct accounts for segment\n",
    "                    base_segment_count = train_segment[account_id_col].nunique()\n",
    "                    actual_segment_count = actual_segment[account_id_col].nunique()\n",
    "\n",
    "                    # Get all bins\n",
    "                    all_bins = train_baseline.index.union(actual_dist.index)\n",
    "\n",
    "                    for bin_name in all_bins:\n",
    "                        # Simplified epsilon logic - no redundancy\n",
    "                        expected_pct = train_baseline.get(bin_name, 0)\n",
    "                        actual_pct = actual_dist.get(bin_name, 0)\n",
    "\n",
    "                        # Add epsilon only if zero\n",
    "                        expected_pct = epsilon if expected_pct == 0 else expected_pct\n",
    "                        actual_pct = epsilon if actual_pct == 0 else actual_pct\n",
    "\n",
    "                        # Calculate bin-level PSI\n",
    "                        bin_psi = (actual_pct - expected_pct) * np.log(actual_pct / expected_pct)\n",
    "\n",
    "                        # Get bin range information\n",
    "                        bin_ranges = binning_info[feature]['bin_ranges']\n",
    "                        if bin_name in bin_ranges:\n",
    "                            bin_min = bin_ranges[bin_name]['min']\n",
    "                            bin_max = bin_ranges[bin_name]['max']\n",
    "                            bin_range = bin_ranges[bin_name]['range_str']\n",
    "                        else:\n",
    "                            # For categorical or special bins (Missing, Others)\n",
    "                            bin_min = None\n",
    "                            bin_max = None\n",
    "                            bin_range = bin_name\n",
    "\n",
    "                        results.append({\n",
    "                            'Feature': feature,\n",
    "                            'Feature_Type': binning_info[feature]['type'],\n",
    "                            'Segment_Column': segment_col,\n",
    "                            'Segment_Value': segment_val,\n",
    "                            'Month': f\"{month}\",\n",
    "                            'Base_Month': 'Train (Jun 2024 - Mar 2025)',\n",
    "                            'Current_Month': month,\n",
    "                            'Base_Count': base_segment_count,\n",
    "                            'Actual_Count': actual_segment_count,\n",
    "                            'Bin': bin_name,\n",
    "                            'Bin_Range': bin_range,\n",
    "                            'Bin_Min': bin_min,\n",
    "                            'Bin_Max': bin_max,\n",
    "                            'Base_Percentage': (train_baseline.get(bin_name, 0) * 100),\n",
    "                            'Actual_Percentage': (actual_dist.get(bin_name, 0) * 100),\n",
    "                            'Bin_PSI': bin_psi\n",
    "                        })\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5aa65f4",
   "metadata": {},
   "source": [
    "# dropping_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "e07a0a87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropping_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop duplicates based on digitalLoanAccountId, Data_selection, and modelVersionid,\n",
    "    keeping the first occurrence based on appln_submit_datetime.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with duplicates dropped\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sort_values(\n",
    "        ['digitalLoanAccountId', 'Data_selection', 'modelVersionId', 'appln_submit_datetime'],\n",
    "        ascending=[True, True, True, True],\n",
    "        na_position='last'\n",
    "    )\n",
    "\n",
    "    result = df.drop_duplicates(\n",
    "        subset=['digitalLoanAccountId', 'Data_selection', 'modelVersionId'],\n",
    "        keep='first'\n",
    "    ).copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54986073",
   "metadata": {},
   "source": [
    "# New PSI Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c32d80d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# Assuming you have df with concatenated Train and Test data\\n\\n# Calculate Overall PSI (Overall + By Segments)\\npsi_results = calculate_psi_by_model_version(\\n    df=your_concatenated_df,\\n    score_column=\\'Alpha_cic_sil_score\\',\\n    segment_columns=[\\'new_loan_type\\', \\'loan_product_type\\', \\'osType\\'],\\n    month_col=\\'Application_month\\',\\n    data_selection_col=\\'Data_selection\\',\\n    model_version_col=\\'modelVersionId\\',\\n    account_id_col=\\'digitalLoanAccountId\\'\\n)\\n\\nprint(psi_results.head(20))\\nprint(f\"\\nTotal rows: {len(psi_results)}\")\\n\\n# View Overall results only\\noverall_results = psi_results[psi_results[\\'Segment_Column\\'] == \\'Overall\\']\\nprint(overall_results)\\n\\n# View specific model version\\nv1_results = psi_results[psi_results[\\'Model_Version\\'] == \\'v1\\']\\nprint(v1_results)\\n\\n# View specific segment\\nloan_type_results = psi_results[psi_results[\\'Segment_Column\\'] == \\'new_loan_type\\']\\nprint(loan_type_results)\\n\\n# Save results\\npsi_results.to_csv(\\'psi_results_overall_and_segments.csv\\', index=False)\\n\\n# ---- For Bin-Level Details ----\\nbin_psi_results = calculate_bin_level_psi_by_model_version(\\n    df=your_concatenated_df,\\n    score_column=\\'Alpha_cic_sil_score\\',\\n    segment_columns=[\\'new_loan_type\\', \\'loan_product_type\\', \\'osType\\']\\n)\\n\\nbin_psi_results.to_csv(\\'bin_level_psi_results_overall_and_segments.csv\\', index=False)\\nprint(bin_psi_results.head(30))\\n'"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def expand_calc_features(df, exclude_columns: List[str] = None):\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns and return the complete DataFrame.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): Input DataFrame with calcFeatures column containing JSON data\n",
    "    exclude_columns (List[str]): List of column names to exclude from expansion\n",
    "                                (e.g., ['digitalLoanAccountId', 'customerId', 'crifApplicationId', 'run_date'])\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Expanded DataFrame with all original columns plus JSON features as separate columns\n",
    "    \"\"\"\n",
    "    if exclude_columns is None:\n",
    "        exclude_columns = []\n",
    "    \n",
    "    df_expanded = df.copy()\n",
    "    calc_features_list = []\n",
    "\n",
    "    for idx, calc_features_str in enumerate(df['calcFeatures']):\n",
    "        try:\n",
    "            features_dict = json.loads(calc_features_str.replace(\"'\", '\"'))\n",
    "            # Remove excluded columns from the features dict\n",
    "            for col in exclude_columns:\n",
    "                features_dict.pop(col, None)\n",
    "            calc_features_list.append(features_dict)\n",
    "        except (json.JSONDecodeError, AttributeError) as e:\n",
    "            print(f\"Warning: Could not parse calcFeatures at index {idx}: {e}\")\n",
    "            calc_features_list.append({})\n",
    "\n",
    "    calc_features_df = pd.DataFrame(calc_features_list)\n",
    "    calc_features_df = calc_features_df.add_prefix('calc_')\n",
    "\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    calc_features_df = calc_features_df.reset_index(drop=True)\n",
    "\n",
    "    result_df = pd.concat([df_expanded, calc_features_df], axis=1)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Identify categorical and numerical features from the feature list.\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "\n",
    "    for feature in feature_list:\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            unique_vals = df[feature].nunique()\n",
    "            if unique_vals < 15 and df[feature].dropna().apply(lambda x: x == int(x) if isinstance(x, (int, float)) else False).all():\n",
    "                categorical_features.append(feature)\n",
    "            else:\n",
    "                numerical_features.append(feature)\n",
    "        else:\n",
    "            categorical_features.append(feature)\n",
    "\n",
    "    return {\n",
    "        'categorical': categorical_features,\n",
    "        'numerical': numerical_features\n",
    "    }\n",
    "\n",
    "\n",
    "def create_bins_for_features(df: pd.DataFrame,\n",
    "                             numerical_features: List[str],\n",
    "                             categorical_features: List[str],\n",
    "                             train_period_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create bins for numerical features (deciles with fallback) and categorical features (top 6 + others)\n",
    "    based on the training period data.\n",
    "    \"\"\"\n",
    "    binning_info = {}\n",
    "\n",
    "    for feature in numerical_features:\n",
    "        valid_data = train_period_df[feature].dropna()\n",
    "\n",
    "        if len(valid_data) == 0:\n",
    "            binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "            continue\n",
    "\n",
    "        bins = None\n",
    "        bin_count = None\n",
    "\n",
    "        # Try 10 bins (deciles)\n",
    "        try:\n",
    "            test_bins = np.percentile(valid_data, np.arange(0, 101, 10))\n",
    "            test_bins = np.unique(test_bins)\n",
    "            if len(test_bins) >= 11:\n",
    "                bins = test_bins\n",
    "                bin_count = 10\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        # If 10 bins not possible, try 5 bins\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, np.arange(0, 101, 20))\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 6:\n",
    "                    bins = test_bins\n",
    "                    bin_count = 5\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # If 5 bins not possible, try 3 bins\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, [0, 33.33, 66.67, 100])\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 4:\n",
    "                    bins = test_bins\n",
    "                    bin_count = 3\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # If still no bins possible, use equal distance bins of 5\n",
    "        if bins is None:\n",
    "            print(f\"Warning: Feature '{feature}' has insufficient variance - cannot create standard bins\")\n",
    "            min_val = valid_data.min()\n",
    "            max_val = valid_data.max()\n",
    "            bins = np.linspace(min_val, max_val, 6)\n",
    "            bins = np.unique(bins)\n",
    "            bin_count = len(bins) - 1\n",
    "\n",
    "            if bin_count == 1:\n",
    "                bins = np.array([min_val - 0.1, min_val, min_val + 0.1])\n",
    "                bin_count = 2\n",
    "\n",
    "        bins = bins.copy()\n",
    "        bins[0] = -np.inf\n",
    "        bins[-1] = np.inf\n",
    "\n",
    "        print(f\"Feature '{feature}': Created {bin_count} bins\")\n",
    "\n",
    "        bin_ranges = {}\n",
    "        for i in range(len(bins)-1):\n",
    "            bin_name = f\"Bin_{i+1}\"\n",
    "            bin_ranges[bin_name] = {\n",
    "                'min': bins[i],\n",
    "                'max': bins[i+1],\n",
    "                'range_str': f\"[{bins[i]:.2f}, {bins[i+1]:.2f}]\" if not np.isinf(bins[i]) and not np.isinf(bins[i+1]) else f\"({bins[i]}, {bins[i+1]})\"\n",
    "            }\n",
    "\n",
    "        binning_info[feature] = {\n",
    "            'type': 'numerical',\n",
    "            'bins': bins,\n",
    "            'bin_ranges': bin_ranges,\n",
    "            'bin_count': bin_count\n",
    "        }\n",
    "\n",
    "    # Categorical features\n",
    "    for feature in categorical_features:\n",
    "        value_counts = train_period_df[feature].value_counts()\n",
    "        unique_categories = value_counts.index.tolist()\n",
    "\n",
    "        if len(unique_categories) <= 6:\n",
    "            top_categories = unique_categories\n",
    "        else:\n",
    "            top_categories = value_counts.nlargest(6).index.tolist()\n",
    "\n",
    "        print(f\"Top categories for feature '{feature}': {top_categories}\")\n",
    "\n",
    "        binning_info[feature] = {\n",
    "            'type': 'categorical',\n",
    "            'top_categories': top_categories,\n",
    "            'bin_ranges': {}\n",
    "        }\n",
    "\n",
    "    return binning_info\n",
    "\n",
    "\n",
    "def apply_binning(df: pd.DataFrame,\n",
    "                  feature: str,\n",
    "                  binning_info: Dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply binning to a feature based on binning information.\n",
    "    \"\"\"\n",
    "    if binning_info['type'] == 'numerical':\n",
    "        if binning_info['bins'] is None:\n",
    "            return pd.Series(['Missing'] * len(df), index=df.index)\n",
    "\n",
    "        bins = binning_info['bins']\n",
    "        labels = [f\"Bin_{i+1}\" for i in range(len(bins)-1)]\n",
    "\n",
    "        binned = pd.cut(df[feature],\n",
    "                       bins=bins,\n",
    "                       labels=labels,\n",
    "                       include_lowest=True,\n",
    "                       duplicates='drop')\n",
    "\n",
    "        binned = binned.astype(str)\n",
    "        binned[df[feature].isna()] = 'Missing'\n",
    "\n",
    "        return binned\n",
    "\n",
    "    else:\n",
    "        top_cats = binning_info['top_categories']\n",
    "\n",
    "        if pd.api.types.is_categorical_dtype(df[feature]):\n",
    "            feature_data = df[feature].astype(str)\n",
    "        else:\n",
    "            feature_data = df[feature].astype(str)\n",
    "\n",
    "        feature_data = feature_data.replace('nan', 'Missing')\n",
    "        top_cats_str = [str(cat) for cat in top_cats]\n",
    "\n",
    "        binned = feature_data.apply(lambda x: x if x in top_cats_str else ('Others' if x != 'Missing' else 'Missing'))\n",
    "\n",
    "        return binned\n",
    "\n",
    "\n",
    "def calculate_psi(expected_pct: pd.Series,\n",
    "                  actual_pct: pd.Series,\n",
    "                  epsilon: float = 0.0001) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index with proper epsilon handling and renormalization.\n",
    "    \"\"\"\n",
    "    all_bins = expected_pct.index.union(actual_pct.index)\n",
    "    expected_pct = expected_pct.reindex(all_bins, fill_value=0)\n",
    "    actual_pct = actual_pct.reindex(all_bins, fill_value=0)\n",
    "\n",
    "    expected_pct = expected_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "    actual_pct = actual_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "\n",
    "    expected_pct = expected_pct / expected_pct.sum()\n",
    "    actual_pct = actual_pct / actual_pct.sum()\n",
    "\n",
    "    psi_value = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "\n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def calculate_psi_by_model_version(df: pd.DataFrame,\n",
    "                                   score_column: str,\n",
    "                                   segment_columns: List[str] = None,\n",
    "                                   month_col: str = 'Application_month',\n",
    "                                   data_selection_col: str = 'Data_selection',\n",
    "                                   model_version_col: str = 'modelVersionId',\n",
    "                                   trench_category_col: str = 'trenchCategory',\n",
    "                                   model_display_name_col: str = 'modelDisplayName',\n",
    "                                   account_id_col: str = 'digitalLoanAccountId') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate PSI for each model version by comparing Train vs Test periods.\n",
    "    Expands calcFeatures for each model version and calculates PSI for:\n",
    "    1. Overall score\n",
    "    2. Each expanded feature\n",
    "    3. By segments (optional)\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with concatenated Train and Test data\n",
    "    score_column : str\n",
    "        Name of the score column (e.g., 'Alpha_cic_sil_score')\n",
    "    segment_columns : List[str]\n",
    "        List of segment columns (e.g., ['new_loan_type', 'loan_product_type', 'osType'])\n",
    "    month_col : str\n",
    "        Name of month column\n",
    "    data_selection_col : str\n",
    "        Name of data selection column (Train/Test)\n",
    "    model_version_col : str\n",
    "        Name of model version column\n",
    "    trench_category_col : str\n",
    "        Name of trench category column\n",
    "    model_display_name_col : str\n",
    "        Name of model display name column\n",
    "    account_id_col : str\n",
    "        Name of account ID column\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with PSI values for overall and by features\n",
    "    \"\"\"\n",
    "    \n",
    "    if segment_columns is None:\n",
    "        segment_columns = []\n",
    "    \n",
    "    # Columns to exclude when expanding calcFeatures\n",
    "    exclude_cols = ['digitalLoanAccountId', 'customerId', 'crifApplicationId', 'run_date']\n",
    "    \n",
    "    # Expand calcFeatures for entire dataset\n",
    "    print(\"Expanding calcFeatures (excluding: digitalLoanAccountId, customerId, crifApplicationId, run_date)...\")\n",
    "    df_expanded = expand_calc_features(df, exclude_columns=exclude_cols)\n",
    "    \n",
    "    # Get unique model versions\n",
    "    model_versions = sorted(df_expanded[model_version_col].unique())\n",
    "    print(f\"Found model versions: {model_versions}\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Process each model version\n",
    "    for model_version in model_versions:\n",
    "        print(f\"{'='*100}\")\n",
    "        print(f\"Processing Model Version: {model_version}\")\n",
    "        print(f\"{'='*100}\")\n",
    "        \n",
    "        # Filter data for current model version\n",
    "        mv_df = df_expanded[df_expanded[model_version_col] == model_version].copy()\n",
    "        \n",
    "        # Extract trenchCategory and modelDisplayName for this model version\n",
    "        trench_category = mv_df[trench_category_col].iloc[0] if trench_category_col in mv_df.columns else None\n",
    "        model_display_name = mv_df[model_display_name_col].iloc[0] if model_display_name_col in mv_df.columns else None\n",
    "        \n",
    "        # Split into train and test\n",
    "        train_df = mv_df[mv_df[data_selection_col] == 'Train'].copy()\n",
    "        test_df = mv_df[mv_df[data_selection_col] == 'Test'].copy()\n",
    "        \n",
    "        if len(train_df) == 0:\n",
    "            print(f\"Warning: No training data for model version {model_version}\\n\")\n",
    "            continue\n",
    "        \n",
    "        if len(test_df) == 0:\n",
    "            print(f\"Warning: No test data for model version {model_version}\\n\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"Train records: {len(train_df)}, Test records: {len(test_df)}\")\n",
    "        print(f\"Training period: {train_df[month_col].min()} to {train_df[month_col].max()}\")\n",
    "        print(f\"Test period: {test_df[month_col].min()} to {test_df[month_col].max()}\")\n",
    "        \n",
    "        # Get all expanded features (calc_ prefixed columns)\n",
    "        calc_features = [col for col in df_expanded.columns if col.startswith('calc_')]\n",
    "        \n",
    "        # Add score column to feature list\n",
    "        feature_list = [score_column] + calc_features\n",
    "        \n",
    "        print(f\"Score column: {score_column}\")\n",
    "        print(f\"Number of expanded features: {len(calc_features)}\\n\")\n",
    "        \n",
    "        # Identify feature types\n",
    "        feature_types = identify_feature_types(mv_df, feature_list)\n",
    "        \n",
    "        # Create binning strategy based on training period\n",
    "        binning_info = create_bins_for_features(\n",
    "            mv_df,\n",
    "            feature_types['numerical'],\n",
    "            feature_types['categorical'],\n",
    "            train_df\n",
    "        )\n",
    "        \n",
    "        # Get sorted test months\n",
    "        test_months = sorted(test_df[month_col].unique())\n",
    "        \n",
    "        # ========== OVERALL PSI ==========\n",
    "        print(f\"\\nCalculating Overall PSI...\")\n",
    "        for feature in feature_list:\n",
    "            if feature not in mv_df.columns:\n",
    "                continue\n",
    "            \n",
    "            # Apply binning to entire dataset\n",
    "            mv_df[f'{feature}_binned'] = apply_binning(mv_df, feature, binning_info[feature])\n",
    "            \n",
    "            # Get training period distribution (baseline)\n",
    "            train_baseline = mv_df[mv_df[data_selection_col] == 'Train'][f'{feature}_binned'].value_counts(normalize=True)\n",
    "            \n",
    "            # Calculate PSI for each test month\n",
    "            for month in test_months:\n",
    "                month_data = mv_df[mv_df[month_col] == month]\n",
    "                \n",
    "                if len(month_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                actual_dist = month_data[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "                \n",
    "                # Count distinct accounts\n",
    "                base_count = mv_df[mv_df[data_selection_col] == 'Train'][account_id_col].nunique()\n",
    "                actual_count = month_data[account_id_col].nunique()\n",
    "                \n",
    "                # Calculate average percentages\n",
    "                expected_avg_pct = train_baseline.mean() * 100\n",
    "                actual_avg_pct = actual_dist.mean() * 100\n",
    "                \n",
    "                # Determine feature category\n",
    "                feature_category = 'Score' if feature == score_column else 'Expanded_Feature'\n",
    "                \n",
    "                results_row = {\n",
    "                    'Model_Version': model_version,\n",
    "                    'Trench_Category': trench_category,\n",
    "                    'Model_Display_Name': model_display_name,\n",
    "                    'Feature': feature,\n",
    "                    'Feature_Type': binning_info[feature]['type'],\n",
    "                    'Feature_Category': feature_category,\n",
    "                    'Segment_Column': 'Overall',\n",
    "                    'Segment_Value': 'All',\n",
    "                    'Month': f\"{month}\",\n",
    "                    'Base_Month': 'Train',\n",
    "                    'Current_Month': month,\n",
    "                    'Base_Count': base_count,\n",
    "                    'Actual_Count': actual_count,\n",
    "                    'Expected_Percentage': expected_avg_pct,\n",
    "                    'Actual_Percentage': actual_avg_pct,\n",
    "                    'PSI': psi_value\n",
    "                }\n",
    "                \n",
    "                all_results.append(results_row)\n",
    "        \n",
    "        # ========== SEGMENT-WISE PSI ==========\n",
    "        if segment_columns:\n",
    "            print(f\"\\nCalculating Segment-wise PSI...\")\n",
    "            for segment_col in segment_columns:\n",
    "                if segment_col not in mv_df.columns:\n",
    "                    print(f\"Warning: Segment column '{segment_col}' not found\")\n",
    "                    continue\n",
    "                \n",
    "                segments = mv_df[segment_col].dropna().unique()\n",
    "                print(f\"\\nSegment Column: {segment_col} | Values: {list(segments)}\")\n",
    "                \n",
    "                for segment_val in segments:\n",
    "                    segment_df = mv_df[mv_df[segment_col] == segment_val].copy()\n",
    "                    \n",
    "                    # Get train and test data for this segment\n",
    "                    train_segment = segment_df[segment_df[data_selection_col] == 'Train']\n",
    "                    test_segment = segment_df[segment_df[data_selection_col] == 'Test']\n",
    "                    \n",
    "                    if len(train_segment) == 0 or len(test_segment) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    for feature in feature_list:\n",
    "                        if feature not in segment_df.columns:\n",
    "                            continue\n",
    "                        \n",
    "                        # Get baseline from train segment\n",
    "                        train_baseline = train_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                        \n",
    "                        # Calculate PSI for each test month in this segment\n",
    "                        for month in test_months:\n",
    "                            actual_segment = segment_df[segment_df[month_col] == month]\n",
    "                            \n",
    "                            if len(actual_segment) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            actual_dist = actual_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                            psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "                            \n",
    "                            # Count distinct accounts for segment\n",
    "                            base_segment_count = train_segment[account_id_col].nunique()\n",
    "                            actual_segment_count = actual_segment[account_id_col].nunique()\n",
    "                            \n",
    "                            # Calculate average percentages\n",
    "                            expected_avg_pct = train_baseline.mean() * 100\n",
    "                            actual_avg_pct = actual_dist.mean() * 100\n",
    "                            \n",
    "                            feature_category = 'Score' if feature == score_column else 'Expanded_Feature'\n",
    "                            \n",
    "                            results_row = {\n",
    "                                'Model_Version': model_version,\n",
    "                                'Trench_Category': trench_category,\n",
    "                                'Model_Display_Name': model_display_name,\n",
    "                                'Feature': feature,\n",
    "                                'Feature_Type': binning_info[feature]['type'],\n",
    "                                'Feature_Category': feature_category,\n",
    "                                'Segment_Column': segment_col,\n",
    "                                'Segment_Value': segment_val,\n",
    "                                'Month': f\"{month}\",\n",
    "                                'Base_Month': 'Train',\n",
    "                                'Current_Month': month,\n",
    "                                'Base_Count': base_segment_count,\n",
    "                                'Actual_Count': actual_segment_count,\n",
    "                                'Expected_Percentage': expected_avg_pct,\n",
    "                                'Actual_Percentage': actual_avg_pct,\n",
    "                                'PSI': psi_value\n",
    "                            }\n",
    "                            \n",
    "                            all_results.append(results_row)\n",
    "        \n",
    "        print(f\"Completed processing for Model Version: {model_version}\\n\")\n",
    "    \n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    return results_df\n",
    "\n",
    "\n",
    "def calculate_bin_level_psi_by_model_version(df: pd.DataFrame,\n",
    "                                             score_column: str,\n",
    "                                             segment_columns: List[str] = None,\n",
    "                                             month_col: str = 'Application_month',\n",
    "                                             data_selection_col: str = 'Data_selection',\n",
    "                                             model_version_col: str = 'modelVersionId',\n",
    "                                             trench_category_col: str = 'trenchCategory',\n",
    "                                             model_display_name_col: str = 'modelDisplayName',\n",
    "                                             account_id_col: str = 'digitalLoanAccountId') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate bin-level PSI for each model version.\n",
    "    Provides detailed breakdown by bins/categories.\n",
    "    \"\"\"\n",
    "    \n",
    "    if segment_columns is None:\n",
    "        segment_columns = []\n",
    "    \n",
    "    print(\"Expanding calcFeatures for bin-level analysis (excluding: digitalLoanAccountId, customerId, crifApplicationId, run_date)...\")\n",
    "    df_expanded = expand_calc_features(df, exclude_columns=['digitalLoanAccountId', 'customerId', 'crifApplicationId', 'run_date'])\n",
    "    \n",
    "    model_versions = sorted(df_expanded[model_version_col].unique())\n",
    "    \n",
    "    all_results = []\n",
    "    epsilon = 0.0001\n",
    "    \n",
    "    for model_version in model_versions:\n",
    "        print(f\"\\nProcessing bin-level PSI for Model Version: {model_version}\")\n",
    "        \n",
    "        mv_df = df_expanded[df_expanded[model_version_col] == model_version].copy()\n",
    "        \n",
    "        train_df = mv_df[mv_df[data_selection_col] == 'Train'].copy()\n",
    "        test_df = mv_df[mv_df[data_selection_col] == 'Test'].copy()\n",
    "        \n",
    "        calc_features = [col for col in df_expanded.columns if col.startswith('calc_')]\n",
    "        feature_list = [score_column] + calc_features\n",
    "        \n",
    "        feature_types = identify_feature_types(mv_df, feature_list)\n",
    "        binning_info = create_bins_for_features(\n",
    "            mv_df,\n",
    "            feature_types['numerical'],\n",
    "            feature_types['categorical'],\n",
    "            train_df\n",
    "        )\n",
    "        \n",
    "        test_months = sorted(test_df[month_col].unique())\n",
    "        \n",
    "        # ========== OVERALL BIN-LEVEL PSI ==========\n",
    "        for feature in feature_list:\n",
    "            if feature not in mv_df.columns:\n",
    "                continue\n",
    "            \n",
    "            mv_df[f'{feature}_binned'] = apply_binning(mv_df, feature, binning_info[feature])\n",
    "            train_baseline = mv_df[mv_df[data_selection_col] == 'Train'][f'{feature}_binned'].value_counts(normalize=True)\n",
    "            \n",
    "            for month in test_months:\n",
    "                month_data = mv_df[mv_df[month_col] == month]\n",
    "                if len(month_data) == 0:\n",
    "                    continue\n",
    "                \n",
    "                actual_dist = month_data[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                \n",
    "                base_count = mv_df[mv_df[data_selection_col] == 'Train'][account_id_col].nunique()\n",
    "                actual_count = month_data[account_id_col].nunique()\n",
    "                \n",
    "                all_bins = train_baseline.index.union(actual_dist.index)\n",
    "                \n",
    "                for bin_name in all_bins:\n",
    "                    expected_pct = train_baseline.get(bin_name, 0)\n",
    "                    actual_pct = actual_dist.get(bin_name, 0)\n",
    "                    \n",
    "                    expected_pct = epsilon if expected_pct == 0 else expected_pct\n",
    "                    actual_pct = epsilon if actual_pct == 0 else actual_pct\n",
    "                    \n",
    "                    bin_psi = (actual_pct - expected_pct) * np.log(actual_pct / expected_pct)\n",
    "                    \n",
    "                    bin_ranges = binning_info[feature]['bin_ranges']\n",
    "                    if bin_name in bin_ranges:\n",
    "                        bin_min = bin_ranges[bin_name]['min']\n",
    "                        bin_max = bin_ranges[bin_name]['max']\n",
    "                        bin_range = bin_ranges[bin_name]['range_str']\n",
    "                    else:\n",
    "                        bin_min = None\n",
    "                        bin_max = None\n",
    "                        bin_range = bin_name\n",
    "                    \n",
    "                    feature_category = 'Score' if feature == score_column else 'Expanded_Feature'\n",
    "                    \n",
    "                    all_results.append({\n",
    "                        'Model_Version': model_version,\n",
    "                        'Trench_Category': trench_category,\n",
    "                        'Model_Display_Name': model_display_name,\n",
    "                        'Feature': feature,\n",
    "                        'Feature_Type': binning_info[feature]['type'],\n",
    "                        'Feature_Category': feature_category,\n",
    "                        'Segment_Column': 'Overall',\n",
    "                        'Segment_Value': 'All',\n",
    "                        'Month': f\"{month}\",\n",
    "                        'Base_Month': 'Train',\n",
    "                        'Current_Month': month,\n",
    "                        'Base_Count': base_count,\n",
    "                        'Actual_Count': actual_count,\n",
    "                        'Bin': bin_name,\n",
    "                        'Bin_Range': bin_range,\n",
    "                        'Bin_Min': bin_min,\n",
    "                        'Bin_Max': bin_max,\n",
    "                        'Base_Percentage': train_baseline.get(bin_name, 0) * 100,\n",
    "                        'Actual_Percentage': actual_dist.get(bin_name, 0) * 100,\n",
    "                        'Bin_PSI': bin_psi\n",
    "                    })\n",
    "        \n",
    "        # ========== SEGMENT-WISE BIN-LEVEL PSI ==========\n",
    "        if segment_columns:\n",
    "            for segment_col in segment_columns:\n",
    "                if segment_col not in mv_df.columns:\n",
    "                    continue\n",
    "                \n",
    "                segments = mv_df[segment_col].dropna().unique()\n",
    "                \n",
    "                for segment_val in segments:\n",
    "                    segment_df = mv_df[mv_df[segment_col] == segment_val].copy()\n",
    "                    \n",
    "                    train_segment = segment_df[segment_df[data_selection_col] == 'Train']\n",
    "                    test_segment = segment_df[segment_df[data_selection_col] == 'Test']\n",
    "                    \n",
    "                    if len(train_segment) == 0 or len(test_segment) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    for feature in feature_list:\n",
    "                        if feature not in segment_df.columns:\n",
    "                            continue\n",
    "                        \n",
    "                        train_baseline = train_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                        \n",
    "                        for month in test_months:\n",
    "                            actual_segment = segment_df[segment_df[month_col] == month]\n",
    "                            \n",
    "                            if len(actual_segment) == 0:\n",
    "                                continue\n",
    "                            \n",
    "                            actual_dist = actual_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                            \n",
    "                            base_segment_count = train_segment[account_id_col].nunique()\n",
    "                            actual_segment_count = actual_segment[account_id_col].nunique()\n",
    "                            \n",
    "                            all_bins = train_baseline.index.union(actual_dist.index)\n",
    "                            \n",
    "                            for bin_name in all_bins:\n",
    "                                expected_pct = train_baseline.get(bin_name, 0)\n",
    "                                actual_pct = actual_dist.get(bin_name, 0)\n",
    "                                \n",
    "                                expected_pct = epsilon if expected_pct == 0 else expected_pct\n",
    "                                actual_pct = epsilon if actual_pct == 0 else actual_pct\n",
    "                                \n",
    "                                bin_psi = (actual_pct - expected_pct) * np.log(actual_pct / expected_pct)\n",
    "                                \n",
    "                                bin_ranges = binning_info[feature]['bin_ranges']\n",
    "                                if bin_name in bin_ranges:\n",
    "                                    bin_min = bin_ranges[bin_name]['min']\n",
    "                                    bin_max = bin_ranges[bin_name]['max']\n",
    "                                    bin_range = bin_ranges[bin_name]['range_str']\n",
    "                                else:\n",
    "                                    bin_min = None\n",
    "                                    bin_max = None\n",
    "                                    bin_range = bin_name\n",
    "                                \n",
    "                                feature_category = 'Score' if feature == score_column else 'Expanded_Feature'\n",
    "                                \n",
    "                                all_results.append({\n",
    "                                    'Model_Version': model_version,\n",
    "                                    'Trench_Category': trench_category,\n",
    "                                    'Model_Display_Name': model_display_name,\n",
    "                                    'Feature': feature,\n",
    "                                    'Feature_Type': binning_info[feature]['type'],\n",
    "                                    'Feature_Category': feature_category,\n",
    "                                    'Segment_Column': segment_col,\n",
    "                                    'Segment_Value': segment_val,\n",
    "                                    'Month': f\"{month}\",\n",
    "                                    'Base_Month': 'Train',\n",
    "                                    'Current_Month': month,\n",
    "                                    'Base_Count': base_segment_count,\n",
    "                                    'Actual_Count': actual_segment_count,\n",
    "                                    'Bin': bin_name,\n",
    "                                    'Bin_Range': bin_range,\n",
    "                                    'Bin_Min': bin_min,\n",
    "                                    'Bin_Max': bin_max,\n",
    "                                    'Base_Percentage': train_baseline.get(bin_name, 0) * 100,\n",
    "                                    'Actual_Percentage': actual_dist.get(bin_name, 0) * 100,\n",
    "                                    'Bin_PSI': bin_psi\n",
    "                                })\n",
    "    \n",
    "    return pd.DataFrame(all_results)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# USAGE EXAMPLE\n",
    "# ============================================================================\n",
    "\"\"\"\n",
    "# Assuming you have df with concatenated Train and Test data\n",
    "\n",
    "# Calculate Overall PSI (Overall + By Segments)\n",
    "psi_results = calculate_psi_by_model_version(\n",
    "    df=your_concatenated_df,\n",
    "    score_column='Alpha_cic_sil_score',\n",
    "    segment_columns=['new_loan_type', 'loan_product_type', 'osType'],\n",
    "    month_col='Application_month',\n",
    "    data_selection_col='Data_selection',\n",
    "    model_version_col='modelVersionId',\n",
    "    account_id_col='digitalLoanAccountId'\n",
    ")\n",
    "\n",
    "print(psi_results.head(20))\n",
    "print(f\"\\nTotal rows: {len(psi_results)}\")\n",
    "\n",
    "# View Overall results only\n",
    "overall_results = psi_results[psi_results['Segment_Column'] == 'Overall']\n",
    "print(overall_results)\n",
    "\n",
    "# View specific model version\n",
    "v1_results = psi_results[psi_results['Model_Version'] == 'v1']\n",
    "print(v1_results)\n",
    "\n",
    "# View specific segment\n",
    "loan_type_results = psi_results[psi_results['Segment_Column'] == 'new_loan_type']\n",
    "print(loan_type_results)\n",
    "\n",
    "# Save results\n",
    "psi_results.to_csv('psi_results_overall_and_segments.csv', index=False)\n",
    "\n",
    "# ---- For Bin-Level Details ----\n",
    "bin_psi_results = calculate_bin_level_psi_by_model_version(\n",
    "    df=your_concatenated_df,\n",
    "    score_column='Alpha_cic_sil_score',\n",
    "    segment_columns=['new_loan_type', 'loan_product_type', 'osType']\n",
    ")\n",
    "\n",
    "bin_psi_results.to_csv('bin_level_psi_results_overall_and_segments.csv', index=False)\n",
    "print(bin_psi_results.head(30))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "e41015b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# import json\n",
    "# from typing import List, Dict, Tuple\n",
    "# import warnings\n",
    "# warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "# def expand_calc_features(df):\n",
    "#     \"\"\"\n",
    "#     Expand the calcFeatures JSON column into separate columns and return the complete DataFrame.\n",
    "\n",
    "#     Parameters:\n",
    "#     df (pd.DataFrame): Input DataFrame with calcFeatures column containing JSON data\n",
    "\n",
    "#     Returns:\n",
    "#     pd.DataFrame: Expanded DataFrame with all original columns plus JSON features as separate columns\n",
    "#     \"\"\"\n",
    "#     df_expanded = df.copy()\n",
    "#     calc_features_list = []\n",
    "\n",
    "#     for idx, calc_features_str in enumerate(df['calcFeatures']):\n",
    "#         try:\n",
    "#             features_dict = json.loads(calc_features_str.replace(\"'\", '\"'))\n",
    "#             calc_features_list.append(features_dict)\n",
    "#         except (json.JSONDecodeError, AttributeError) as e:\n",
    "#             print(f\"Warning: Could not parse calcFeatures at index {idx}: {e}\")\n",
    "#             calc_features_list.append({})\n",
    "\n",
    "#     calc_features_df = pd.DataFrame(calc_features_list)\n",
    "#     calc_features_df = calc_features_df.add_prefix('calc_')\n",
    "\n",
    "#     df_expanded = df_expanded.reset_index(drop=True)\n",
    "#     calc_features_df = calc_features_df.reset_index(drop=True)\n",
    "\n",
    "#     result_df = pd.concat([df_expanded, calc_features_df], axis=1)\n",
    "#     return result_df\n",
    "\n",
    "\n",
    "# def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, List[str]]:\n",
    "#     \"\"\"\n",
    "#     Identify categorical and numerical features from the feature list.\n",
    "#     \"\"\"\n",
    "#     categorical_features = []\n",
    "#     numerical_features = []\n",
    "\n",
    "#     for feature in feature_list:\n",
    "#         if feature not in df.columns:\n",
    "#             continue\n",
    "\n",
    "#         if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "#             unique_vals = df[feature].nunique()\n",
    "#             if unique_vals < 15 and df[feature].dropna().apply(lambda x: x == int(x) if isinstance(x, (int, float)) else False).all():\n",
    "#                 categorical_features.append(feature)\n",
    "#             else:\n",
    "#                 numerical_features.append(feature)\n",
    "#         else:\n",
    "#             categorical_features.append(feature)\n",
    "\n",
    "#     return {\n",
    "#         'categorical': categorical_features,\n",
    "#         'numerical': numerical_features\n",
    "#     }\n",
    "\n",
    "\n",
    "# def create_bins_for_features(df: pd.DataFrame,\n",
    "#                              numerical_features: List[str],\n",
    "#                              categorical_features: List[str],\n",
    "#                              train_period_df: pd.DataFrame) -> Dict:\n",
    "#     \"\"\"\n",
    "#     Create bins for numerical features (deciles with fallback) and categorical features (top 6 + others)\n",
    "#     based on the training period data.\n",
    "#     \"\"\"\n",
    "#     binning_info = {}\n",
    "\n",
    "#     for feature in numerical_features:\n",
    "#         valid_data = train_period_df[feature].dropna()\n",
    "\n",
    "#         if len(valid_data) == 0:\n",
    "#             binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "#             continue\n",
    "\n",
    "#         bins = None\n",
    "#         bin_count = None\n",
    "\n",
    "#         # Try 10 bins (deciles)\n",
    "#         try:\n",
    "#             test_bins = np.percentile(valid_data, np.arange(0, 101, 10))\n",
    "#             test_bins = np.unique(test_bins)\n",
    "#             if len(test_bins) >= 11:\n",
    "#                 bins = test_bins\n",
    "#                 bin_count = 10\n",
    "#         except Exception as e:\n",
    "#             pass\n",
    "\n",
    "#         # If 10 bins not possible, try 5 bins\n",
    "#         if bins is None:\n",
    "#             try:\n",
    "#                 test_bins = np.percentile(valid_data, np.arange(0, 101, 20))\n",
    "#                 test_bins = np.unique(test_bins)\n",
    "#                 if len(test_bins) >= 6:\n",
    "#                     bins = test_bins\n",
    "#                     bin_count = 5\n",
    "#             except Exception as e:\n",
    "#                 pass\n",
    "\n",
    "#         # If 5 bins not possible, try 3 bins\n",
    "#         if bins is None:\n",
    "#             try:\n",
    "#                 test_bins = np.percentile(valid_data, [0, 33.33, 66.67, 100])\n",
    "#                 test_bins = np.unique(test_bins)\n",
    "#                 if len(test_bins) >= 4:\n",
    "#                     bins = test_bins\n",
    "#                     bin_count = 3\n",
    "#             except Exception as e:\n",
    "#                 pass\n",
    "\n",
    "#         # If still no bins possible, use equal distance bins of 5\n",
    "#         if bins is None:\n",
    "#             print(f\"Warning: Feature '{feature}' has insufficient variance - cannot create standard bins\")\n",
    "#             min_val = valid_data.min()\n",
    "#             max_val = valid_data.max()\n",
    "#             bins = np.linspace(min_val, max_val, 6)\n",
    "#             bins = np.unique(bins)\n",
    "#             bin_count = len(bins) - 1\n",
    "\n",
    "#             if bin_count == 1:\n",
    "#                 bins = np.array([min_val - 0.1, min_val, min_val + 0.1])\n",
    "#                 bin_count = 2\n",
    "\n",
    "#         bins = bins.copy()\n",
    "#         bins[0] = -np.inf\n",
    "#         bins[-1] = np.inf\n",
    "\n",
    "#         print(f\"Feature '{feature}': Created {bin_count} bins\")\n",
    "\n",
    "#         bin_ranges = {}\n",
    "#         for i in range(len(bins)-1):\n",
    "#             bin_name = f\"Bin_{i+1}\"\n",
    "#             bin_ranges[bin_name] = {\n",
    "#                 'min': bins[i],\n",
    "#                 'max': bins[i+1],\n",
    "#                 'range_str': f\"[{bins[i]:.2f}, {bins[i+1]:.2f}]\" if not np.isinf(bins[i]) and not np.isinf(bins[i+1]) else f\"({bins[i]}, {bins[i+1]})\"\n",
    "#             }\n",
    "\n",
    "#         binning_info[feature] = {\n",
    "#             'type': 'numerical',\n",
    "#             'bins': bins,\n",
    "#             'bin_ranges': bin_ranges,\n",
    "#             'bin_count': bin_count\n",
    "#         }\n",
    "\n",
    "#     # Categorical features\n",
    "#     for feature in categorical_features:\n",
    "#         value_counts = train_period_df[feature].value_counts()\n",
    "#         unique_categories = value_counts.index.tolist()\n",
    "\n",
    "#         if len(unique_categories) <= 6:\n",
    "#             top_categories = unique_categories\n",
    "#         else:\n",
    "#             top_categories = value_counts.nlargest(6).index.tolist()\n",
    "\n",
    "#         print(f\"Top categories for feature '{feature}': {top_categories}\")\n",
    "\n",
    "#         binning_info[feature] = {\n",
    "#             'type': 'categorical',\n",
    "#             'top_categories': top_categories,\n",
    "#             'bin_ranges': {}\n",
    "#         }\n",
    "\n",
    "#     return binning_info\n",
    "\n",
    "\n",
    "# def apply_binning(df: pd.DataFrame,\n",
    "#                   feature: str,\n",
    "#                   binning_info: Dict) -> pd.Series:\n",
    "#     \"\"\"\n",
    "#     Apply binning to a feature based on binning information.\n",
    "#     \"\"\"\n",
    "#     if binning_info['type'] == 'numerical':\n",
    "#         if binning_info['bins'] is None:\n",
    "#             return pd.Series(['Missing'] * len(df), index=df.index)\n",
    "\n",
    "#         bins = binning_info['bins']\n",
    "#         labels = [f\"Bin_{i+1}\" for i in range(len(bins)-1)]\n",
    "\n",
    "#         binned = pd.cut(df[feature],\n",
    "#                        bins=bins,\n",
    "#                        labels=labels,\n",
    "#                        include_lowest=True,\n",
    "#                        duplicates='drop')\n",
    "\n",
    "#         binned = binned.astype(str)\n",
    "#         binned[df[feature].isna()] = 'Missing'\n",
    "\n",
    "#         return binned\n",
    "\n",
    "#     else:\n",
    "#         top_cats = binning_info['top_categories']\n",
    "\n",
    "#         if pd.api.types.is_categorical_dtype(df[feature]):\n",
    "#             feature_data = df[feature].astype(str)\n",
    "#         else:\n",
    "#             feature_data = df[feature].astype(str)\n",
    "\n",
    "#         feature_data = feature_data.replace('nan', 'Missing')\n",
    "#         top_cats_str = [str(cat) for cat in top_cats]\n",
    "\n",
    "#         binned = feature_data.apply(lambda x: x if x in top_cats_str else ('Others' if x != 'Missing' else 'Missing'))\n",
    "\n",
    "#         return binned\n",
    "\n",
    "\n",
    "# def calculate_psi(expected_pct: pd.Series,\n",
    "#                   actual_pct: pd.Series,\n",
    "#                   epsilon: float = 0.0001) -> float:\n",
    "#     \"\"\"\n",
    "#     Calculate Population Stability Index with proper epsilon handling and renormalization.\n",
    "#     \"\"\"\n",
    "#     all_bins = expected_pct.index.union(actual_pct.index)\n",
    "#     expected_pct = expected_pct.reindex(all_bins, fill_value=0)\n",
    "#     actual_pct = actual_pct.reindex(all_bins, fill_value=0)\n",
    "\n",
    "#     expected_pct = expected_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "#     actual_pct = actual_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "\n",
    "#     expected_pct = expected_pct / expected_pct.sum()\n",
    "#     actual_pct = actual_pct / actual_pct.sum()\n",
    "\n",
    "#     psi_value = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "\n",
    "#     return psi_value\n",
    "\n",
    "\n",
    "# def calculate_psi_by_model_version(df: pd.DataFrame,\n",
    "#                                    score_column: str,\n",
    "#                                    segment_columns: List[str] = None,\n",
    "#                                    month_col: str = 'Application_month',\n",
    "#                                    data_selection_col: str = 'Data_selection',\n",
    "#                                    model_version_col: str = 'modelVersionId',\n",
    "#                                    account_id_col: str = 'digitalLoanAccountId') -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Calculate PSI for each model version by comparing Train vs Test periods.\n",
    "#     Expands calcFeatures for each model version and calculates PSI for:\n",
    "#     1. Overall score\n",
    "#     2. Each expanded feature\n",
    "#     3. By segments (optional)\n",
    "\n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     df : pd.DataFrame\n",
    "#         Input dataframe with concatenated Train and Test data\n",
    "#     score_column : str\n",
    "#         Name of the score column (e.g., 'Alpha_cic_sil_score')\n",
    "#     segment_columns : List[str]\n",
    "#         List of segment columns (e.g., ['new_loan_type', 'loan_product_type', 'osType'])\n",
    "#     month_col : str\n",
    "#         Name of month column\n",
    "#     data_selection_col : str\n",
    "#         Name of data selection column (Train/Test)\n",
    "#     model_version_col : str\n",
    "#         Name of model version column\n",
    "#     account_id_col : str\n",
    "#         Name of account ID column\n",
    "\n",
    "#     Returns:\n",
    "#     --------\n",
    "#     pd.DataFrame with PSI values for overall and by features\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if segment_columns is None:\n",
    "#         segment_columns = []\n",
    "    \n",
    "#     # Expand calcFeatures for entire dataset\n",
    "#     print(\"Expanding calcFeatures...\")\n",
    "#     df_expanded = expand_calc_features(df)\n",
    "    \n",
    "#     # Get unique model versions\n",
    "#     model_versions = sorted(df_expanded[model_version_col].unique())\n",
    "#     print(f\"Found model versions: {model_versions}\\n\")\n",
    "    \n",
    "#     all_results = []\n",
    "    \n",
    "#     # Process each model version\n",
    "#     for model_version in model_versions:\n",
    "#         print(f\"{'='*100}\")\n",
    "#         print(f\"Processing Model Version: {model_version}\")\n",
    "#         print(f\"{'='*100}\")\n",
    "        \n",
    "#         # Filter data for current model version\n",
    "#         mv_df = df_expanded[df_expanded[model_version_col] == model_version].copy()\n",
    "        \n",
    "#         # Split into train and test\n",
    "#         train_df = mv_df[mv_df[data_selection_col] == 'Train'].copy()\n",
    "#         test_df = mv_df[mv_df[data_selection_col] == 'Test'].copy()\n",
    "        \n",
    "#         if len(train_df) == 0:\n",
    "#             print(f\"Warning: No training data for model version {model_version}\\n\")\n",
    "#             continue\n",
    "        \n",
    "#         if len(test_df) == 0:\n",
    "#             print(f\"Warning: No test data for model version {model_version}\\n\")\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"Train records: {len(train_df)}, Test records: {len(test_df)}\")\n",
    "#         print(f\"Training period: {train_df[month_col].min()} to {train_df[month_col].max()}\")\n",
    "#         print(f\"Test period: {test_df[month_col].min()} to {test_df[month_col].max()}\")\n",
    "        \n",
    "#         # Get all expanded features (calc_ prefixed columns)\n",
    "#         calc_features = [col for col in df_expanded.columns if col.startswith('calc_')]\n",
    "        \n",
    "#         # Add score column to feature list\n",
    "#         feature_list = [score_column] + calc_features\n",
    "        \n",
    "#         print(f\"Score column: {score_column}\")\n",
    "#         print(f\"Number of expanded features: {len(calc_features)}\\n\")\n",
    "        \n",
    "#         # Identify feature types\n",
    "#         feature_types = identify_feature_types(mv_df, feature_list)\n",
    "        \n",
    "#         # Create binning strategy based on training period\n",
    "#         binning_info = create_bins_for_features(\n",
    "#             mv_df,\n",
    "#             feature_types['numerical'],\n",
    "#             feature_types['categorical'],\n",
    "#             train_df\n",
    "#         )\n",
    "        \n",
    "#         # Get sorted test months\n",
    "#         test_months = sorted(test_df[month_col].unique())\n",
    "        \n",
    "#         # ========== OVERALL PSI ==========\n",
    "#         print(f\"\\nCalculating Overall PSI...\")\n",
    "#         for feature in feature_list:\n",
    "#             if feature not in mv_df.columns:\n",
    "#                 continue\n",
    "            \n",
    "#             # Apply binning to entire dataset\n",
    "#             mv_df[f'{feature}_binned'] = apply_binning(mv_df, feature, binning_info[feature])\n",
    "            \n",
    "#             # Get training period distribution (baseline)\n",
    "#             train_baseline = mv_df[mv_df[data_selection_col] == 'Train'][f'{feature}_binned'].value_counts(normalize=True)\n",
    "            \n",
    "#             # Calculate PSI for each test month\n",
    "#             for month in test_months:\n",
    "#                 month_data = mv_df[mv_df[month_col] == month]\n",
    "                \n",
    "#                 if len(month_data) == 0:\n",
    "#                     continue\n",
    "                \n",
    "#                 actual_dist = month_data[f'{feature}_binned'].value_counts(normalize=True)\n",
    "#                 psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "                \n",
    "#                 # Count distinct accounts\n",
    "#                 base_count = mv_df[mv_df[data_selection_col] == 'Train'][account_id_col].nunique()\n",
    "#                 actual_count = month_data[account_id_col].nunique()\n",
    "                \n",
    "#                 # Calculate average percentages\n",
    "#                 expected_avg_pct = train_baseline.mean() * 100\n",
    "#                 actual_avg_pct = actual_dist.mean() * 100\n",
    "                \n",
    "#                 # Determine feature category\n",
    "#                 feature_category = 'Score' if feature == score_column else 'Expanded_Feature'\n",
    "                \n",
    "#                 results_row = {\n",
    "#                     'Model_Version': model_version,\n",
    "#                     'Feature': feature,\n",
    "#                     'Feature_Type': binning_info[feature]['type'],\n",
    "#                     'Feature_Category': feature_category,\n",
    "#                     'Segment_Column': 'Overall',\n",
    "#                     'Segment_Value': 'All',\n",
    "#                     'Month': f\"{month}\",\n",
    "#                     'Base_Month': 'Train',\n",
    "#                     'Current_Month': month,\n",
    "#                     'Base_Count': base_count,\n",
    "#                     'Actual_Count': actual_count,\n",
    "#                     'Expected_Percentage': expected_avg_pct,\n",
    "#                     'Actual_Percentage': actual_avg_pct,\n",
    "#                     'PSI': psi_value\n",
    "#                 }\n",
    "                \n",
    "#                 all_results.append(results_row)\n",
    "        \n",
    "#         # ========== SEGMENT-WISE PSI ==========\n",
    "#         if segment_columns:\n",
    "#             print(f\"\\nCalculating Segment-wise PSI...\")\n",
    "#             for segment_col in segment_columns:\n",
    "#                 if segment_col not in mv_df.columns:\n",
    "#                     print(f\"Warning: Segment column '{segment_col}' not found\")\n",
    "#                     continue\n",
    "                \n",
    "#                 segments = mv_df[segment_col].dropna().unique()\n",
    "#                 print(f\"\\nSegment Column: {segment_col} | Values: {list(segments)}\")\n",
    "                \n",
    "#                 for segment_val in segments:\n",
    "#                     segment_df = mv_df[mv_df[segment_col] == segment_val].copy()\n",
    "                    \n",
    "#                     # Get train and test data for this segment\n",
    "#                     train_segment = segment_df[segment_df[data_selection_col] == 'Train']\n",
    "#                     test_segment = segment_df[segment_df[data_selection_col] == 'Test']\n",
    "                    \n",
    "#                     if len(train_segment) == 0 or len(test_segment) == 0:\n",
    "#                         continue\n",
    "                    \n",
    "#                     for feature in feature_list:\n",
    "#                         if feature not in segment_df.columns:\n",
    "#                             continue\n",
    "                        \n",
    "#                         # Get baseline from train segment\n",
    "#                         train_baseline = train_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                        \n",
    "#                         # Calculate PSI for each test month in this segment\n",
    "#                         for month in test_months:\n",
    "#                             actual_segment = segment_df[segment_df[month_col] == month]\n",
    "                            \n",
    "#                             if len(actual_segment) == 0:\n",
    "#                                 continue\n",
    "                            \n",
    "#                             actual_dist = actual_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "#                             psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "                            \n",
    "#                             # Count distinct accounts for segment\n",
    "#                             base_segment_count = train_segment[account_id_col].nunique()\n",
    "#                             actual_segment_count = actual_segment[account_id_col].nunique()\n",
    "                            \n",
    "#                             # Calculate average percentages\n",
    "#                             expected_avg_pct = train_baseline.mean() * 100\n",
    "#                             actual_avg_pct = actual_dist.mean() * 100\n",
    "                            \n",
    "#                             feature_category = 'Score' if feature == score_column else 'Expanded_Feature'\n",
    "                            \n",
    "#                             results_row = {\n",
    "#                                 'Model_Version': model_version,\n",
    "#                                 'Feature': feature,\n",
    "#                                 'Feature_Type': binning_info[feature]['type'],\n",
    "#                                 'Feature_Category': feature_category,\n",
    "#                                 'Segment_Column': segment_col,\n",
    "#                                 'Segment_Value': segment_val,\n",
    "#                                 'Month': f\"{month}\",\n",
    "#                                 'Base_Month': 'Train',\n",
    "#                                 'Current_Month': month,\n",
    "#                                 'Base_Count': base_segment_count,\n",
    "#                                 'Actual_Count': actual_segment_count,\n",
    "#                                 'Expected_Percentage': expected_avg_pct,\n",
    "#                                 'Actual_Percentage': actual_avg_pct,\n",
    "#                                 'PSI': psi_value\n",
    "#                             }\n",
    "                            \n",
    "#                             all_results.append(results_row)\n",
    "        \n",
    "#         print(f\"Completed processing for Model Version: {model_version}\\n\")\n",
    "    \n",
    "#     results_df = pd.DataFrame(all_results)\n",
    "#     return results_df\n",
    "\n",
    "\n",
    "# def calculate_bin_level_psi_by_model_version(df: pd.DataFrame,\n",
    "#                                              score_column: str,\n",
    "#                                              segment_columns: List[str] = None,\n",
    "#                                              month_col: str = 'Application_month',\n",
    "#                                              data_selection_col: str = 'Data_selection',\n",
    "#                                              model_version_col: str = 'modelVersionId',\n",
    "#                                              account_id_col: str = 'digitalLoanAccountId') -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Calculate bin-level PSI for each model version.\n",
    "#     Provides detailed breakdown by bins/categories.\n",
    "#     \"\"\"\n",
    "    \n",
    "#     if segment_columns is None:\n",
    "#         segment_columns = []\n",
    "    \n",
    "#     print(\"Expanding calcFeatures for bin-level analysis...\")\n",
    "#     df_expanded = expand_calc_features(df)\n",
    "    \n",
    "#     model_versions = sorted(df_expanded[model_version_col].unique())\n",
    "    \n",
    "#     all_results = []\n",
    "#     epsilon = 0.0001\n",
    "    \n",
    "#     for model_version in model_versions:\n",
    "#         print(f\"\\nProcessing bin-level PSI for Model Version: {model_version}\")\n",
    "        \n",
    "#         mv_df = df_expanded[df_expanded[model_version_col] == model_version].copy()\n",
    "        \n",
    "#         train_df = mv_df[mv_df[data_selection_col] == 'Train'].copy()\n",
    "#         test_df = mv_df[mv_df[data_selection_col] == 'Test'].copy()\n",
    "        \n",
    "#         if len(train_df) == 0 or len(test_df) == 0:\n",
    "#             continue\n",
    "        \n",
    "#         calc_features = [col for col in df_expanded.columns if col.startswith('calc_')]\n",
    "#         feature_list = [score_column] + calc_features\n",
    "        \n",
    "#         feature_types = identify_feature_types(mv_df, feature_list)\n",
    "#         binning_info = create_bins_for_features(\n",
    "#             mv_df,\n",
    "#             feature_types['numerical'],\n",
    "#             feature_types['categorical'],\n",
    "#             train_df\n",
    "#         )\n",
    "        \n",
    "#         test_months = sorted(test_df[month_col].unique())\n",
    "        \n",
    "#         # ========== OVERALL BIN-LEVEL PSI ==========\n",
    "#         for feature in feature_list:\n",
    "#             if feature not in mv_df.columns:\n",
    "#                 continue\n",
    "            \n",
    "#             mv_df[f'{feature}_binned'] = apply_binning(mv_df, feature, binning_info[feature])\n",
    "#             train_baseline = mv_df[mv_df[data_selection_col] == 'Train'][f'{feature}_binned'].value_counts(normalize=True)\n",
    "            \n",
    "#             for month in test_months:\n",
    "#                 month_data = mv_df[mv_df[month_col] == month]\n",
    "#                 if len(month_data) == 0:\n",
    "#                     continue\n",
    "                \n",
    "#                 actual_dist = month_data[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                \n",
    "#                 base_count = mv_df[mv_df[data_selection_col] == 'Train'][account_id_col].nunique()\n",
    "#                 actual_count = month_data[account_id_col].nunique()\n",
    "                \n",
    "#                 all_bins = train_baseline.index.union(actual_dist.index)\n",
    "                \n",
    "#                 for bin_name in all_bins:\n",
    "#                     expected_pct = train_baseline.get(bin_name, 0)\n",
    "#                     actual_pct = actual_dist.get(bin_name, 0)\n",
    "                    \n",
    "#                     expected_pct = epsilon if expected_pct == 0 else expected_pct\n",
    "#                     actual_pct = epsilon if actual_pct == 0 else actual_pct\n",
    "                    \n",
    "#                     bin_psi = (actual_pct - expected_pct) * np.log(actual_pct / expected_pct)\n",
    "                    \n",
    "#                     bin_ranges = binning_info[feature]['bin_ranges']\n",
    "#                     if bin_name in bin_ranges:\n",
    "#                         bin_min = bin_ranges[bin_name]['min']\n",
    "#                         bin_max = bin_ranges[bin_name]['max']\n",
    "#                         bin_range = bin_ranges[bin_name]['range_str']\n",
    "#                     else:\n",
    "#                         bin_min = None\n",
    "#                         bin_max = None\n",
    "#                         bin_range = bin_name\n",
    "                    \n",
    "#                     feature_category = 'Score' if feature == score_column else 'Expanded_Feature'\n",
    "                    \n",
    "#                     all_results.append({\n",
    "#                         'Model_Version': model_version,\n",
    "#                         'Feature': feature,\n",
    "#                         'Feature_Type': binning_info[feature]['type'],\n",
    "#                         'Feature_Category': feature_category,\n",
    "#                         'Segment_Column': 'Overall',\n",
    "#                         'Segment_Value': 'All',\n",
    "#                         'Month': f\"{month}\",\n",
    "#                         'Base_Month': 'Train',\n",
    "#                         'Current_Month': month,\n",
    "#                         'Base_Count': base_count,\n",
    "#                         'Actual_Count': actual_count,\n",
    "#                         'Bin': bin_name,\n",
    "#                         'Bin_Range': bin_range,\n",
    "#                         'Bin_Min': bin_min,\n",
    "#                         'Bin_Max': bin_max,\n",
    "#                         'Base_Percentage': train_baseline.get(bin_name, 0) * 100,\n",
    "#                         'Actual_Percentage': actual_dist.get(bin_name, 0) * 100,\n",
    "#                         'Bin_PSI': bin_psi\n",
    "#                     })\n",
    "        \n",
    "#         # ========== SEGMENT-WISE BIN-LEVEL PSI ==========\n",
    "#         if segment_columns:\n",
    "#             for segment_col in segment_columns:\n",
    "#                 if segment_col not in mv_df.columns:\n",
    "#                     continue\n",
    "                \n",
    "#                 segments = mv_df[segment_col].dropna().unique()\n",
    "                \n",
    "#                 for segment_val in segments:\n",
    "#                     segment_df = mv_df[mv_df[segment_col] == segment_val].copy()\n",
    "                    \n",
    "#                     train_segment = segment_df[segment_df[data_selection_col] == 'Train']\n",
    "#                     test_segment = segment_df[segment_df[data_selection_col] == 'Test']\n",
    "                    \n",
    "#                     if len(train_segment) == 0 or len(test_segment) == 0:\n",
    "#                         continue\n",
    "                    \n",
    "#                     for feature in feature_list:\n",
    "#                         if feature not in segment_df.columns:\n",
    "#                             continue\n",
    "                        \n",
    "#                         train_baseline = train_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                        \n",
    "#                         for month in test_months:\n",
    "#                             actual_segment = segment_df[segment_df[month_col] == month]\n",
    "                            \n",
    "#                             if len(actual_segment) == 0:\n",
    "#                                 continue\n",
    "                            \n",
    "#                             actual_dist = actual_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "                            \n",
    "#                             base_segment_count = train_segment[account_id_col].nunique()\n",
    "#                             actual_segment_count = actual_segment[account_id_col].nunique()\n",
    "                            \n",
    "#                             all_bins = train_baseline.index.union(actual_dist.index)\n",
    "                            \n",
    "#                             for bin_name in all_bins:\n",
    "#                                 expected_pct = train_baseline.get(bin_name, 0)\n",
    "#                                 actual_pct = actual_dist.get(bin_name, 0)\n",
    "                                \n",
    "#                                 expected_pct = epsilon if expected_pct == 0 else expected_pct\n",
    "#                                 actual_pct = epsilon if actual_pct == 0 else actual_pct\n",
    "                                \n",
    "#                                 bin_psi = (actual_pct - expected_pct) * np.log(actual_pct / expected_pct)\n",
    "                                \n",
    "#                                 bin_ranges = binning_info[feature]['bin_ranges']\n",
    "#                                 if bin_name in bin_ranges:\n",
    "#                                     bin_min = bin_ranges[bin_name]['min']\n",
    "#                                     bin_max = bin_ranges[bin_name]['max']\n",
    "#                                     bin_range = bin_ranges[bin_name]['range_str']\n",
    "#                                 else:\n",
    "#                                     bin_min = None\n",
    "#                                     bin_max = None\n",
    "#                                     bin_range = bin_name\n",
    "                                \n",
    "#                                 feature_category = 'Score' if feature == score_column else 'Expanded_Feature'\n",
    "                                \n",
    "#                                 all_results.append({\n",
    "#                                     'Model_Version': model_version,\n",
    "#                                     'Feature': feature,\n",
    "#                                     'Feature_Type': binning_info[feature]['type'],\n",
    "#                                     'Feature_Category': feature_category,\n",
    "#                                     'Segment_Column': segment_col,\n",
    "#                                     'Segment_Value': segment_val,\n",
    "#                                     'Month': f\"{month}\",\n",
    "#                                     'Base_Month': 'Train',\n",
    "#                                     'Current_Month': month,\n",
    "#                                     'Base_Count': base_segment_count,\n",
    "#                                     'Actual_Count': actual_segment_count,\n",
    "#                                     'Bin': bin_name,\n",
    "#                                     'Bin_Range': bin_range,\n",
    "#                                     'Bin_Min': bin_min,\n",
    "#                                     'Bin_Max': bin_max,\n",
    "#                                     'Base_Percentage': train_baseline.get(bin_name, 0) * 100,\n",
    "#                                     'Actual_Percentage': actual_dist.get(bin_name, 0) * 100,\n",
    "#                                     'Bin_PSI': bin_psi\n",
    "#                                 })\n",
    "    \n",
    "#     return pd.DataFrame(all_results)\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # USAGE EXAMPLE\n",
    "# # ============================================================================\n",
    "# \"\"\"\n",
    "# # Assuming you have df with concatenated Train and Test data\n",
    "\n",
    "# # Calculate Overall PSI (Overall + By Segments)\n",
    "# psi_results = calculate_psi_by_model_version(\n",
    "#     df=your_concatenated_df,\n",
    "#     score_column='Alpha_cic_sil_score',\n",
    "#     segment_columns=['new_loan_type', 'loan_product_type', 'osType'],\n",
    "#     month_col='Application_month',\n",
    "#     data_selection_col='Data_selection',\n",
    "#     model_version_col='modelVersionId',\n",
    "#     account_id_col='digitalLoanAccountId'\n",
    "# )\n",
    "\n",
    "# print(psi_results.head(20))\n",
    "# print(f\"\\nTotal rows: {len(psi_results)}\")\n",
    "\n",
    "# # View Overall results only\n",
    "# overall_results = psi_results[psi_results['Segment_Column'] == 'Overall']\n",
    "# print(overall_results)\n",
    "\n",
    "# # View specific model version\n",
    "# v1_results = psi_results[psi_results['Model_Version'] == 'v1']\n",
    "# print(v1_results)\n",
    "\n",
    "# # View specific segment\n",
    "# loan_type_results = psi_results[psi_results['Segment_Column'] == 'new_loan_type']\n",
    "# print(loan_type_results)\n",
    "\n",
    "# # Save results\n",
    "# psi_results.to_csv('psi_results_overall_and_segments.csv', index=False)\n",
    "\n",
    "# # ---- For Bin-Level Details ----\n",
    "# bin_psi_results = calculate_bin_level_psi_by_model_version(\n",
    "#     df=your_concatenated_df,\n",
    "#     score_column='Alpha_cic_sil_score',\n",
    "#     segment_columns=['new_loan_type', 'loan_product_type', 'osType']\n",
    "# )\n",
    "\n",
    "# bin_psi_results.to_csv('bin_level_psi_results_overall_and_segments.csv', index=False)\n",
    "# print(bin_psi_results.head(30))\n",
    "# \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed1286b",
   "metadata": {},
   "source": [
    "# create_comprehensive_psi_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "970bfc80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def expand_calc_features(df):\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns.\n",
    "    \"\"\"\n",
    "    df_expanded = df.copy()\n",
    "    calc_features_list = []\n",
    "\n",
    "    for idx, calc_features_str in enumerate(df['calcFeatures']):\n",
    "        try:\n",
    "            features_dict = json.loads(calc_features_str.replace(\"'\", '\"'))\n",
    "            calc_features_list.append(features_dict)\n",
    "        except (json.JSONDecodeError, AttributeError) as e:\n",
    "            print(f\"Warning: Could not parse calcFeatures at index {idx}: {e}\")\n",
    "            calc_features_list.append({})\n",
    "\n",
    "    calc_features_df = pd.DataFrame(calc_features_list)\n",
    "    calc_features_df = calc_features_df.add_prefix('calc_')\n",
    "\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    calc_features_df = calc_features_df.reset_index(drop=True)\n",
    "\n",
    "    result_df = pd.concat([df_expanded, calc_features_df], axis=1)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Identify categorical and numerical features.\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "\n",
    "    for feature in feature_list:\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            unique_vals = df[feature].nunique()\n",
    "            if unique_vals < 15 and df[feature].dropna().apply(\n",
    "                lambda x: x == int(x) if isinstance(x, (int, float)) else False\n",
    "            ).all():\n",
    "                categorical_features.append(feature)\n",
    "            else:\n",
    "                numerical_features.append(feature)\n",
    "        else:\n",
    "            categorical_features.append(feature)\n",
    "\n",
    "    return {\n",
    "        'categorical': categorical_features,\n",
    "        'numerical': numerical_features\n",
    "    }\n",
    "\n",
    "\n",
    "def create_bins_for_features(df: pd.DataFrame,\n",
    "                             numerical_features: List[str],\n",
    "                             categorical_features: List[str],\n",
    "                             train_period_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create bins for numerical features (deciles) and categorical features (top 9 + others).\n",
    "    \"\"\"\n",
    "    binning_info = {}\n",
    "\n",
    "    # Numerical features - Deciles\n",
    "    for feature in numerical_features:\n",
    "        valid_data = train_period_df[feature].dropna()\n",
    "\n",
    "        if len(valid_data) == 0:\n",
    "            binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "            continue\n",
    "\n",
    "        bins = None\n",
    "        bin_count = None\n",
    "\n",
    "        # Try 10 bins (deciles)\n",
    "        try:\n",
    "            test_bins = np.percentile(valid_data, np.arange(0, 101, 10))\n",
    "            test_bins = np.unique(test_bins)\n",
    "            if len(test_bins) >= 11:\n",
    "                bins = test_bins\n",
    "                bin_count = 10\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        # Fallback to 5 bins\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, np.arange(0, 101, 20))\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 6:\n",
    "                    bins = test_bins\n",
    "                    bin_count = 5\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Fallback to 3 bins\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, [0, 33.33, 66.67, 100])\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 4:\n",
    "                    bins = test_bins\n",
    "                    bin_count = 3\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Equal distance bins\n",
    "        if bins is None:\n",
    "            print(f\"Warning: Feature '{feature}' has insufficient variance\")\n",
    "            min_val = valid_data.min()\n",
    "            max_val = valid_data.max()\n",
    "            bins = np.linspace(min_val, max_val, 6)\n",
    "            bins = np.unique(bins)\n",
    "            bin_count = len(bins) - 1\n",
    "\n",
    "            if bin_count == 1:\n",
    "                bins = np.array([min_val - 0.1, min_val, min_val + 0.1])\n",
    "                bin_count = 2\n",
    "\n",
    "        bins = bins.copy()\n",
    "        bins[0] = -np.inf\n",
    "        bins[-1] = np.inf\n",
    "\n",
    "        print(f\"Feature '{feature}': Created {bin_count} bins\")\n",
    "\n",
    "        bin_ranges = {}\n",
    "        for i in range(len(bins)-1):\n",
    "            bin_name = f\"Bin_{i+1}\"\n",
    "            bin_ranges[bin_name] = {\n",
    "                'min': bins[i],\n",
    "                'max': bins[i+1],\n",
    "                'range_str': f\"[{bins[i]:.2f}, {bins[i+1]:.2f}]\" if not np.isinf(bins[i]) and not np.isinf(bins[i+1]) else f\"({bins[i]}, {bins[i+1]})\"\n",
    "            }\n",
    "\n",
    "        binning_info[feature] = {\n",
    "            'type': 'numerical',\n",
    "            'bins': bins,\n",
    "            'bin_ranges': bin_ranges,\n",
    "            'bin_count': bin_count\n",
    "        }\n",
    "\n",
    "    # Categorical features - Top 9 + Others\n",
    "    for feature in categorical_features:\n",
    "        value_counts = train_period_df[feature].value_counts()\n",
    "        unique_categories = value_counts.index.tolist()\n",
    "\n",
    "        if len(unique_categories) <= 9:\n",
    "            top_categories = unique_categories\n",
    "        else:\n",
    "            top_categories = value_counts.nlargest(9).index.tolist()\n",
    "\n",
    "        print(f\"Feature '{feature}': Using {len(top_categories)} top categories (total unique: {len(unique_categories)})\")\n",
    "\n",
    "        binning_info[feature] = {\n",
    "            'type': 'categorical',\n",
    "            'top_categories': top_categories,\n",
    "            'bin_ranges': {}\n",
    "        }\n",
    "\n",
    "    return binning_info\n",
    "\n",
    "\n",
    "def apply_binning(df: pd.DataFrame,\n",
    "                  feature: str,\n",
    "                  binning_info: Dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply binning to a feature.\n",
    "    \"\"\"\n",
    "    if binning_info['type'] == 'numerical':\n",
    "        if binning_info['bins'] is None:\n",
    "            return pd.Series(['Missing'] * len(df), index=df.index)\n",
    "\n",
    "        bins = binning_info['bins']\n",
    "        labels = [f\"Bin_{i+1}\" for i in range(len(bins)-1)]\n",
    "\n",
    "        binned = pd.cut(df[feature],\n",
    "                       bins=bins,\n",
    "                       labels=labels,\n",
    "                       include_lowest=True,\n",
    "                       duplicates='drop')\n",
    "\n",
    "        binned = binned.astype(str)\n",
    "        binned[df[feature].isna()] = 'Missing'\n",
    "\n",
    "        return binned\n",
    "\n",
    "    else:  # categorical\n",
    "        top_cats = binning_info['top_categories']\n",
    "        feature_data = df[feature].astype(str)\n",
    "        feature_data = feature_data.replace('nan', 'Missing')\n",
    "        top_cats_str = [str(cat) for cat in top_cats]\n",
    "\n",
    "        binned = feature_data.apply(\n",
    "            lambda x: x if x in top_cats_str else ('Others' if x != 'Missing' else 'Missing')\n",
    "        )\n",
    "\n",
    "        return binned\n",
    "\n",
    "\n",
    "def calculate_psi(expected_pct: pd.Series,\n",
    "                  actual_pct: pd.Series,\n",
    "                  epsilon: float = 0.0001) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index.\n",
    "    \"\"\"\n",
    "    all_bins = expected_pct.index.union(actual_pct.index)\n",
    "    expected_pct = expected_pct.reindex(all_bins, fill_value=0)\n",
    "    actual_pct = actual_pct.reindex(all_bins, fill_value=0)\n",
    "\n",
    "    expected_pct = expected_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "    actual_pct = actual_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "\n",
    "    expected_pct = expected_pct / expected_pct.sum()\n",
    "    actual_pct = actual_pct / actual_pct.sum()\n",
    "\n",
    "    psi_value = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "\n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def get_valid_model_trench_combinations(df: pd.DataFrame) -> List[Tuple[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Get all valid combinations of modelVersionId and trenchCategory.\n",
    "    Returns list of tuples: (modelVersionId, [trenchCategory_list])\n",
    "    \"\"\"\n",
    "    combinations_list = []\n",
    "    \n",
    "    for model_id in df['modelVersionId'].unique():\n",
    "        model_df = df[df['modelVersionId'] == model_id]\n",
    "        trenches = model_df['trenchCategory'].dropna().unique().tolist()\n",
    "        \n",
    "        if trenches:\n",
    "            combinations_list.append((model_id, trenches))\n",
    "    \n",
    "    return combinations_list\n",
    "\n",
    "\n",
    "def generate_segment_combinations(model_trenches_list: List[Tuple]) -> List[Tuple[str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Generate all combinations of models and trenches.\n",
    "    Returns (modelVersionId, [trench_combinations])\n",
    "    \"\"\"\n",
    "    all_combinations = []\n",
    "    \n",
    "    for model_id, trenches in model_trenches_list:\n",
    "        # Single trenches\n",
    "        for trench in trenches:\n",
    "            all_combinations.append((model_id, [trench]))\n",
    "        \n",
    "        # Multiple trench combinations\n",
    "        for r in range(2, len(trenches) + 1):\n",
    "            for combo in combinations(trenches, r):\n",
    "                all_combinations.append((model_id, list(combo)))\n",
    "    \n",
    "    return all_combinations\n",
    "\n",
    "\n",
    "def calculate_psi_for_segments(df: pd.DataFrame,\n",
    "                               feature: str,\n",
    "                               binning_info: Dict,\n",
    "                               segment_def: Dict,\n",
    "                               account_id_col: str = 'digitalLoanAccountId',\n",
    "                               data_selection_col: str = 'Data_selection',\n",
    "                               month_col: str = 'Application_month') -> Dict:\n",
    "    \"\"\"\n",
    "    Calculate PSI for a specific feature and segment definition.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Apply binning\n",
    "    df[f'{feature}_binned'] = apply_binning(df, feature, binning_info[feature])\n",
    "    \n",
    "    # Get train baseline\n",
    "    train_mask = df[data_selection_col] == 'Train'\n",
    "    \n",
    "    # Apply segment filters\n",
    "    segment_mask = train_mask.copy()\n",
    "    for col, values in segment_def.items():\n",
    "        if col in df.columns:\n",
    "            if isinstance(values, list):\n",
    "                segment_mask = segment_mask & (df[col].isin(values))\n",
    "            else:\n",
    "                segment_mask = segment_mask & (df[col] == values)\n",
    "    \n",
    "    train_segment = df[segment_mask]\n",
    "    \n",
    "    if len(train_segment) == 0:\n",
    "        return []\n",
    "    \n",
    "    train_baseline = train_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "    \n",
    "    # Get test months\n",
    "    test_df = df[~train_mask]\n",
    "    test_months = sorted(test_df[month_col].unique())\n",
    "    \n",
    "    for month in test_months:\n",
    "        # Apply segment filters to test data\n",
    "        test_mask = (df[month_col] == month)\n",
    "        for col, values in segment_def.items():\n",
    "            if col in df.columns:\n",
    "                if isinstance(values, list):\n",
    "                    test_mask = test_mask & (df[col].isin(values))\n",
    "                else:\n",
    "                    test_mask = test_mask & (df[col] == values)\n",
    "        \n",
    "        actual_segment = df[test_mask]\n",
    "        \n",
    "        if len(actual_segment) == 0:\n",
    "            continue\n",
    "        \n",
    "        actual_dist = actual_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "        psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "        \n",
    "        expected_avg_pct = train_baseline.mean() * 100\n",
    "        actual_avg_pct = actual_dist.mean() * 100\n",
    "        \n",
    "        base_count = train_segment[account_id_col].nunique()\n",
    "        actual_count = actual_segment[account_id_col].nunique()\n",
    "        \n",
    "        results.append({\n",
    "            'Feature': feature,\n",
    "            'Feature_Type': binning_info[feature]['type'],\n",
    "            'Month': month,\n",
    "            'Base_Month': 'Train',\n",
    "            'Base_Count': base_count,\n",
    "            'Actual_Count': actual_count,\n",
    "            'Expected_Percentage': expected_avg_pct,\n",
    "            'Actual_Percentage': actual_avg_pct,\n",
    "            'PSI': psi_value\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_comprehensive_psi_report(df: pd.DataFrame,\n",
    "                                    excluded_features: List[str] = None,\n",
    "                                    account_id_col: str = 'digitalLoanAccountId',\n",
    "                                    data_selection_col: str = 'Data_selection',\n",
    "                                    month_col: str = 'Application_month') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive PSI report with intelligent segmentation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Concatenated train+test dataframe (already expanded with calcFeatures)\n",
    "    excluded_features : List[str]\n",
    "        Features to exclude (e.g., ['digitalLoanAccountId', 'customerId', 'crifapplicationid'])\n",
    "    account_id_col : str\n",
    "        Column name for counting distinct accounts\n",
    "    data_selection_col : str\n",
    "        Column identifying Train vs Test\n",
    "    month_col : str\n",
    "        Column with month information\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with comprehensive PSI results and segment definitions\n",
    "    \"\"\"\n",
    "    \n",
    "    if excluded_features is None:\n",
    "        excluded_features = ['digitalLoanAccountId', 'customerId', 'crifapplicationid', 'rundate']\n",
    "    \n",
    "    # Expand calcFeatures\n",
    "    print(\"Expanding calcFeatures...\")\n",
    "    df = expand_calc_features(df)\n",
    "    \n",
    "    # Get feature list (exclude specific columns)\n",
    "    exclude_cols = {\n",
    "        'customerId', 'digitalLoanAccountId', 'Alpha_cic_sil_score',\n",
    "        'start_time', 'end_time', 'modelDisplayName', 'modelVersionId',\n",
    "        'new_loan_type', 'gender', 'loan_product_type', 'osType',\n",
    "        'Model_Name', 'product', 'trenchCategory', 'calcFeatures',\n",
    "        'Data_selection', 'appln_submit_datetime', 'disbursementDateTime',\n",
    "        'Application_month'\n",
    "    }\n",
    "    exclude_cols.update(excluded_features)\n",
    "    \n",
    "    feature_list = [col for col in df.columns if col not in exclude_cols]\n",
    "    \n",
    "    print(f\"Found {len(feature_list)} features to analyze\")\n",
    "    \n",
    "    # Identify feature types from training data\n",
    "    train_df = df[df[data_selection_col] == 'Train']\n",
    "    feature_types = identify_feature_types(df, feature_list)\n",
    "    \n",
    "    # Create binning strategy\n",
    "    print(\"Creating binning strategy...\")\n",
    "    binning_info = create_bins_for_features(\n",
    "        df,\n",
    "        feature_types['numerical'],\n",
    "        feature_types['categorical'],\n",
    "        train_df\n",
    "    )\n",
    "    \n",
    "    all_results = []\n",
    "    segment_id_counter = 0\n",
    "    \n",
    "    # Get valid model-trench combinations\n",
    "    model_trenches = get_valid_model_trench_combinations(df)\n",
    "    segment_combos = generate_segment_combinations(model_trenches)\n",
    "    \n",
    "    print(f\"Found {len(segment_combos)} model-trench combinations\")\n",
    "    \n",
    "    # Level 1: Model Score PSI (modelVersionId + trenchCategory)\n",
    "    print(\"\\n=== Level 1: Model Score PSI ===\")\n",
    "    for model_id, trenches in segment_combos:\n",
    "        segment_def = {\n",
    "            'modelVersionId': model_id,\n",
    "            'trenchCategory': trenches\n",
    "        }\n",
    "        \n",
    "        # Model score is Alpha_cic_sil_score\n",
    "        feature = 'Alpha_cic_sil_score'\n",
    "        if feature not in binning_info:\n",
    "            feature_types_score = identify_feature_types(df, [feature])\n",
    "            binning_info[feature] = create_bins_for_features(\n",
    "                df, feature_types_score['numerical'], \n",
    "                feature_types_score['categorical'], train_df\n",
    "            )[feature]\n",
    "        \n",
    "        results = calculate_psi_for_segments(df, feature, binning_info, segment_def)\n",
    "        \n",
    "        for res in results:\n",
    "            res['Segment_ID'] = segment_id_counter\n",
    "            res['Segment_Name'] = f\"Model_{model_id}_Trench_{'_'.join(trenches)}\"\n",
    "            res['Segment_Column_1'] = 'modelVersionId'\n",
    "            res['Segment_Value_1'] = model_id\n",
    "            res['Segment_Column_2'] = 'trenchCategory'\n",
    "            res['Segment_Value_2'] = ','.join(trenches)\n",
    "            all_results.append(res)\n",
    "        \n",
    "        segment_id_counter += 1\n",
    "    \n",
    "    # Level 2-4: Additional segmentations\n",
    "    additional_segments = [\n",
    "        ('loan_product_type', 'Level 2: By Loan Product Type'),\n",
    "        ('new_loan_type', 'Level 3: By Loan Type'),\n",
    "        ('osType', 'Level 4: By OS Type')\n",
    "    ]\n",
    "    \n",
    "    for segment_col, level_name in additional_segments:\n",
    "        print(f\"\\n=== {level_name} ===\")\n",
    "        \n",
    "        for model_id, trenches in segment_combos:\n",
    "            segment_values = df[\n",
    "                (df['modelVersionId'] == model_id) & \n",
    "                (df['trenchCategory'].isin(trenches))\n",
    "            ][segment_col].dropna().unique().tolist()\n",
    "            \n",
    "            for seg_val in segment_values:\n",
    "                segment_def = {\n",
    "                    'modelVersionId': model_id,\n",
    "                    'trenchCategory': trenches,\n",
    "                    segment_col: seg_val\n",
    "                }\n",
    "                \n",
    "                # Calculate for all features\n",
    "                for feature in feature_list:\n",
    "                    if feature not in binning_info:\n",
    "                        continue\n",
    "                    \n",
    "                    results = calculate_psi_for_segments(df, feature, binning_info, segment_def)\n",
    "                    \n",
    "                    for res in results:\n",
    "                        res['Segment_ID'] = segment_id_counter\n",
    "                        res['Segment_Name'] = (\n",
    "                            f\"Model_{model_id}_Trench_{'_'.join(trenches)}_\"\n",
    "                            f\"{segment_col}_{seg_val}\"\n",
    "                        )\n",
    "                        res['Segment_Column_1'] = 'modelVersionId'\n",
    "                        res['Segment_Value_1'] = model_id\n",
    "                        res['Segment_Column_2'] = 'trenchCategory'\n",
    "                        res['Segment_Value_2'] = ','.join(trenches)\n",
    "                        res['Segment_Column_3'] = segment_col\n",
    "                        res['Segment_Value_3'] = seg_val\n",
    "                        all_results.append(res)\n",
    "                \n",
    "                segment_id_counter += 1\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Reorder columns\n",
    "    core_cols = [\n",
    "        'Segment_ID', 'Segment_Name',\n",
    "        'Segment_Column_1', 'Segment_Value_1',\n",
    "        'Segment_Column_2', 'Segment_Value_2',\n",
    "        'Segment_Column_3', 'Segment_Value_3',\n",
    "        'Feature', 'Feature_Type', 'Month',\n",
    "        'Base_Count', 'Actual_Count',\n",
    "        'Expected_Percentage', 'Actual_Percentage', 'PSI'\n",
    "    ]\n",
    "    \n",
    "    available_cols = [col for col in core_cols if col in results_df.columns]\n",
    "    results_df = results_df[available_cols]\n",
    "    \n",
    "    print(f\"\\nTotal PSI calculations: {len(results_df)}\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# psi_report = create_comprehensive_psi_report(df)\n",
    "# psi_report.to_csv('psi_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6dd01d",
   "metadata": {},
   "source": [
    "# create_comprehensive_psi_report Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "cd5b0fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def expand_calc_features(df):\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns.\n",
    "    \"\"\"\n",
    "    df_expanded = df.copy()\n",
    "    calc_features_list = []\n",
    "\n",
    "    for idx, calc_features_str in enumerate(df['calcFeatures']):\n",
    "        try:\n",
    "            features_dict = json.loads(calc_features_str.replace(\"'\", '\"'))\n",
    "            calc_features_list.append(features_dict)\n",
    "        except (json.JSONDecodeError, AttributeError) as e:\n",
    "            print(f\"Warning: Could not parse calcFeatures at index {idx}: {e}\")\n",
    "            calc_features_list.append({})\n",
    "\n",
    "    calc_features_df = pd.DataFrame(calc_features_list)\n",
    "    calc_features_df = calc_features_df.add_prefix('calc_')\n",
    "\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    calc_features_df = calc_features_df.reset_index(drop=True)\n",
    "\n",
    "    result_df = pd.concat([df_expanded, calc_features_df], axis=1)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def get_model_version_specific_features(df: pd.DataFrame,\n",
    "                                        model_id: str,\n",
    "                                        trenches: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get features that are specific to a modelVersionId + trenchCategory combination.\n",
    "    Only includes calc_ features that have actual data in this segment.\n",
    "    \"\"\"\n",
    "    # Filter to specific model and trenches\n",
    "    segment_df = df[\n",
    "        (df['modelVersionId'] == model_id) & \n",
    "        (df['trenchCategory'].isin(trenches))\n",
    "    ]\n",
    "    \n",
    "    if len(segment_df) == 0:\n",
    "        return []\n",
    "    \n",
    "    # Get all calc_ columns\n",
    "    calc_cols = [col for col in segment_df.columns if col.startswith('calc_')]\n",
    "    \n",
    "    # Filter to only those with actual data (not all NaN)\n",
    "    valid_features = []\n",
    "    for col in calc_cols:\n",
    "        if segment_df[col].notna().sum() > 0:\n",
    "            valid_features.append(col)\n",
    "    \n",
    "    return valid_features\n",
    "\n",
    "\n",
    "def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Identify categorical and numerical features.\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "\n",
    "    for feature in feature_list:\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            unique_vals = df[feature].nunique()\n",
    "            if unique_vals < 15 and df[feature].dropna().apply(\n",
    "                lambda x: x == int(x) if isinstance(x, (int, float)) else False\n",
    "            ).all():\n",
    "                categorical_features.append(feature)\n",
    "            else:\n",
    "                numerical_features.append(feature)\n",
    "        else:\n",
    "            categorical_features.append(feature)\n",
    "\n",
    "    return {\n",
    "        'categorical': categorical_features,\n",
    "        'numerical': numerical_features\n",
    "    }\n",
    "\n",
    "\n",
    "def create_bins_for_features(df: pd.DataFrame,\n",
    "                             numerical_features: List[str],\n",
    "                             categorical_features: List[str],\n",
    "                             train_period_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create bins for numerical features (deciles) and categorical features (top 9 + others).\n",
    "    \"\"\"\n",
    "    binning_info = {}\n",
    "\n",
    "    # Numerical features - Deciles\n",
    "    for feature in numerical_features:\n",
    "        valid_data = train_period_df[feature].dropna()\n",
    "\n",
    "        if len(valid_data) == 0:\n",
    "            binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "            continue\n",
    "\n",
    "        bins = None\n",
    "        bin_count = None\n",
    "\n",
    "        # Try 10 bins (deciles)\n",
    "        try:\n",
    "            test_bins = np.percentile(valid_data, np.arange(0, 101, 10))\n",
    "            test_bins = np.unique(test_bins)\n",
    "            if len(test_bins) >= 11:\n",
    "                bins = test_bins\n",
    "                bin_count = 10\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        # Fallback to 5 bins\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, np.arange(0, 101, 20))\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 6:\n",
    "                    bins = test_bins\n",
    "                    bin_count = 5\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Fallback to 3 bins\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, [0, 33.33, 66.67, 100])\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 4:\n",
    "                    bins = test_bins\n",
    "                    bin_count = 3\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        # Equal distance bins\n",
    "        if bins is None:\n",
    "            print(f\"Warning: Feature '{feature}' has insufficient variance\")\n",
    "            min_val = valid_data.min()\n",
    "            max_val = valid_data.max()\n",
    "            bins = np.linspace(min_val, max_val, 6)\n",
    "            bins = np.unique(bins)\n",
    "            bin_count = len(bins) - 1\n",
    "\n",
    "            if bin_count == 1:\n",
    "                bins = np.array([min_val - 0.1, min_val, min_val + 0.1])\n",
    "                bin_count = 2\n",
    "\n",
    "        bins = bins.copy()\n",
    "        bins[0] = -np.inf\n",
    "        bins[-1] = np.inf\n",
    "\n",
    "        print(f\"  Feature '{feature}': Created {bin_count} bins\")\n",
    "\n",
    "        bin_ranges = {}\n",
    "        for i in range(len(bins)-1):\n",
    "            bin_name = f\"Bin_{i+1}\"\n",
    "            bin_ranges[bin_name] = {\n",
    "                'min': bins[i],\n",
    "                'max': bins[i+1],\n",
    "                'range_str': f\"[{bins[i]:.2f}, {bins[i+1]:.2f}]\" if not np.isinf(bins[i]) and not np.isinf(bins[i+1]) else f\"({bins[i]}, {bins[i+1]})\"\n",
    "            }\n",
    "\n",
    "        binning_info[feature] = {\n",
    "            'type': 'numerical',\n",
    "            'bins': bins,\n",
    "            'bin_ranges': bin_ranges,\n",
    "            'bin_count': bin_count\n",
    "        }\n",
    "\n",
    "    # Categorical features - Top 9 + Others\n",
    "    for feature in categorical_features:\n",
    "        value_counts = train_period_df[feature].value_counts()\n",
    "        unique_categories = value_counts.index.tolist()\n",
    "\n",
    "        if len(unique_categories) <= 9:\n",
    "            top_categories = unique_categories\n",
    "        else:\n",
    "            top_categories = value_counts.nlargest(9).index.tolist()\n",
    "\n",
    "        print(f\"  Feature '{feature}': Using {len(top_categories)} top categories (total: {len(unique_categories)})\")\n",
    "\n",
    "        binning_info[feature] = {\n",
    "            'type': 'categorical',\n",
    "            'top_categories': top_categories,\n",
    "            'bin_ranges': {}\n",
    "        }\n",
    "\n",
    "    return binning_info\n",
    "\n",
    "\n",
    "def apply_binning(df: pd.DataFrame,\n",
    "                  feature: str,\n",
    "                  binning_info: Dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply binning to a feature.\n",
    "    \"\"\"\n",
    "    if binning_info['type'] == 'numerical':\n",
    "        if binning_info['bins'] is None:\n",
    "            return pd.Series(['Missing'] * len(df), index=df.index)\n",
    "\n",
    "        bins = binning_info['bins']\n",
    "        labels = [f\"Bin_{i+1}\" for i in range(len(bins)-1)]\n",
    "\n",
    "        binned = pd.cut(df[feature],\n",
    "                       bins=bins,\n",
    "                       labels=labels,\n",
    "                       include_lowest=True,\n",
    "                       duplicates='drop')\n",
    "\n",
    "        binned = binned.astype(str)\n",
    "        binned[df[feature].isna()] = 'Missing'\n",
    "\n",
    "        return binned\n",
    "\n",
    "    else:  # categorical\n",
    "        top_cats = binning_info['top_categories']\n",
    "        feature_data = df[feature].astype(str)\n",
    "        feature_data = feature_data.replace('nan', 'Missing')\n",
    "        top_cats_str = [str(cat) for cat in top_cats]\n",
    "\n",
    "        binned = feature_data.apply(\n",
    "            lambda x: x if x in top_cats_str else ('Others' if x != 'Missing' else 'Missing')\n",
    "        )\n",
    "\n",
    "        return binned\n",
    "\n",
    "\n",
    "def calculate_psi(expected_pct: pd.Series,\n",
    "                  actual_pct: pd.Series,\n",
    "                  epsilon: float = 0.0001) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index.\n",
    "    \"\"\"\n",
    "    all_bins = expected_pct.index.union(actual_pct.index)\n",
    "    expected_pct = expected_pct.reindex(all_bins, fill_value=0)\n",
    "    actual_pct = actual_pct.reindex(all_bins, fill_value=0)\n",
    "\n",
    "    expected_pct = expected_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "    actual_pct = actual_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "\n",
    "    expected_pct = expected_pct / expected_pct.sum()\n",
    "    actual_pct = actual_pct / actual_pct.sum()\n",
    "\n",
    "    psi_value = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "\n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def get_valid_model_trench_combinations(df: pd.DataFrame) -> List[Tuple[str, str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Get all valid combinations of modelVersionId, modelDisplayName, and trenchCategory.\n",
    "    Returns list of tuples: (modelVersionId, modelDisplayName, [trenchCategory_list])\n",
    "    \"\"\"\n",
    "    combinations_list = []\n",
    "    \n",
    "    for model_id in df['modelVersionId'].unique():\n",
    "        model_df = df[df['modelVersionId'] == model_id]\n",
    "        model_display = model_df['modelDisplayName'].iloc[0]\n",
    "        trenches = sorted(model_df['trenchCategory'].dropna().unique().tolist())\n",
    "        \n",
    "        if trenches:\n",
    "            combinations_list.append((model_id, model_display, trenches))\n",
    "    \n",
    "    return combinations_list\n",
    "\n",
    "\n",
    "def generate_segment_combinations(model_trenches_list: List[Tuple]) -> List[Tuple[str, str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Generate all combinations of models and trenches with modelDisplayName.\n",
    "    Returns (modelVersionId, modelDisplayName, [trench_combinations])\n",
    "    \"\"\"\n",
    "    all_combinations = []\n",
    "    \n",
    "    for model_id, model_display, trenches in model_trenches_list:\n",
    "        # Single trenches\n",
    "        for trench in trenches:\n",
    "            all_combinations.append((model_id, model_display, [trench]))\n",
    "        \n",
    "        # Multiple trench combinations\n",
    "        for r in range(2, len(trenches) + 1):\n",
    "            for combo in combinations(trenches, r):\n",
    "                all_combinations.append((model_id, model_display, sorted(list(combo))))\n",
    "    \n",
    "    return all_combinations\n",
    "\n",
    "\n",
    "def calculate_psi_for_segments(df: pd.DataFrame,\n",
    "                               feature: str,\n",
    "                               binning_info: Dict,\n",
    "                               segment_def: Dict,\n",
    "                               account_id_col: str = 'digitalLoanAccountId',\n",
    "                               data_selection_col: str = 'Data_selection',\n",
    "                               month_col: str = 'Application_month') -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Calculate PSI for a specific feature and segment definition.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    # Apply binning\n",
    "    df[f'{feature}_binned'] = apply_binning(df, feature, binning_info[feature])\n",
    "    \n",
    "    # Get train baseline\n",
    "    train_mask = df[data_selection_col] == 'Train'\n",
    "    \n",
    "    # Apply segment filters\n",
    "    segment_mask = train_mask.copy()\n",
    "    for col, values in segment_def.items():\n",
    "        if col in df.columns:\n",
    "            if isinstance(values, list):\n",
    "                segment_mask = segment_mask & (df[col].isin(values))\n",
    "            else:\n",
    "                segment_mask = segment_mask & (df[col] == values)\n",
    "    \n",
    "    train_segment = df[segment_mask]\n",
    "    \n",
    "    if len(train_segment) == 0:\n",
    "        return []\n",
    "    \n",
    "    train_baseline = train_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "    \n",
    "    # Get test months\n",
    "    test_df = df[~train_mask]\n",
    "    test_months = sorted(test_df[month_col].unique())\n",
    "    \n",
    "    for month in test_months:\n",
    "        # Apply segment filters to test data\n",
    "        test_mask = (df[month_col] == month)\n",
    "        for col, values in segment_def.items():\n",
    "            if col in df.columns:\n",
    "                if isinstance(values, list):\n",
    "                    test_mask = test_mask & (df[col].isin(values))\n",
    "                else:\n",
    "                    test_mask = test_mask & (df[col] == values)\n",
    "        \n",
    "        actual_segment = df[test_mask]\n",
    "        \n",
    "        if len(actual_segment) == 0:\n",
    "            continue\n",
    "        \n",
    "        actual_dist = actual_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "        psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "        \n",
    "        expected_avg_pct = train_baseline.mean() * 100\n",
    "        actual_avg_pct = actual_dist.mean() * 100\n",
    "        \n",
    "        base_count = train_segment[account_id_col].nunique()\n",
    "        actual_count = actual_segment[account_id_col].nunique()\n",
    "        \n",
    "        results.append({\n",
    "            'Feature': feature,\n",
    "            'Feature_Type': binning_info[feature]['type'],\n",
    "            'Month': month,\n",
    "            'Base_Month': 'Train',\n",
    "            'Base_Count': base_count,\n",
    "            'Actual_Count': actual_count,\n",
    "            'Expected_Percentage': expected_avg_pct,\n",
    "            'Actual_Percentage': actual_avg_pct,\n",
    "            'PSI': psi_value\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_comprehensive_psi_report(df: pd.DataFrame,\n",
    "                                    excluded_features: List[str] = None,\n",
    "                                    account_id_col: str = 'digitalLoanAccountId',\n",
    "                                    data_selection_col: str = 'Data_selection',\n",
    "                                    month_col: str = 'Application_month') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create comprehensive PSI report with intelligent segmentation.\n",
    "    Features are calculated per modelVersionId + trenchCategory combination ONLY.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Concatenated train+test dataframe (already expanded with calcFeatures)\n",
    "    excluded_features : List[str]\n",
    "        Features to exclude (e.g., ['digitalLoanAccountId', 'customerId', 'crifapplicationid'])\n",
    "    account_id_col : str\n",
    "        Column name for counting distinct accounts\n",
    "    data_selection_col : str\n",
    "        Column identifying Train vs Test\n",
    "    month_col : str\n",
    "        Column with month information\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with comprehensive PSI results and segment definitions\n",
    "    \"\"\"\n",
    "    \n",
    "    if excluded_features is None:\n",
    "        excluded_features = ['digitalLoanAccountId', 'customerId', 'crifapplicationid', 'rundate']\n",
    "    \n",
    "    # Expand calcFeatures\n",
    "    print(\"Expanding calcFeatures...\")\n",
    "    df = expand_calc_features(df)\n",
    "    \n",
    "    # Get valid model-trench combinations with modelDisplayName\n",
    "    model_trenches = get_valid_model_trench_combinations(df)\n",
    "    segment_combos = generate_segment_combinations(model_trenches)\n",
    "    \n",
    "    print(f\"Found {len(segment_combos)} model-trench combinations\\n\")\n",
    "    \n",
    "    all_results = []\n",
    "    segment_id_counter = 0\n",
    "    \n",
    "    # Get columns to exclude from analysis\n",
    "    exclude_cols = {\n",
    "        'customerId', 'digitalLoanAccountId', 'Alpha_cic_sil_score',\n",
    "        'start_time', 'end_time', 'modelDisplayName', 'modelVersionId',\n",
    "        'new_loan_type', 'gender', 'loan_product_type', 'osType',\n",
    "        'Model_Name', 'product', 'trenchCategory', 'calcFeatures',\n",
    "        'Data_selection', 'appln_submit_datetime', 'disbursementDateTime',\n",
    "        'Application_month'\n",
    "    }\n",
    "    exclude_cols.update(excluded_features)\n",
    "    \n",
    "    # Process each model-trench combination\n",
    "    for model_id, model_display, trenches in segment_combos:\n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Processing Model: {model_display} (ID: {model_id}), Trenches: {trenches}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        # Filter data for this model-trench combo\n",
    "        segment_data = df[\n",
    "            (df['modelVersionId'] == model_id) & \n",
    "            (df['trenchCategory'].isin(trenches))\n",
    "        ].copy()\n",
    "        \n",
    "        if len(segment_data) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get features specific to this model version + trenches\n",
    "        model_specific_features = get_model_version_specific_features(df, model_id, trenches)\n",
    "        feature_list = [f for f in model_specific_features if f not in exclude_cols]\n",
    "        \n",
    "        print(f\"Found {len(feature_list)} features for this combination\")\n",
    "        \n",
    "        # Identify feature types from training data WITHIN this segment\n",
    "        train_segment_data = segment_data[segment_data[data_selection_col] == 'Train']\n",
    "        feature_types = identify_feature_types(segment_data, feature_list)\n",
    "        \n",
    "        # Create binning strategy based on training data WITHIN this segment\n",
    "        print(\"\\nCreating binning strategy...\")\n",
    "        binning_info = create_bins_for_features(\n",
    "            segment_data,\n",
    "            feature_types['numerical'],\n",
    "            feature_types['categorical'],\n",
    "            train_segment_data\n",
    "        )\n",
    "        \n",
    "        trench_str = '_'.join(trenches)\n",
    "        \n",
    "        # Level 1: Model Score PSI (modelVersionId + trenchCategory)\n",
    "        print(f\"\\n--- Level 1: Model Score PSI ---\")\n",
    "        segment_def = {\n",
    "            'modelVersionId': model_id,\n",
    "            'trenchCategory': trenches\n",
    "        }\n",
    "        \n",
    "        feature = 'Alpha_cic_sil_score'\n",
    "        if feature in segment_data.columns:\n",
    "            if feature not in binning_info:\n",
    "                feature_types_score = identify_feature_types(segment_data, [feature])\n",
    "                binning_info[feature] = create_bins_for_features(\n",
    "                    segment_data, feature_types_score['numerical'], \n",
    "                    feature_types_score['categorical'], train_segment_data\n",
    "                )[feature]\n",
    "            \n",
    "            results = calculate_psi_for_segments(segment_data, feature, binning_info, segment_def)\n",
    "            \n",
    "            for res in results:\n",
    "                res['Segment_ID'] = segment_id_counter\n",
    "                res['Model_Version_ID'] = model_id\n",
    "                res['Model_Display_Name'] = model_display\n",
    "                res['Segment_Name'] = f\"{model_display}_Trench_{trench_str}\"\n",
    "                res['Segment_Column_1'] = 'modelVersionId'\n",
    "                res['Segment_Value_1'] = model_id\n",
    "                res['Segment_Column_2'] = 'trenchCategory'\n",
    "                res['Segment_Value_2'] = ','.join(trenches)\n",
    "                all_results.append(res)\n",
    "        \n",
    "        segment_id_counter += 1\n",
    "        \n",
    "        # Level 2: By Loan Product Type\n",
    "        print(f\"--- Level 2: By Loan Product Type ---\")\n",
    "        loan_products = segment_data[segment_data[data_selection_col] == 'Train']['loan_product_type'].dropna().unique()\n",
    "        \n",
    "        for loan_product in loan_products:\n",
    "            segment_def = {\n",
    "                'modelVersionId': model_id,\n",
    "                'trenchCategory': trenches,\n",
    "                'loan_product_type': loan_product\n",
    "            }\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                if feature not in binning_info:\n",
    "                    continue\n",
    "                \n",
    "                results = calculate_psi_for_segments(segment_data, feature, binning_info, segment_def)\n",
    "                \n",
    "                for res in results:\n",
    "                    res['Segment_ID'] = segment_id_counter\n",
    "                    res['Model_Version_ID'] = model_id\n",
    "                    res['Model_Display_Name'] = model_display\n",
    "                    res['Segment_Name'] = f\"{model_display}_Trench_{trench_str}_LoanProduct_{loan_product}\"\n",
    "                    res['Segment_Column_1'] = 'modelVersionId'\n",
    "                    res['Segment_Value_1'] = model_id\n",
    "                    res['Segment_Column_2'] = 'trenchCategory'\n",
    "                    res['Segment_Value_2'] = ','.join(trenches)\n",
    "                    res['Segment_Column_3'] = 'loan_product_type'\n",
    "                    res['Segment_Value_3'] = loan_product\n",
    "                    all_results.append(res)\n",
    "            \n",
    "            segment_id_counter += 1\n",
    "        \n",
    "        # Level 3: By Loan Type\n",
    "        print(f\"--- Level 3: By Loan Type ---\")\n",
    "        loan_types = segment_data[segment_data[data_selection_col] == 'Train']['new_loan_type'].dropna().unique()\n",
    "        \n",
    "        for loan_type in loan_types:\n",
    "            segment_def = {\n",
    "                'modelVersionId': model_id,\n",
    "                'trenchCategory': trenches,\n",
    "                'new_loan_type': loan_type\n",
    "            }\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                if feature not in binning_info:\n",
    "                    continue\n",
    "                \n",
    "                results = calculate_psi_for_segments(segment_data, feature, binning_info, segment_def)\n",
    "                \n",
    "                for res in results:\n",
    "                    res['Segment_ID'] = segment_id_counter\n",
    "                    res['Model_Version_ID'] = model_id\n",
    "                    res['Model_Display_Name'] = model_display\n",
    "                    res['Segment_Name'] = f\"{model_display}_Trench_{trench_str}_LoanType_{loan_type}\"\n",
    "                    res['Segment_Column_1'] = 'modelVersionId'\n",
    "                    res['Segment_Value_1'] = model_id\n",
    "                    res['Segment_Column_2'] = 'trenchCategory'\n",
    "                    res['Segment_Value_2'] = ','.join(trenches)\n",
    "                    res['Segment_Column_3'] = 'new_loan_type'\n",
    "                    res['Segment_Value_3'] = loan_type\n",
    "                    all_results.append(res)\n",
    "            \n",
    "            segment_id_counter += 1\n",
    "        \n",
    "        # Level 4: By OS Type\n",
    "        print(f\"--- Level 4: By OS Type ---\")\n",
    "        os_types = segment_data[segment_data[data_selection_col] == 'Train']['osType'].dropna().unique()\n",
    "        \n",
    "        for os_type in os_types:\n",
    "            segment_def = {\n",
    "                'modelVersionId': model_id,\n",
    "                'trenchCategory': trenches,\n",
    "                'osType': os_type\n",
    "            }\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                if feature not in binning_info:\n",
    "                    continue\n",
    "                \n",
    "                results = calculate_psi_for_segments(segment_data, feature, binning_info, segment_def)\n",
    "                \n",
    "                for res in results:\n",
    "                    res['Segment_ID'] = segment_id_counter\n",
    "                    res['Model_Version_ID'] = model_id\n",
    "                    res['Model_Display_Name'] = model_display\n",
    "                    res['Segment_Name'] = f\"{model_display}_Trench_{trench_str}_OSType_{os_type}\"\n",
    "                    res['Segment_Column_1'] = 'modelVersionId'\n",
    "                    res['Segment_Value_1'] = model_id\n",
    "                    res['Segment_Column_2'] = 'trenchCategory'\n",
    "                    res['Segment_Value_2'] = ','.join(trenches)\n",
    "                    res['Segment_Column_3'] = 'osType'\n",
    "                    res['Segment_Value_3'] = os_type\n",
    "                    all_results.append(res)\n",
    "            \n",
    "            segment_id_counter += 1\n",
    "    \n",
    "    # Create results dataframe\n",
    "    results_df = pd.DataFrame(all_results)\n",
    "    \n",
    "    # Reorder columns intelligently\n",
    "    core_cols = [\n",
    "        'Segment_ID', 'Segment_Name', 'Model_Display_Name', 'Model_Version_ID',\n",
    "        'Segment_Column_1', 'Segment_Value_1',\n",
    "        'Segment_Column_2', 'Segment_Value_2',\n",
    "        'Segment_Column_3', 'Segment_Value_3',\n",
    "        'Feature', 'Feature_Type', 'Month',\n",
    "        'Base_Count', 'Actual_Count',\n",
    "        'Expected_Percentage', 'Actual_Percentage', 'PSI'\n",
    "    ]\n",
    "    \n",
    "    available_cols = [col for col in core_cols if col in results_df.columns]\n",
    "    results_df = results_df[available_cols]\n",
    "    \n",
    "    # Sort by Segment_ID and Feature\n",
    "    results_df = results_df.sort_values(['Segment_ID', 'Feature', 'Month']).reset_index(drop=True)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Total PSI calculations: {len(results_df)}\")\n",
    "    print(f\"Unique segments: {results_df['Segment_ID'].nunique()}\")\n",
    "    print(f\"Unique features analyzed: {results_df['Feature'].nunique()}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# psi_report = create_comprehensive_psi_report(df)\n",
    "# psi_report.to_csv('psi_report.csv', index=False)\n",
    "# psi_report.to_excel('psi_report.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b1da5b",
   "metadata": {},
   "source": [
    "# Comprehensive PSI report function version 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "86e8641a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from typing import List, Dict, Tuple, Set\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def expand_calc_features(df):\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns.\n",
    "    \"\"\"\n",
    "    df_expanded = df.copy()\n",
    "    calc_features_list = []\n",
    "\n",
    "    for idx, calc_features_str in enumerate(df['calcFeatures']):\n",
    "        try:\n",
    "            features_dict = json.loads(calc_features_str.replace(\"'\", '\"'))\n",
    "            calc_features_list.append(features_dict)\n",
    "        except (json.JSONDecodeError, AttributeError) as e:\n",
    "            print(f\"Warning: Could not parse calcFeatures at index {idx}: {e}\")\n",
    "            calc_features_list.append({})\n",
    "\n",
    "    calc_features_df = pd.DataFrame(calc_features_list)\n",
    "    calc_features_df = calc_features_df.add_prefix('calc_')\n",
    "\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    calc_features_df = calc_features_df.reset_index(drop=True)\n",
    "\n",
    "    result_df = pd.concat([df_expanded, calc_features_df], axis=1)\n",
    "    return result_df\n",
    "\n",
    "\n",
    "def get_model_version_specific_features(df: pd.DataFrame,\n",
    "                                        model_id: str,\n",
    "                                        trenches: List[str]) -> List[str]:\n",
    "    \"\"\"\n",
    "    Get features that are specific to a modelVersionId + trenchCategory combination.\n",
    "    Only includes calc_ features that have actual data in this segment.\n",
    "    \"\"\"\n",
    "    segment_df = df[\n",
    "        (df['modelVersionId'] == model_id) & \n",
    "        (df['trenchCategory'].isin(trenches))\n",
    "    ]\n",
    "    \n",
    "    if len(segment_df) == 0:\n",
    "        return []\n",
    "    \n",
    "    calc_cols = [col for col in segment_df.columns if col.startswith('calc_')]\n",
    "    \n",
    "    valid_features = []\n",
    "    for col in calc_cols:\n",
    "        if segment_df[col].notna().sum() > 0:\n",
    "            valid_features.append(col)\n",
    "    \n",
    "    return valid_features\n",
    "\n",
    "\n",
    "def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Identify categorical and numerical features.\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "\n",
    "    for feature in feature_list:\n",
    "        if feature not in df.columns:\n",
    "            continue\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            unique_vals = df[feature].nunique()\n",
    "            if unique_vals < 15 and df[feature].dropna().apply(\n",
    "                lambda x: x == int(x) if isinstance(x, (int, float)) else False\n",
    "            ).all():\n",
    "                categorical_features.append(feature)\n",
    "            else:\n",
    "                numerical_features.append(feature)\n",
    "        else:\n",
    "            categorical_features.append(feature)\n",
    "\n",
    "    return {\n",
    "        'categorical': categorical_features,\n",
    "        'numerical': numerical_features\n",
    "    }\n",
    "\n",
    "\n",
    "def create_bins_for_features(df: pd.DataFrame,\n",
    "                             numerical_features: List[str],\n",
    "                             categorical_features: List[str],\n",
    "                             train_period_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create bins for numerical features (deciles) and categorical features (top 9 + others).\n",
    "    \"\"\"\n",
    "    binning_info = {}\n",
    "\n",
    "    for feature in numerical_features:\n",
    "        valid_data = train_period_df[feature].dropna()\n",
    "\n",
    "        if len(valid_data) == 0:\n",
    "            binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "            continue\n",
    "\n",
    "        bins = None\n",
    "        bin_count = None\n",
    "\n",
    "        try:\n",
    "            test_bins = np.percentile(valid_data, np.arange(0, 101, 10))\n",
    "            test_bins = np.unique(test_bins)\n",
    "            if len(test_bins) >= 11:\n",
    "                bins = test_bins\n",
    "                bin_count = 10\n",
    "        except Exception as e:\n",
    "            pass\n",
    "\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, np.arange(0, 101, 20))\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 6:\n",
    "                    bins = test_bins\n",
    "                    bin_count = 5\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        if bins is None:\n",
    "            try:\n",
    "                test_bins = np.percentile(valid_data, [0, 33.33, 66.67, 100])\n",
    "                test_bins = np.unique(test_bins)\n",
    "                if len(test_bins) >= 4:\n",
    "                    bins = test_bins\n",
    "                    bin_count = 3\n",
    "            except Exception as e:\n",
    "                pass\n",
    "\n",
    "        if bins is None:\n",
    "            print(f\"Warning: Feature '{feature}' has insufficient variance\")\n",
    "            min_val = valid_data.min()\n",
    "            max_val = valid_data.max()\n",
    "            bins = np.linspace(min_val, max_val, 6)\n",
    "            bins = np.unique(bins)\n",
    "            bin_count = len(bins) - 1\n",
    "\n",
    "            if bin_count == 1:\n",
    "                bins = np.array([min_val - 0.1, min_val, min_val + 0.1])\n",
    "                bin_count = 2\n",
    "\n",
    "        bins = bins.copy()\n",
    "        bins[0] = -np.inf\n",
    "        bins[-1] = np.inf\n",
    "\n",
    "        print(f\"  Feature '{feature}': Created {bin_count} bins\")\n",
    "\n",
    "        bin_ranges = {}\n",
    "        for i in range(len(bins)-1):\n",
    "            bin_name = f\"Bin_{i+1}\"\n",
    "            bin_ranges[bin_name] = {\n",
    "                'min': bins[i],\n",
    "                'max': bins[i+1],\n",
    "                'range_str': f\"[{bins[i]:.2f}, {bins[i+1]:.2f}]\" if not np.isinf(bins[i]) and not np.isinf(bins[i+1]) else f\"({bins[i]}, {bins[i+1]})\"\n",
    "            }\n",
    "\n",
    "        binning_info[feature] = {\n",
    "            'type': 'numerical',\n",
    "            'bins': bins,\n",
    "            'bin_ranges': bin_ranges,\n",
    "            'bin_count': bin_count\n",
    "        }\n",
    "\n",
    "    for feature in categorical_features:\n",
    "        value_counts = train_period_df[feature].value_counts()\n",
    "        unique_categories = value_counts.index.tolist()\n",
    "\n",
    "        if len(unique_categories) <= 9:\n",
    "            top_categories = unique_categories\n",
    "        else:\n",
    "            top_categories = value_counts.nlargest(9).index.tolist()\n",
    "\n",
    "        print(f\"  Feature '{feature}': Using {len(top_categories)} top categories (total: {len(unique_categories)})\")\n",
    "\n",
    "        binning_info[feature] = {\n",
    "            'type': 'categorical',\n",
    "            'top_categories': top_categories,\n",
    "            'bin_ranges': {}\n",
    "        }\n",
    "\n",
    "    return binning_info\n",
    "\n",
    "\n",
    "def apply_binning(df: pd.DataFrame,\n",
    "                  feature: str,\n",
    "                  binning_info: Dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply binning to a feature.\n",
    "    \"\"\"\n",
    "    if binning_info['type'] == 'numerical':\n",
    "        if binning_info['bins'] is None:\n",
    "            return pd.Series(['Missing'] * len(df), index=df.index)\n",
    "\n",
    "        bins = binning_info['bins']\n",
    "        labels = [f\"Bin_{i+1}\" for i in range(len(bins)-1)]\n",
    "\n",
    "        binned = pd.cut(df[feature],\n",
    "                       bins=bins,\n",
    "                       labels=labels,\n",
    "                       include_lowest=True,\n",
    "                       duplicates='drop')\n",
    "\n",
    "        binned = binned.astype(str)\n",
    "        binned[df[feature].isna()] = 'Missing'\n",
    "\n",
    "        return binned\n",
    "\n",
    "    else:\n",
    "        top_cats = binning_info['top_categories']\n",
    "        feature_data = df[feature].astype(str)\n",
    "        feature_data = feature_data.replace('nan', 'Missing')\n",
    "        top_cats_str = [str(cat) for cat in top_cats]\n",
    "\n",
    "        binned = feature_data.apply(\n",
    "            lambda x: x if x in top_cats_str else ('Others' if x != 'Missing' else 'Missing')\n",
    "        )\n",
    "\n",
    "        return binned\n",
    "\n",
    "\n",
    "def calculate_psi(expected_pct: pd.Series,\n",
    "                  actual_pct: pd.Series,\n",
    "                  epsilon: float = 0.0001) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index.\n",
    "    \"\"\"\n",
    "    all_bins = expected_pct.index.union(actual_pct.index)\n",
    "    expected_pct = expected_pct.reindex(all_bins, fill_value=0)\n",
    "    actual_pct = actual_pct.reindex(all_bins, fill_value=0)\n",
    "\n",
    "    expected_pct = expected_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "    actual_pct = actual_pct.apply(lambda x: epsilon if x == 0 else x)\n",
    "\n",
    "    expected_pct = expected_pct / expected_pct.sum()\n",
    "    actual_pct = actual_pct / actual_pct.sum()\n",
    "\n",
    "    psi_value = np.sum((actual_pct - expected_pct) * np.log(actual_pct / expected_pct))\n",
    "\n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def get_valid_model_trench_combinations(df: pd.DataFrame) -> List[Tuple[str, str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Get all valid combinations of modelVersionId, modelDisplayName, and trenchCategory.\n",
    "    \"\"\"\n",
    "    combinations_list = []\n",
    "    \n",
    "    for model_id in df['modelVersionId'].unique():\n",
    "        model_df = df[df['modelVersionId'] == model_id]\n",
    "        model_display = model_df['modelDisplayName'].iloc[0]\n",
    "        trenches = sorted(model_df['trenchCategory'].dropna().unique().tolist())\n",
    "        \n",
    "        if trenches:\n",
    "            combinations_list.append((model_id, model_display, trenches))\n",
    "    \n",
    "    return combinations_list\n",
    "\n",
    "\n",
    "def generate_segment_combinations(model_trenches_list: List[Tuple]) -> List[Tuple[str, str, List[str]]]:\n",
    "    \"\"\"\n",
    "    Generate all combinations of models and trenches with modelDisplayName.\n",
    "    \"\"\"\n",
    "    all_combinations = []\n",
    "    \n",
    "    for model_id, model_display, trenches in model_trenches_list:\n",
    "        for trench in trenches:\n",
    "            all_combinations.append((model_id, model_display, [trench]))\n",
    "        \n",
    "        for r in range(2, len(trenches) + 1):\n",
    "            for combo in combinations(trenches, r):\n",
    "                all_combinations.append((model_id, model_display, sorted(list(combo))))\n",
    "    \n",
    "    return all_combinations\n",
    "\n",
    "\n",
    "def calculate_psi_for_segments(df: pd.DataFrame,\n",
    "                               feature: str,\n",
    "                               binning_info: Dict,\n",
    "                               segment_def: Dict,\n",
    "                               account_id_col: str = 'digitalLoanAccountId',\n",
    "                               data_selection_col: str = 'Data_selection',\n",
    "                               month_col: str = 'Application_month') -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Calculate PSI for a specific feature and segment definition.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    df[f'{feature}_binned'] = apply_binning(df, feature, binning_info[feature])\n",
    "    \n",
    "    train_mask = df[data_selection_col] == 'Train'\n",
    "    \n",
    "    segment_mask = train_mask.copy()\n",
    "    for col, values in segment_def.items():\n",
    "        if col in df.columns:\n",
    "            if isinstance(values, list):\n",
    "                segment_mask = segment_mask & (df[col].isin(values))\n",
    "            else:\n",
    "                segment_mask = segment_mask & (df[col] == values)\n",
    "    \n",
    "    train_segment = df[segment_mask]\n",
    "    \n",
    "    if len(train_segment) == 0:\n",
    "        return []\n",
    "    \n",
    "    train_baseline = train_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "    \n",
    "    test_df = df[~train_mask]\n",
    "    test_months = sorted(test_df[month_col].unique())\n",
    "    \n",
    "    for month in test_months:\n",
    "        test_mask = (df[month_col] == month)\n",
    "        for col, values in segment_def.items():\n",
    "            if col in df.columns:\n",
    "                if isinstance(values, list):\n",
    "                    test_mask = test_mask & (df[col].isin(values))\n",
    "                else:\n",
    "                    test_mask = test_mask & (df[col] == values)\n",
    "        \n",
    "        actual_segment = df[test_mask]\n",
    "        \n",
    "        if len(actual_segment) == 0:\n",
    "            continue\n",
    "        \n",
    "        actual_dist = actual_segment[f'{feature}_binned'].value_counts(normalize=True)\n",
    "        psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "        \n",
    "        expected_avg_pct = train_baseline.mean() * 100\n",
    "        actual_avg_pct = actual_dist.mean() * 100\n",
    "        \n",
    "        base_count = train_segment[account_id_col].nunique()\n",
    "        actual_count = actual_segment[account_id_col].nunique()\n",
    "        \n",
    "        results.append({\n",
    "            'Feature': feature,\n",
    "            'Feature_Type': binning_info[feature]['type'],\n",
    "            'Month': month,\n",
    "            'Base_Month': 'Train',\n",
    "            'Base_Count': base_count,\n",
    "            'Actual_Count': actual_count,\n",
    "            'Expected_Percentage': expected_avg_pct,\n",
    "            'Actual_Percentage': actual_avg_pct,\n",
    "            'PSI': psi_value\n",
    "        })\n",
    "    \n",
    "    return results\n",
    "\n",
    "\n",
    "def create_comprehensive_psi_report(df: pd.DataFrame,\n",
    "                                    excluded_features: List[str] = None,\n",
    "                                    account_id_col: str = 'digitalLoanAccountId',\n",
    "                                    data_selection_col: str = 'Data_selection',\n",
    "                                    month_col: str = 'Application_month') -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Create comprehensive PSI report split into dimension and fact tables for Power BI.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Tuple of 4 DataFrames:\n",
    "    - dim_segment: Segment dimension table\n",
    "    - dim_feature: Feature dimension table\n",
    "    - dim_month: Month dimension table\n",
    "    - fact_psi: PSI fact table with foreign keys\n",
    "    \"\"\"\n",
    "    \n",
    "    if excluded_features is None:\n",
    "        excluded_features = ['digitalLoanAccountId', 'customerId', 'crifapplicationid', 'rundate']\n",
    "    \n",
    "    print(\"Expanding calcFeatures...\")\n",
    "    df = expand_calc_features(df)\n",
    "    \n",
    "    model_trenches = get_valid_model_trench_combinations(df)\n",
    "    segment_combos = generate_segment_combinations(model_trenches)\n",
    "    \n",
    "    print(f\"Found {len(segment_combos)} model-trench combinations\\n\")\n",
    "    \n",
    "    exclude_cols = {\n",
    "        'customerId', 'digitalLoanAccountId', 'Alpha_cic_sil_score',\n",
    "        'start_time', 'end_time', 'modelDisplayName', 'modelVersionId',\n",
    "        'new_loan_type', 'gender', 'loan_product_type', 'osType',\n",
    "        'Model_Name', 'product', 'trenchCategory', 'calcFeatures',\n",
    "        'Data_selection', 'appln_submit_datetime', 'disbursementDateTime',\n",
    "        'Application_month'\n",
    "    }\n",
    "    exclude_cols.update(excluded_features)\n",
    "    \n",
    "    # Storage for dimension data\n",
    "    segment_data = []\n",
    "    feature_data = []\n",
    "    month_data = []\n",
    "    fact_data = []\n",
    "    \n",
    "    feature_id_map = {}  # Map feature names to IDs\n",
    "    month_id_map = {}    # Map months to IDs\n",
    "    segment_id_counter = 0\n",
    "    feature_id_counter = 1\n",
    "    month_id_counter = 1\n",
    "    \n",
    "    # Process each model-trench combination\n",
    "    for model_id, model_display, trenches in segment_combos:\n",
    "        print(f\"\\nProcessing Model: {model_display} (ID: {model_id}), Trenches: {trenches}\")\n",
    "        \n",
    "        segment_data_subset = df[\n",
    "            (df['modelVersionId'] == model_id) & \n",
    "            (df['trenchCategory'].isin(trenches))\n",
    "        ].copy()\n",
    "        \n",
    "        if len(segment_data_subset) == 0:\n",
    "            continue\n",
    "        \n",
    "        model_specific_features = get_model_version_specific_features(df, model_id, trenches)\n",
    "        feature_list = [f for f in model_specific_features if f not in exclude_cols]\n",
    "        \n",
    "        print(f\"Found {len(feature_list)} features for this combination\")\n",
    "        \n",
    "        train_segment_data = segment_data_subset[segment_data_subset[data_selection_col] == 'Train']\n",
    "        feature_types = identify_feature_types(segment_data_subset, feature_list)\n",
    "        \n",
    "        print(\"Creating binning strategy...\")\n",
    "        binning_info = create_bins_for_features(\n",
    "            segment_data_subset,\n",
    "            feature_types['numerical'],\n",
    "            feature_types['categorical'],\n",
    "            train_segment_data\n",
    "        )\n",
    "        \n",
    "        trench_str = '_'.join(trenches)\n",
    "        \n",
    "        # Level 1: Model Score\n",
    "        print(f\"Level 1: Model Score PSI\")\n",
    "        segment_def = {\n",
    "            'modelVersionId': model_id,\n",
    "            'trenchCategory': trenches\n",
    "        }\n",
    "        \n",
    "        segment_key = f\"Model_{model_id}_Trench_{trench_str}\"\n",
    "        \n",
    "        segment_data.append({\n",
    "            'Segment_ID': segment_id_counter,\n",
    "            'Segment_Key': segment_key,\n",
    "            'Segment_Name': f\"{model_display}_Trench_{trench_str}\",\n",
    "            'Model_Version_ID': model_id,\n",
    "            'Model_Display_Name': model_display,\n",
    "            'Segment_Type': 'Model Score',\n",
    "            'Trench_Category': ','.join(trenches),\n",
    "            'Loan_Product_Type': None,\n",
    "            'New_Loan_Type': None,\n",
    "            'OS_Type': None,\n",
    "            'Segment_Level': 1\n",
    "        })\n",
    "        \n",
    "        feature = 'Alpha_cic_sil_score'\n",
    "        if feature in segment_data_subset.columns:\n",
    "            if feature not in feature_id_map:\n",
    "                feature_id_map[feature] = feature_id_counter\n",
    "                feature_data.append({\n",
    "                    'Feature_ID': feature_id_counter,\n",
    "                    'Feature_Name': feature,\n",
    "                    'Feature_Type': 'Numerical',\n",
    "                    'Display_Name': 'Model Score'\n",
    "                })\n",
    "                feature_id_counter += 1\n",
    "            \n",
    "            results = calculate_psi_for_segments(segment_data_subset, feature, binning_info, segment_def)\n",
    "            \n",
    "            for res in results:\n",
    "                month = res['Month']\n",
    "                if month not in month_id_map:\n",
    "                    month_id_map[month] = month_id_counter\n",
    "                    month_data.append({\n",
    "                        'Month_ID': month_id_counter,\n",
    "                        'Month': month,\n",
    "                        'Month_Sort_Order': pd.to_datetime(month).strftime('%Y%m%d')\n",
    "                    })\n",
    "                    month_id_counter += 1\n",
    "                \n",
    "                fact_data.append({\n",
    "                    'Segment_ID': segment_id_counter,\n",
    "                    'Feature_ID': feature_id_map[feature],\n",
    "                    'Month_ID': month_id_map[month],\n",
    "                    'Base_Count': res['Base_Count'],\n",
    "                    'Actual_Count': res['Actual_Count'],\n",
    "                    'Expected_Percentage': res['Expected_Percentage'],\n",
    "                    'Actual_Percentage': res['Actual_Percentage'],\n",
    "                    'PSI': res['PSI']\n",
    "                })\n",
    "        \n",
    "        segment_id_counter += 1\n",
    "        \n",
    "        # Level 2: By Loan Product Type\n",
    "        print(f\"Level 2: By Loan Product Type\")\n",
    "        loan_products = segment_data_subset[segment_data_subset[data_selection_col] == 'Train']['loan_product_type'].dropna().unique()\n",
    "        \n",
    "        for loan_product in loan_products:\n",
    "            segment_def = {\n",
    "                'modelVersionId': model_id,\n",
    "                'trenchCategory': trenches,\n",
    "                'loan_product_type': loan_product\n",
    "            }\n",
    "            \n",
    "            segment_key = f\"Model_{model_id}_Trench_{trench_str}_LoanProduct_{loan_product}\"\n",
    "            \n",
    "            segment_data.append({\n",
    "                'Segment_ID': segment_id_counter,\n",
    "                'Segment_Key': segment_key,\n",
    "                'Segment_Name': f\"{model_display}_Trench_{trench_str}_LoanProduct_{loan_product}\",\n",
    "                'Model_Version_ID': model_id,\n",
    "                'Model_Display_Name': model_display,\n",
    "                'Segment_Type': 'By Loan Product Type',\n",
    "                'Trench_Category': ','.join(trenches),\n",
    "                'Loan_Product_Type': loan_product,\n",
    "                'New_Loan_Type': None,\n",
    "                'OS_Type': None,\n",
    "                'Segment_Level': 2\n",
    "            })\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                if feature not in binning_info:\n",
    "                    continue\n",
    "                \n",
    "                if feature not in feature_id_map:\n",
    "                    feature_id_map[feature] = feature_id_counter\n",
    "                    feature_data.append({\n",
    "                        'Feature_ID': feature_id_counter,\n",
    "                        'Feature_Name': feature,\n",
    "                        'Feature_Type': binning_info[feature]['type'],\n",
    "                        'Display_Name': feature.replace('calc_', '')\n",
    "                    })\n",
    "                    feature_id_counter += 1\n",
    "                \n",
    "                results = calculate_psi_for_segments(segment_data_subset, feature, binning_info, segment_def)\n",
    "                \n",
    "                for res in results:\n",
    "                    month = res['Month']\n",
    "                    if month not in month_id_map:\n",
    "                        month_id_map[month] = month_id_counter\n",
    "                        month_data.append({\n",
    "                            'Month_ID': month_id_counter,\n",
    "                            'Month': month,\n",
    "                            'Month_Sort_Order': pd.to_datetime(month).strftime('%Y%m%d')\n",
    "                        })\n",
    "                        month_id_counter += 1\n",
    "                    \n",
    "                    fact_data.append({\n",
    "                        'Segment_ID': segment_id_counter,\n",
    "                        'Feature_ID': feature_id_map[feature],\n",
    "                        'Month_ID': month_id_map[month],\n",
    "                        'Base_Count': res['Base_Count'],\n",
    "                        'Actual_Count': res['Actual_Count'],\n",
    "                        'Expected_Percentage': res['Expected_Percentage'],\n",
    "                        'Actual_Percentage': res['Actual_Percentage'],\n",
    "                        'PSI': res['PSI']\n",
    "                    })\n",
    "            \n",
    "            segment_id_counter += 1\n",
    "        \n",
    "        # Level 3: By Loan Type\n",
    "        print(f\"Level 3: By Loan Type\")\n",
    "        loan_types = segment_data_subset[segment_data_subset[data_selection_col] == 'Train']['new_loan_type'].dropna().unique()\n",
    "        \n",
    "        for loan_type in loan_types:\n",
    "            segment_def = {\n",
    "                'modelVersionId': model_id,\n",
    "                'trenchCategory': trenches,\n",
    "                'new_loan_type': loan_type\n",
    "            }\n",
    "            \n",
    "            segment_key = f\"Model_{model_id}_Trench_{trench_str}_LoanType_{loan_type}\"\n",
    "            \n",
    "            segment_data.append({\n",
    "                'Segment_ID': segment_id_counter,\n",
    "                'Segment_Key': segment_key,\n",
    "                'Segment_Name': f\"{model_display}_Trench_{trench_str}_LoanType_{loan_type}\",\n",
    "                'Model_Version_ID': model_id,\n",
    "                'Model_Display_Name': model_display,\n",
    "                'Segment_Type': 'By Loan Type',\n",
    "                'Trench_Category': ','.join(trenches),\n",
    "                'Loan_Product_Type': None,\n",
    "                'New_Loan_Type': loan_type,\n",
    "                'OS_Type': None,\n",
    "                'Segment_Level': 3\n",
    "            })\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                if feature not in binning_info:\n",
    "                    continue\n",
    "                \n",
    "                if feature not in feature_id_map:\n",
    "                    feature_id_map[feature] = feature_id_counter\n",
    "                    feature_data.append({\n",
    "                        'Feature_ID': feature_id_counter,\n",
    "                        'Feature_Name': feature,\n",
    "                        'Feature_Type': binning_info[feature]['type'],\n",
    "                        'Display_Name': feature.replace('calc_', '')\n",
    "                    })\n",
    "                    feature_id_counter += 1\n",
    "                \n",
    "                results = calculate_psi_for_segments(segment_data_subset, feature, binning_info, segment_def)\n",
    "                \n",
    "                for res in results:\n",
    "                    month = res['Month']\n",
    "                    if month not in month_id_map:\n",
    "                        month_id_map[month] = month_id_counter\n",
    "                        month_data.append({\n",
    "                            'Month_ID': month_id_counter,\n",
    "                            'Month': month,\n",
    "                            'Month_Sort_Order': pd.to_datetime(month).strftime('%Y%m%d')\n",
    "                        })\n",
    "                        month_id_counter += 1\n",
    "                    \n",
    "                    fact_data.append({\n",
    "                        'Segment_ID': segment_id_counter,\n",
    "                        'Feature_ID': feature_id_map[feature],\n",
    "                        'Month_ID': month_id_map[month],\n",
    "                        'Base_Count': res['Base_Count'],\n",
    "                        'Actual_Count': res['Actual_Count'],\n",
    "                        'Expected_Percentage': res['Expected_Percentage'],\n",
    "                        'Actual_Percentage': res['Actual_Percentage'],\n",
    "                        'PSI': res['PSI']\n",
    "                    })\n",
    "            \n",
    "            segment_id_counter += 1\n",
    "        \n",
    "        # Level 4: By OS Type\n",
    "        print(f\"Level 4: By OS Type\")\n",
    "        os_types = segment_data_subset[segment_data_subset[data_selection_col] == 'Train']['osType'].dropna().unique()\n",
    "        \n",
    "        for os_type in os_types:\n",
    "            segment_def = {\n",
    "                'modelVersionId': model_id,\n",
    "                'trenchCategory': trenches,\n",
    "                'osType': os_type\n",
    "            }\n",
    "            \n",
    "            segment_key = f\"Model_{model_id}_Trench_{trench_str}_OSType_{os_type}\"\n",
    "            \n",
    "            segment_data.append({\n",
    "                'Segment_ID': segment_id_counter,\n",
    "                'Segment_Key': segment_key,\n",
    "                'Segment_Name': f\"{model_display}_Trench_{trench_str}_OSType_{os_type}\",\n",
    "                'Model_Version_ID': model_id,\n",
    "                'Model_Display_Name': model_display,\n",
    "                'Segment_Type': 'By OS Type',\n",
    "                'Trench_Category': ','.join(trenches),\n",
    "                'Loan_Product_Type': None,\n",
    "                'New_Loan_Type': None,\n",
    "                'OS_Type': os_type,\n",
    "                'Segment_Level': 4\n",
    "            })\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                if feature not in binning_info:\n",
    "                    continue\n",
    "                \n",
    "                if feature not in feature_id_map:\n",
    "                    feature_id_map[feature] = feature_id_counter\n",
    "                    feature_data.append({\n",
    "                        'Feature_ID': feature_id_counter,\n",
    "                        'Feature_Name': feature,\n",
    "                        'Feature_Type': binning_info[feature]['type'],\n",
    "                        'Display_Name': feature.replace('calc_', '')\n",
    "                    })\n",
    "                    feature_id_counter += 1\n",
    "                \n",
    "                results = calculate_psi_for_segments(segment_data_subset, feature, binning_info, segment_def)\n",
    "                \n",
    "                for res in results:\n",
    "                    month = res['Month']\n",
    "                    if month not in month_id_map:\n",
    "                        month_id_map[month] = month_id_counter\n",
    "                        month_data.append({\n",
    "                            'Month_ID': month_id_counter,\n",
    "                            'Month': month,\n",
    "                            'Month_Sort_Order': pd.to_datetime(month).strftime('%Y%m%d')\n",
    "                        })\n",
    "                        month_id_counter += 1\n",
    "                    \n",
    "                    fact_data.append({\n",
    "                        'Segment_ID': segment_id_counter,\n",
    "                        'Feature_ID': feature_id_map[feature],\n",
    "                        'Month_ID': month_id_map[month],\n",
    "                        'Base_Count': res['Base_Count'],\n",
    "                        'Actual_Count': res['Actual_Count'],\n",
    "                        'Expected_Percentage': res['Expected_Percentage'],\n",
    "                        'Actual_Percentage': res['Actual_Percentage'],\n",
    "                        'PSI': res['PSI']\n",
    "                    })\n",
    "            \n",
    "            segment_id_counter += 1\n",
    "    \n",
    "    # Convert to DataFrames\n",
    "    dim_segment = pd.DataFrame(segment_data)\n",
    "    dim_feature = pd.DataFrame(feature_data)\n",
    "    dim_month = pd.DataFrame(month_data).sort_values('Month_Sort_Order').reset_index(drop=True)\n",
    "    fact_psi = pd.DataFrame(fact_data)\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"DIMENSION TABLES CREATED:\")\n",
    "    print(f\"  - dim_segment: {len(dim_segment)} records\")\n",
    "    print(f\"  - dim_feature: {len(dim_feature)} records\")\n",
    "    print(f\"  - dim_month: {len(dim_month)} records\")\n",
    "    print(f\"FACT TABLE:\")\n",
    "    print(f\"  - fact_psi: {len(fact_psi)} records\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    return dim_segment, dim_feature, dim_month, fact_psi\n",
    "\n",
    "\n",
    "# Example Usage:\n",
    "# df = pd.read_csv('your_data.csv')\n",
    "# dim_segment, dim_feature, dim_month, fact_psi = create_comprehensive_psi_report(df)\n",
    "#\n",
    "# # Save to CSV\n",
    "# dim_segment.to_csv('dim_segment.csv', index=False)\n",
    "# dim_feature.to_csv('dim_feature.csv', index=False)\n",
    "# dim_month.to_csv('dim_month.csv', index=False)\n",
    "# fact_psi.to_csv('fact_psi.csv', index=False)\n",
    "#\n",
    "# # Or save to Excel\n",
    "# with pd.ExcelWriter('psi_model.xlsx') as writer:\n",
    "#     dim_segment.to_excel(writer, sheet_name='dim_segment', index=False)\n",
    "#     dim_feature.to_excel(writer, sheet_name='dim_feature', index=False)\n",
    "#     dim_month.to_excel(writer, sheet_name='dim_month', index=False)\n",
    "#     fact_psi.to_excel(writer, sheet_name='fact_psi', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18aaaf6a",
   "metadata": {},
   "source": [
    "# Function Ends"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b339bb77-2a06-49fe-a366-28893e7a0797",
   "metadata": {},
   "source": [
    "# SIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d774f3f0-6ceb-4518-8258-2e1d7d90ad28",
   "metadata": {},
   "source": [
    "## Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9bd2f20-c27c-48b4-87c6-42b3b3bd7b2e",
   "metadata": {},
   "source": [
    "#### Alpha - CIC-SIL-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ce9ff17-1116-45fa-bdbc-5e4871de272d",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "24f3fb61-6980-4977-880f-c062d809c50e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe is: (100984, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>digitalLoanAccountId</th>\n",
       "      <th>Alpha_cic_sil_score</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>modelDisplayName</th>\n",
       "      <th>modelVersionId</th>\n",
       "      <th>new_loan_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>loan_product_type</th>\n",
       "      <th>osType</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>product</th>\n",
       "      <th>trenchCategory</th>\n",
       "      <th>calcFeatures</th>\n",
       "      <th>Data_selection</th>\n",
       "      <th>appln_submit_datetime</th>\n",
       "      <th>disbursementDateTime</th>\n",
       "      <th>Application_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2147044</td>\n",
       "      <td>aef56954-6c97-4520-aa14-2e163676f889</td>\n",
       "      <td>0.20132748968215752</td>\n",
       "      <td>2025-08-23 05:04:52.388799</td>\n",
       "      <td>2025-08-23 05:04:52.395256</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL Competitor</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1755907200000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-08-23 13:04:42</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2551996</td>\n",
       "      <td>81cf347d-8d6c-41ee-a67c-043c0a91e9de</td>\n",
       "      <td>0.1214890420101941</td>\n",
       "      <td>2025-11-04 10:54:49.330879</td>\n",
       "      <td>2025-11-04 10:54:49.337092</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1762214400000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-11-04 18:53:56</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2700374</td>\n",
       "      <td>73de3961-c48b-40a0-b961-84c9adb10e4a</td>\n",
       "      <td>0.19584553244636096</td>\n",
       "      <td>2025-06-15 07:14:26.730021</td>\n",
       "      <td>2025-06-15 07:14:26.736034</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1749945600000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-06-15 15:14:18</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2741007</td>\n",
       "      <td>28222315-a058-4b2e-858e-689a3837a08d</td>\n",
       "      <td>0.12350283937750572</td>\n",
       "      <td>2025-06-12 04:54:47.541738</td>\n",
       "      <td>2025-06-12 04:54:47.547863</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1749686400000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-06-12 12:54:37</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2907116</td>\n",
       "      <td>b22a1f11-eac7-4a15-96dd-db53f75e452b</td>\n",
       "      <td>0.16215984839484424</td>\n",
       "      <td>2025-11-12 03:32:13.445054</td>\n",
       "      <td>2025-11-12 03:32:13.451666</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL Competitor</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1762905600000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-11-12 11:32:03</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customerId                  digitalLoanAccountId  Alpha_cic_sil_score  \\\n",
       "0    2147044  aef56954-6c97-4520-aa14-2e163676f889  0.20132748968215752   \n",
       "1    2551996  81cf347d-8d6c-41ee-a67c-043c0a91e9de   0.1214890420101941   \n",
       "2    2700374  73de3961-c48b-40a0-b961-84c9adb10e4a  0.19584553244636096   \n",
       "3    2741007  28222315-a058-4b2e-858e-689a3837a08d  0.12350283937750572   \n",
       "4    2907116  b22a1f11-eac7-4a15-96dd-db53f75e452b  0.16215984839484424   \n",
       "\n",
       "                  start_time                   end_time modelDisplayName  \\\n",
       "0 2025-08-23 05:04:52.388799 2025-08-23 05:04:52.395256    cic_model_sil   \n",
       "1 2025-11-04 10:54:49.330879 2025-11-04 10:54:49.337092    cic_model_sil   \n",
       "2 2025-06-15 07:14:26.730021 2025-06-15 07:14:26.736034    cic_model_sil   \n",
       "3 2025-06-12 04:54:47.541738 2025-06-12 04:54:47.547863    cic_model_sil   \n",
       "4 2025-11-12 03:32:13.445054 2025-11-12 03:32:13.451666    cic_model_sil   \n",
       "\n",
       "  modelVersionId   new_loan_type gender loan_product_type   osType  \\\n",
       "0             v1  SIL Competitor      F         Appliance  android   \n",
       "1             v1     SIL-Instore      M         Appliance  android   \n",
       "2             v1     SIL-Instore      F         Appliance  android   \n",
       "3             v1     SIL-Instore      M         Appliance  android   \n",
       "4             v1  SIL Competitor      F         Appliance  android   \n",
       "\n",
       "      Model_Name product trenchCategory  \\\n",
       "0  cic_model_sil     SIL            ALL   \n",
       "1  cic_model_sil     SIL            ALL   \n",
       "2  cic_model_sil     SIL            ALL   \n",
       "3  cic_model_sil     SIL            ALL   \n",
       "4  cic_model_sil     SIL            ALL   \n",
       "\n",
       "                                        calcFeatures Data_selection  \\\n",
       "0  {\"run_date\":1755907200000,\"cic_Personal_Loans_...           Test   \n",
       "1  {\"run_date\":1762214400000,\"cic_Personal_Loans_...           Test   \n",
       "2  {\"run_date\":1749945600000,\"cic_Personal_Loans_...           Test   \n",
       "3  {\"run_date\":1749686400000,\"cic_Personal_Loans_...           Test   \n",
       "4  {\"run_date\":1762905600000,\"cic_Personal_Loans_...           Test   \n",
       "\n",
       "  appln_submit_datetime disbursementDateTime Application_month  \n",
       "0   2025-08-23 13:04:42                  NaT           2025-08  \n",
       "1   2025-11-04 18:53:56                  NaT           2025-11  \n",
       "2   2025-06-15 15:14:18                  NaT           2025-06  \n",
       "3   2025-06-12 12:54:37                  NaT           2025-06  \n",
       "4   2025-11-12 11:32:03                  NaT           2025-11  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is for the test period of Alpha - CIC sil model - reading the data from ml_model_run_details\n",
    "\n",
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "        case when modelDisplayName = 'Alpha - CIC-SIL-Model' then 'cic_model_sil' else modelDisplayName end as modelDisplayName    \n",
    "    ,modelVersionId,\n",
    "    case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory='' then 'ALL'    \n",
    "    else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in ('Alpha - CIC-SIL-Model', 'cic_model_sil')\n",
    "  ),\n",
    "base as\n",
    "(SELECT distinct\n",
    "  r.customerId,r.digitalLoanAccountId,prediction Alpha_cic_sil_score\n",
    "    ,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "   loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "        when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "        when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "        else 'ios' end osType,\n",
    " 'cic_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Test' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId, modelVersionId \n",
    "order by coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) desc) = 1\n",
    ")\n",
    "select * from base\n",
    ";\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()\n",
    "\n",
    "## this data is not expanded. We will have to expand and get the features from the calcFeatures column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "fda2ff6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = dfd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8a1c366",
   "metadata": {},
   "source": [
    "### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db6aab38",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe is: (450861, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>digitalLoanAccountId</th>\n",
       "      <th>Alpha_cic_sil_score</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>modelDisplayName</th>\n",
       "      <th>modelVersionId</th>\n",
       "      <th>new_loan_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>loan_product_type</th>\n",
       "      <th>osType</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>product</th>\n",
       "      <th>trenchCategory</th>\n",
       "      <th>calcFeatures</th>\n",
       "      <th>Data_selection</th>\n",
       "      <th>appln_submit_datetime</th>\n",
       "      <th>disbursementDateTime</th>\n",
       "      <th>Application_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1878354</td>\n",
       "      <td>20559e3f-43ed-4383-8d61-77821b466934</td>\n",
       "      <td>0.080499</td>\n",
       "      <td>2025-12-01T08:59:43.458980</td>\n",
       "      <td>2025-12-01T08:59:43.458980</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"cic_Personal_Loans_granted_contracts_amt_24M...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2023-01-29 18:32:36</td>\n",
       "      <td>2023-01-29 18:40:02</td>\n",
       "      <td>2023-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2033393</td>\n",
       "      <td>897bccad-ac68-4c02-aff6-53628030bfa6</td>\n",
       "      <td>0.126740</td>\n",
       "      <td>2025-12-01T09:00:34.513600</td>\n",
       "      <td>2025-12-01T09:00:34.513600</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"cic_days_since_last_inquiry\": 1967.0, \"cic_z...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2023-05-07 16:51:20</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2134557</td>\n",
       "      <td>a5d436ea-9086-4c5a-945c-0cd17589ef66</td>\n",
       "      <td>0.128572</td>\n",
       "      <td>2025-12-01T09:00:49.389048</td>\n",
       "      <td>2025-12-01T09:00:49.389048</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"cic_Personal_Loans_granted_contracts_amt_24M...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2024-09-20 17:33:41</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2134961</td>\n",
       "      <td>215caf75-9b47-49fb-bed1-30c6e153bdf5</td>\n",
       "      <td>0.189113</td>\n",
       "      <td>2025-12-01T09:00:22.123586</td>\n",
       "      <td>2025-12-01T09:00:22.123586</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"cic_days_since_last_inquiry\": 0.0, \"cic_vel_...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2025-02-09 12:17:30</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2203660</td>\n",
       "      <td>f7e4ec78-2085-4bf7-86a7-6a5d0516957e</td>\n",
       "      <td>0.123928</td>\n",
       "      <td>2025-12-01T09:00:25.037224</td>\n",
       "      <td>2025-12-01T09:00:25.037224</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"cic_days_since_last_inquiry\": 2124.0, \"cic_z...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2023-08-23 11:42:41</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2023-08</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerId                  digitalLoanAccountId  Alpha_cic_sil_score  \\\n",
       "0     1878354  20559e3f-43ed-4383-8d61-77821b466934             0.080499   \n",
       "1     2033393  897bccad-ac68-4c02-aff6-53628030bfa6             0.126740   \n",
       "2     2134557  a5d436ea-9086-4c5a-945c-0cd17589ef66             0.128572   \n",
       "3     2134961  215caf75-9b47-49fb-bed1-30c6e153bdf5             0.189113   \n",
       "4     2203660  f7e4ec78-2085-4bf7-86a7-6a5d0516957e             0.123928   \n",
       "\n",
       "                   start_time                    end_time modelDisplayName  \\\n",
       "0  2025-12-01T08:59:43.458980  2025-12-01T08:59:43.458980    cic_model_sil   \n",
       "1  2025-12-01T09:00:34.513600  2025-12-01T09:00:34.513600    cic_model_sil   \n",
       "2  2025-12-01T09:00:49.389048  2025-12-01T09:00:49.389048    cic_model_sil   \n",
       "3  2025-12-01T09:00:22.123586  2025-12-01T09:00:22.123586    cic_model_sil   \n",
       "4  2025-12-01T09:00:25.037224  2025-12-01T09:00:25.037224    cic_model_sil   \n",
       "\n",
       "  modelVersionId new_loan_type gender loan_product_type   osType  \\\n",
       "0             v1   SIL-Instore      M         Appliance  android   \n",
       "1             v1   SIL-Instore      F         Appliance  android   \n",
       "2             v1   SIL-Instore      F         Appliance  android   \n",
       "3             v1   SIL-Instore      M         Appliance  android   \n",
       "4             v1   SIL-Instore      M         Appliance  android   \n",
       "\n",
       "              Model_Name product trenchCategory  \\\n",
       "0  Alpha - CIC-SIL-Model     SIL            ALL   \n",
       "1  Alpha - CIC-SIL-Model     SIL            ALL   \n",
       "2  Alpha - CIC-SIL-Model     SIL            ALL   \n",
       "3  Alpha - CIC-SIL-Model     SIL            ALL   \n",
       "4  Alpha - CIC-SIL-Model     SIL            ALL   \n",
       "\n",
       "                                        calcFeatures Data_selection  \\\n",
       "0  {\"cic_Personal_Loans_granted_contracts_amt_24M...          Train   \n",
       "1  {\"cic_days_since_last_inquiry\": 1967.0, \"cic_z...          Train   \n",
       "2  {\"cic_Personal_Loans_granted_contracts_amt_24M...          Train   \n",
       "3  {\"cic_days_since_last_inquiry\": 0.0, \"cic_vel_...          Train   \n",
       "4  {\"cic_days_since_last_inquiry\": 2124.0, \"cic_z...          Train   \n",
       "\n",
       "  appln_submit_datetime disbursementDateTime Application_month  \n",
       "0   2023-01-29 18:32:36  2023-01-29 18:40:02           2023-01  \n",
       "1   2023-05-07 16:51:20                  NaT           2023-05  \n",
       "2   2024-09-20 17:33:41                  NaT           2024-09  \n",
       "3   2025-02-09 12:17:30                  NaT           2025-02  \n",
       "4   2023-08-23 11:42:41                  NaT           2023-08  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq = \"\"\"WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "    \n",
    "    case when modelDisplayName = 'Alpha - CIC-SIL-Model' then 'cic_model_sil' else modelDisplayName end as modelDisplayName\n",
    "    \n",
    "    ,modelVersionId,\n",
    "        case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory = '' then 'ALL'\n",
    "    else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in ('Alpha - CIC-SIL-Model', 'cic_model_sil')\n",
    "  ),\n",
    "base as \n",
    "(SELECT distinct\n",
    "  r.customerId,r.digitalLoanAccountId,prediction Alpha_cic_sil_score\n",
    "    ,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "   loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "        when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "        when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "        else 'ios' end osType,\n",
    " 'Alpha - CIC-SIL-Model' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Train' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime))) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId, modelVersionId \n",
    "order by   coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) desc) = 1\n",
    ")\n",
    "select * from base\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "94300df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "d2 = dfd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "146a7e92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>digitalLoanAccountId</th>\n",
       "      <th>Alpha_cic_sil_score</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>modelDisplayName</th>\n",
       "      <th>modelVersionId</th>\n",
       "      <th>new_loan_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>loan_product_type</th>\n",
       "      <th>osType</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>product</th>\n",
       "      <th>trenchCategory</th>\n",
       "      <th>calcFeatures</th>\n",
       "      <th>Data_selection</th>\n",
       "      <th>appln_submit_datetime</th>\n",
       "      <th>disbursementDateTime</th>\n",
       "      <th>Application_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2147044</td>\n",
       "      <td>aef56954-6c97-4520-aa14-2e163676f889</td>\n",
       "      <td>0.20132748968215752</td>\n",
       "      <td>2025-08-23 05:04:52.388799</td>\n",
       "      <td>2025-08-23 05:04:52.395256</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL Competitor</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1755907200000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-08-23 13:04:42</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2551996</td>\n",
       "      <td>81cf347d-8d6c-41ee-a67c-043c0a91e9de</td>\n",
       "      <td>0.1214890420101941</td>\n",
       "      <td>2025-11-04 10:54:49.330879</td>\n",
       "      <td>2025-11-04 10:54:49.337092</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1762214400000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-11-04 18:53:56</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2700374</td>\n",
       "      <td>73de3961-c48b-40a0-b961-84c9adb10e4a</td>\n",
       "      <td>0.19584553244636096</td>\n",
       "      <td>2025-06-15 07:14:26.730021</td>\n",
       "      <td>2025-06-15 07:14:26.736034</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1749945600000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-06-15 15:14:18</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2741007</td>\n",
       "      <td>28222315-a058-4b2e-858e-689a3837a08d</td>\n",
       "      <td>0.12350283937750572</td>\n",
       "      <td>2025-06-12 04:54:47.541738</td>\n",
       "      <td>2025-06-12 04:54:47.547863</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1749686400000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-06-12 12:54:37</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2907116</td>\n",
       "      <td>b22a1f11-eac7-4a15-96dd-db53f75e452b</td>\n",
       "      <td>0.16215984839484424</td>\n",
       "      <td>2025-11-12 03:32:13.445054</td>\n",
       "      <td>2025-11-12 03:32:13.451666</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL Competitor</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1762905600000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-11-12 11:32:03</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-11</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customerId                  digitalLoanAccountId  Alpha_cic_sil_score  \\\n",
       "0    2147044  aef56954-6c97-4520-aa14-2e163676f889  0.20132748968215752   \n",
       "1    2551996  81cf347d-8d6c-41ee-a67c-043c0a91e9de   0.1214890420101941   \n",
       "2    2700374  73de3961-c48b-40a0-b961-84c9adb10e4a  0.19584553244636096   \n",
       "3    2741007  28222315-a058-4b2e-858e-689a3837a08d  0.12350283937750572   \n",
       "4    2907116  b22a1f11-eac7-4a15-96dd-db53f75e452b  0.16215984839484424   \n",
       "\n",
       "                   start_time                    end_time modelDisplayName  \\\n",
       "0  2025-08-23 05:04:52.388799  2025-08-23 05:04:52.395256    cic_model_sil   \n",
       "1  2025-11-04 10:54:49.330879  2025-11-04 10:54:49.337092    cic_model_sil   \n",
       "2  2025-06-15 07:14:26.730021  2025-06-15 07:14:26.736034    cic_model_sil   \n",
       "3  2025-06-12 04:54:47.541738  2025-06-12 04:54:47.547863    cic_model_sil   \n",
       "4  2025-11-12 03:32:13.445054  2025-11-12 03:32:13.451666    cic_model_sil   \n",
       "\n",
       "  modelVersionId   new_loan_type gender loan_product_type   osType  \\\n",
       "0             v1  SIL Competitor      F         Appliance  android   \n",
       "1             v1     SIL-Instore      M         Appliance  android   \n",
       "2             v1     SIL-Instore      F         Appliance  android   \n",
       "3             v1     SIL-Instore      M         Appliance  android   \n",
       "4             v1  SIL Competitor      F         Appliance  android   \n",
       "\n",
       "      Model_Name product trenchCategory  \\\n",
       "0  cic_model_sil     SIL            ALL   \n",
       "1  cic_model_sil     SIL            ALL   \n",
       "2  cic_model_sil     SIL            ALL   \n",
       "3  cic_model_sil     SIL            ALL   \n",
       "4  cic_model_sil     SIL            ALL   \n",
       "\n",
       "                                        calcFeatures Data_selection  \\\n",
       "0  {\"run_date\":1755907200000,\"cic_Personal_Loans_...           Test   \n",
       "1  {\"run_date\":1762214400000,\"cic_Personal_Loans_...           Test   \n",
       "2  {\"run_date\":1749945600000,\"cic_Personal_Loans_...           Test   \n",
       "3  {\"run_date\":1749686400000,\"cic_Personal_Loans_...           Test   \n",
       "4  {\"run_date\":1762905600000,\"cic_Personal_Loans_...           Test   \n",
       "\n",
       "  appln_submit_datetime disbursementDateTime Application_month  \n",
       "0   2025-08-23 13:04:42                  NaT           2025-08  \n",
       "1   2025-11-04 18:53:56                  NaT           2025-11  \n",
       "2   2025-06-15 15:14:18                  NaT           2025-06  \n",
       "3   2025-06-12 12:54:37                  NaT           2025-06  \n",
       "4   2025-11-12 11:32:03                  NaT           2025-11  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_concat = pd.concat([d1, d2], ignore_index=True)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "da10ab8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the concatenated dataframe is: (551845, 19)\n",
      "The shape of the dataframe after dropping duplicates is: (551845, 19)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")\n",
    "df = dropping_duplicates(df_concat)\n",
    "print(f\"The shape of the dataframe after dropping duplicates is: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "b93441d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['customerId', 'digitalLoanAccountId', 'Alpha_cic_sil_score',\n",
       "       'start_time', 'end_time', 'modelDisplayName', 'modelVersionId',\n",
       "       'new_loan_type', 'gender', 'loan_product_type', 'osType',\n",
       "       'Model_Name', 'product', 'trenchCategory', 'calcFeatures',\n",
       "       'Data_selection', 'appln_submit_datetime', 'disbursementDateTime',\n",
       "       'Application_month'], dtype=object)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "750015de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelVersionId  trenchCategory  modelDisplayName\n",
       "v1              ALL             cic_model_sil       303511\n",
       "v2              Trench 1        cic_model_sil       226411\n",
       "                Trench 3        cic_model_sil        11585\n",
       "                Trench 2        cic_model_sil        10338\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['modelVersionId', 'trenchCategory', 'modelDisplayName']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "4ee54a4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding calcFeatures...\n",
      "Found 8 model-trench combinations\n",
      "\n",
      "\n",
      "Processing Model: cic_model_sil (ID: v2), Trenches: ['Trench 1']\n",
      "Found 19 features for this combination\n",
      "Creating binning strategy...\n",
      "Warning: Feature 'calc_total_overdue_granted_contracts' has insufficient variance\n",
      "  Feature 'calc_total_overdue_granted_contracts': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_nongranted_contracts_3M' has insufficient variance\n",
      "  Feature 'calc_cnt_nongranted_contracts_3M': Created 5 bins\n",
      "  Feature 'calc_max_amt_granted_24M': Created 10 bins\n",
      "  Feature 'calc_days_since_last_closed': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_nongranted_cnt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_nongranted_cnt_6on12': Created 5 bins\n",
      "  Feature 'calc_vel_contract_closed_amt_3on12': Created 3 bins\n",
      "Warning: Feature 'calc_granted_contracts_cnt_6M' has insufficient variance\n",
      "  Feature 'calc_granted_contracts_cnt_6M': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_active_contracts' has insufficient variance\n",
      "  Feature 'calc_cnt_active_contracts': Created 5 bins\n",
      "  Feature 'calc_tot_active_contracts_util': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_granted_amt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_granted_amt_6on12': Created 5 bins\n",
      "  Feature 'calc_Personal_Loans_granted_contracts_amt_24M': Created 10 bins\n",
      "  Feature 'calc_ScoreRange': Using 9 top categories (total: 19)\n",
      "  Feature 'calc_ln_loan_level_user_type': Using 1 top categories (total: 1)\n",
      "  Feature 'calc_flg_zero_non_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_has_ever_been_overdue': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_digitalLoanAccountId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_customerId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_crifApplicationId': Using 0 top categories (total: 0)\n",
      "Level 1: Model Score PSI\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Alpha_cic_sil_score'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m dim_segment, dim_feature, dim_month, fact_psi \u001b[38;5;241m=\u001b[39m create_comprehensive_psi_report(df)\n",
      "Cell \u001b[1;32mIn[78], line 469\u001b[0m, in \u001b[0;36mcreate_comprehensive_psi_report\u001b[1;34m(df, excluded_features, account_id_col, data_selection_col, month_col)\u001b[0m\n\u001b[0;32m    461\u001b[0m     feature_data\u001b[38;5;241m.\u001b[39mappend({\n\u001b[0;32m    462\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature_ID\u001b[39m\u001b[38;5;124m'\u001b[39m: feature_id_counter,\n\u001b[0;32m    463\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature_Name\u001b[39m\u001b[38;5;124m'\u001b[39m: feature,\n\u001b[0;32m    464\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFeature_Type\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNumerical\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    465\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDisplay_Name\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mModel Score\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    466\u001b[0m     })\n\u001b[0;32m    467\u001b[0m     feature_id_counter \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 469\u001b[0m results \u001b[38;5;241m=\u001b[39m calculate_psi_for_segments(segment_data_subset, feature, binning_info, segment_def)\n\u001b[0;32m    471\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m res \u001b[38;5;129;01min\u001b[39;00m results:\n\u001b[0;32m    472\u001b[0m     month \u001b[38;5;241m=\u001b[39m res[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMonth\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "Cell \u001b[1;32mIn[78], line 294\u001b[0m, in \u001b[0;36mcalculate_psi_for_segments\u001b[1;34m(df, feature, binning_info, segment_def, account_id_col, data_selection_col, month_col)\u001b[0m\n\u001b[0;32m    289\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;124;03mCalculate PSI for a specific feature and segment definition.\u001b[39;00m\n\u001b[0;32m    291\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    292\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m--> 294\u001b[0m df[\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfeature\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m_binned\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m apply_binning(df, feature, binning_info[feature])\n\u001b[0;32m    296\u001b[0m train_mask \u001b[38;5;241m=\u001b[39m df[data_selection_col] \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTrain\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    298\u001b[0m segment_mask \u001b[38;5;241m=\u001b[39m train_mask\u001b[38;5;241m.\u001b[39mcopy()\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Alpha_cic_sil_score'"
     ]
    }
   ],
   "source": [
    "dim_segment, dim_feature, dim_month, fact_psi = create_comprehensive_psi_report(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51dcc05c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanding calcFeatures...\n",
      "Found 8 model-trench combinations\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Processing Model: cic_model_sil (ID: v2), Trenches: ['Trench 1']\n",
      "================================================================================\n",
      "Found 19 features for this combination\n",
      "\n",
      "Creating binning strategy...\n",
      "Warning: Feature 'calc_total_overdue_granted_contracts' has insufficient variance\n",
      "  Feature 'calc_total_overdue_granted_contracts': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_nongranted_contracts_3M' has insufficient variance\n",
      "  Feature 'calc_cnt_nongranted_contracts_3M': Created 5 bins\n",
      "  Feature 'calc_max_amt_granted_24M': Created 10 bins\n",
      "  Feature 'calc_days_since_last_closed': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_nongranted_cnt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_nongranted_cnt_6on12': Created 5 bins\n",
      "  Feature 'calc_vel_contract_closed_amt_3on12': Created 3 bins\n",
      "Warning: Feature 'calc_granted_contracts_cnt_6M' has insufficient variance\n",
      "  Feature 'calc_granted_contracts_cnt_6M': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_active_contracts' has insufficient variance\n",
      "  Feature 'calc_cnt_active_contracts': Created 5 bins\n",
      "  Feature 'calc_tot_active_contracts_util': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_granted_amt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_granted_amt_6on12': Created 5 bins\n",
      "  Feature 'calc_Personal_Loans_granted_contracts_amt_24M': Created 10 bins\n",
      "  Feature 'calc_ScoreRange': Using 9 top categories (total: 19)\n",
      "  Feature 'calc_ln_loan_level_user_type': Using 1 top categories (total: 1)\n",
      "  Feature 'calc_flg_zero_non_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_has_ever_been_overdue': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_digitalLoanAccountId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_customerId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_crifApplicationId': Using 0 top categories (total: 0)\n",
      "\n",
      "--- Level 1: Model Score PSI ---\n",
      "  Feature 'Alpha_cic_sil_score': Using 9 top categories (total: 74009)\n",
      "--- Level 2: By Loan Product Type ---\n",
      "--- Level 3: By Loan Type ---\n",
      "--- Level 4: By OS Type ---\n",
      "\n",
      "================================================================================\n",
      "Processing Model: cic_model_sil (ID: v2), Trenches: ['Trench 2']\n",
      "================================================================================\n",
      "Found 19 features for this combination\n",
      "\n",
      "Creating binning strategy...\n",
      "Warning: Feature 'calc_total_overdue_granted_contracts' has insufficient variance\n",
      "  Feature 'calc_total_overdue_granted_contracts': Created 5 bins\n",
      "  Feature 'calc_max_amt_granted_24M': Created 10 bins\n",
      "  Feature 'calc_days_since_last_closed': Created 10 bins\n",
      "  Feature 'calc_vel_contract_nongranted_cnt_6on12': Created 3 bins\n",
      "  Feature 'calc_vel_contract_closed_amt_3on12': Created 3 bins\n",
      "Warning: Feature 'calc_granted_contracts_cnt_6M' has insufficient variance\n",
      "  Feature 'calc_granted_contracts_cnt_6M': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_active_contracts' has insufficient variance\n",
      "  Feature 'calc_cnt_active_contracts': Created 5 bins\n",
      "  Feature 'calc_tot_active_contracts_util': Created 10 bins\n",
      "  Feature 'calc_vel_contract_granted_amt_6on12': Created 3 bins\n",
      "  Feature 'calc_Personal_Loans_granted_contracts_amt_24M': Created 10 bins\n",
      "  Feature 'calc_ScoreRange': Using 9 top categories (total: 18)\n",
      "  Feature 'calc_ln_loan_level_user_type': Using 1 top categories (total: 1)\n",
      "  Feature 'calc_flg_zero_non_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_has_ever_been_overdue': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_cnt_nongranted_contracts_3M': Using 8 top categories (total: 8)\n",
      "  Feature 'calc_digitalLoanAccountId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_customerId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_crifApplicationId': Using 0 top categories (total: 0)\n",
      "\n",
      "--- Level 1: Model Score PSI ---\n",
      "  Feature 'Alpha_cic_sil_score': Using 9 top categories (total: 6708)\n",
      "--- Level 2: By Loan Product Type ---\n",
      "--- Level 3: By Loan Type ---\n",
      "--- Level 4: By OS Type ---\n",
      "\n",
      "================================================================================\n",
      "Processing Model: cic_model_sil (ID: v2), Trenches: ['Trench 3']\n",
      "================================================================================\n",
      "Found 19 features for this combination\n",
      "\n",
      "Creating binning strategy...\n",
      "Warning: Feature 'calc_total_overdue_granted_contracts' has insufficient variance\n",
      "  Feature 'calc_total_overdue_granted_contracts': Created 5 bins\n",
      "  Feature 'calc_max_amt_granted_24M': Created 10 bins\n",
      "  Feature 'calc_days_since_last_closed': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_nongranted_cnt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_nongranted_cnt_6on12': Created 5 bins\n",
      "Warning: Feature 'calc_vel_contract_closed_amt_3on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_closed_amt_3on12': Created 5 bins\n",
      "Warning: Feature 'calc_granted_contracts_cnt_6M' has insufficient variance\n",
      "  Feature 'calc_granted_contracts_cnt_6M': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_active_contracts' has insufficient variance\n",
      "  Feature 'calc_cnt_active_contracts': Created 5 bins\n",
      "  Feature 'calc_tot_active_contracts_util': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_granted_amt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_granted_amt_6on12': Created 5 bins\n",
      "  Feature 'calc_Personal_Loans_granted_contracts_amt_24M': Created 10 bins\n",
      "  Feature 'calc_ScoreRange': Using 9 top categories (total: 18)\n",
      "  Feature 'calc_ln_loan_level_user_type': Using 1 top categories (total: 1)\n",
      "  Feature 'calc_flg_zero_non_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_has_ever_been_overdue': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_cnt_nongranted_contracts_3M': Using 7 top categories (total: 7)\n",
      "  Feature 'calc_digitalLoanAccountId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_customerId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_crifApplicationId': Using 0 top categories (total: 0)\n",
      "\n",
      "--- Level 1: Model Score PSI ---\n",
      "  Feature 'Alpha_cic_sil_score': Using 9 top categories (total: 6418)\n",
      "--- Level 2: By Loan Product Type ---\n",
      "--- Level 3: By Loan Type ---\n",
      "--- Level 4: By OS Type ---\n",
      "\n",
      "================================================================================\n",
      "Processing Model: cic_model_sil (ID: v2), Trenches: ['Trench 1', 'Trench 2']\n",
      "================================================================================\n",
      "Found 19 features for this combination\n",
      "\n",
      "Creating binning strategy...\n",
      "Warning: Feature 'calc_total_overdue_granted_contracts' has insufficient variance\n",
      "  Feature 'calc_total_overdue_granted_contracts': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_nongranted_contracts_3M' has insufficient variance\n",
      "  Feature 'calc_cnt_nongranted_contracts_3M': Created 5 bins\n",
      "  Feature 'calc_max_amt_granted_24M': Created 10 bins\n",
      "  Feature 'calc_days_since_last_closed': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_nongranted_cnt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_nongranted_cnt_6on12': Created 5 bins\n",
      "  Feature 'calc_vel_contract_closed_amt_3on12': Created 3 bins\n",
      "Warning: Feature 'calc_granted_contracts_cnt_6M' has insufficient variance\n",
      "  Feature 'calc_granted_contracts_cnt_6M': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_active_contracts' has insufficient variance\n",
      "  Feature 'calc_cnt_active_contracts': Created 5 bins\n",
      "  Feature 'calc_tot_active_contracts_util': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_granted_amt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_granted_amt_6on12': Created 5 bins\n",
      "  Feature 'calc_Personal_Loans_granted_contracts_amt_24M': Created 10 bins\n",
      "  Feature 'calc_ScoreRange': Using 9 top categories (total: 19)\n",
      "  Feature 'calc_ln_loan_level_user_type': Using 1 top categories (total: 1)\n",
      "  Feature 'calc_flg_zero_non_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_has_ever_been_overdue': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_digitalLoanAccountId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_customerId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_crifApplicationId': Using 0 top categories (total: 0)\n",
      "\n",
      "--- Level 1: Model Score PSI ---\n",
      "  Feature 'Alpha_cic_sil_score': Using 9 top categories (total: 78599)\n",
      "--- Level 2: By Loan Product Type ---\n",
      "--- Level 3: By Loan Type ---\n",
      "--- Level 4: By OS Type ---\n",
      "\n",
      "================================================================================\n",
      "Processing Model: cic_model_sil (ID: v2), Trenches: ['Trench 1', 'Trench 3']\n",
      "================================================================================\n",
      "Found 19 features for this combination\n",
      "\n",
      "Creating binning strategy...\n",
      "Warning: Feature 'calc_total_overdue_granted_contracts' has insufficient variance\n",
      "  Feature 'calc_total_overdue_granted_contracts': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_nongranted_contracts_3M' has insufficient variance\n",
      "  Feature 'calc_cnt_nongranted_contracts_3M': Created 5 bins\n",
      "  Feature 'calc_max_amt_granted_24M': Created 10 bins\n",
      "  Feature 'calc_days_since_last_closed': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_nongranted_cnt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_nongranted_cnt_6on12': Created 5 bins\n",
      "  Feature 'calc_vel_contract_closed_amt_3on12': Created 3 bins\n",
      "Warning: Feature 'calc_granted_contracts_cnt_6M' has insufficient variance\n",
      "  Feature 'calc_granted_contracts_cnt_6M': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_active_contracts' has insufficient variance\n",
      "  Feature 'calc_cnt_active_contracts': Created 5 bins\n",
      "  Feature 'calc_tot_active_contracts_util': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_granted_amt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_granted_amt_6on12': Created 5 bins\n",
      "  Feature 'calc_Personal_Loans_granted_contracts_amt_24M': Created 10 bins\n",
      "  Feature 'calc_ScoreRange': Using 9 top categories (total: 19)\n",
      "  Feature 'calc_ln_loan_level_user_type': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_non_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_has_ever_been_overdue': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_digitalLoanAccountId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_customerId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_crifApplicationId': Using 0 top categories (total: 0)\n",
      "\n",
      "--- Level 1: Model Score PSI ---\n",
      "  Feature 'Alpha_cic_sil_score': Using 9 top categories (total: 80427)\n",
      "--- Level 2: By Loan Product Type ---\n",
      "--- Level 3: By Loan Type ---\n",
      "--- Level 4: By OS Type ---\n",
      "\n",
      "================================================================================\n",
      "Processing Model: cic_model_sil (ID: v2), Trenches: ['Trench 2', 'Trench 3']\n",
      "================================================================================\n",
      "Found 19 features for this combination\n",
      "\n",
      "Creating binning strategy...\n",
      "Warning: Feature 'calc_total_overdue_granted_contracts' has insufficient variance\n",
      "  Feature 'calc_total_overdue_granted_contracts': Created 5 bins\n",
      "  Feature 'calc_max_amt_granted_24M': Created 10 bins\n",
      "  Feature 'calc_days_since_last_closed': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_nongranted_cnt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_nongranted_cnt_6on12': Created 5 bins\n",
      "Warning: Feature 'calc_vel_contract_closed_amt_3on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_closed_amt_3on12': Created 5 bins\n",
      "Warning: Feature 'calc_granted_contracts_cnt_6M' has insufficient variance\n",
      "  Feature 'calc_granted_contracts_cnt_6M': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_active_contracts' has insufficient variance\n",
      "  Feature 'calc_cnt_active_contracts': Created 5 bins\n",
      "  Feature 'calc_tot_active_contracts_util': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_granted_amt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_granted_amt_6on12': Created 5 bins\n",
      "  Feature 'calc_Personal_Loans_granted_contracts_amt_24M': Created 10 bins\n",
      "  Feature 'calc_ScoreRange': Using 9 top categories (total: 18)\n",
      "  Feature 'calc_ln_loan_level_user_type': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_non_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_has_ever_been_overdue': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_cnt_nongranted_contracts_3M': Using 8 top categories (total: 8)\n",
      "  Feature 'calc_digitalLoanAccountId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_customerId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_crifApplicationId': Using 0 top categories (total: 0)\n",
      "\n",
      "--- Level 1: Model Score PSI ---\n",
      "  Feature 'Alpha_cic_sil_score': Using 9 top categories (total: 13126)\n",
      "--- Level 2: By Loan Product Type ---\n",
      "--- Level 3: By Loan Type ---\n",
      "--- Level 4: By OS Type ---\n",
      "\n",
      "================================================================================\n",
      "Processing Model: cic_model_sil (ID: v2), Trenches: ['Trench 1', 'Trench 2', 'Trench 3']\n",
      "================================================================================\n",
      "Found 19 features for this combination\n",
      "\n",
      "Creating binning strategy...\n",
      "Warning: Feature 'calc_total_overdue_granted_contracts' has insufficient variance\n",
      "  Feature 'calc_total_overdue_granted_contracts': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_nongranted_contracts_3M' has insufficient variance\n",
      "  Feature 'calc_cnt_nongranted_contracts_3M': Created 5 bins\n",
      "  Feature 'calc_max_amt_granted_24M': Created 10 bins\n",
      "  Feature 'calc_days_since_last_closed': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_nongranted_cnt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_nongranted_cnt_6on12': Created 5 bins\n",
      "  Feature 'calc_vel_contract_closed_amt_3on12': Created 3 bins\n",
      "Warning: Feature 'calc_granted_contracts_cnt_6M' has insufficient variance\n",
      "  Feature 'calc_granted_contracts_cnt_6M': Created 5 bins\n",
      "Warning: Feature 'calc_cnt_active_contracts' has insufficient variance\n",
      "  Feature 'calc_cnt_active_contracts': Created 5 bins\n",
      "  Feature 'calc_tot_active_contracts_util': Created 10 bins\n",
      "Warning: Feature 'calc_vel_contract_granted_amt_6on12' has insufficient variance\n",
      "  Feature 'calc_vel_contract_granted_amt_6on12': Created 5 bins\n",
      "  Feature 'calc_Personal_Loans_granted_contracts_amt_24M': Created 10 bins\n",
      "  Feature 'calc_ScoreRange': Using 9 top categories (total: 19)\n",
      "  Feature 'calc_ln_loan_level_user_type': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_non_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_flg_zero_granted_ever': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_has_ever_been_overdue': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_digitalLoanAccountId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_customerId': Using 0 top categories (total: 0)\n",
      "  Feature 'calc_crifApplicationId': Using 0 top categories (total: 0)\n",
      "\n",
      "--- Level 1: Model Score PSI ---\n",
      "  Feature 'Alpha_cic_sil_score': Using 9 top categories (total: 85017)\n",
      "--- Level 2: By Loan Product Type ---\n",
      "--- Level 3: By Loan Type ---\n",
      "--- Level 4: By OS Type ---\n",
      "\n",
      "================================================================================\n",
      "Processing Model: cic_model_sil (ID: v1), Trenches: ['ALL']\n",
      "================================================================================\n",
      "Found 10 features for this combination\n",
      "\n",
      "Creating binning strategy...\n",
      "  Feature 'calc_cic_days_since_last_inquiry': Created 5 bins\n",
      "Warning: Feature 'calc_cic_cnt_active_contracts' has insufficient variance\n",
      "  Feature 'calc_cic_cnt_active_contracts': Created 5 bins\n",
      "  Feature 'calc_cic_max_amt_granted_24M': Created 10 bins\n",
      "  Feature 'calc_cic_tot_active_contracts_util': Created 10 bins\n",
      "Warning: Feature 'calc_cic_vel_contract_granted_amt_12on24' has insufficient variance\n",
      "  Feature 'calc_cic_vel_contract_granted_amt_12on24': Created 5 bins\n",
      "  Feature 'calc_cic_Personal_Loans_granted_contracts_amt_24M': Created 10 bins\n",
      "Warning: Feature 'calc_cic_vel_contract_nongranted_cnt_12on24' has insufficient variance\n",
      "  Feature 'calc_cic_vel_contract_nongranted_cnt_12on24': Created 5 bins\n",
      "  Feature 'calc_cic_zero_non_granted_ever_flag': Using 2 top categories (total: 2)\n",
      "  Feature 'calc_cic_zero_granted_ever_flag': Using 2 top categories (total: 2)\n",
      "\n",
      "--- Level 1: Model Score PSI ---\n",
      "  Feature 'Alpha_cic_sil_score': Using 9 top categories (total: 90868)\n",
      "--- Level 2: By Loan Product Type ---\n",
      "--- Level 3: By Loan Type ---\n",
      "--- Level 4: By OS Type ---\n",
      "\n",
      "================================================================================\n",
      "Total PSI calculations: 2576\n",
      "Unique segments: 65\n",
      "Unique features analyzed: 30\n",
      "================================================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# psi_report = create_comprehensive_psi_report(\n",
    "#     df,  # Your concatenated train+test dataframe\n",
    "#     excluded_features=['crifapplicationid', 'run_date', 'customerId', 'digitalLoanAccountId']\n",
    "# )\n",
    "# psi_report.to_csv('comprehensive_psi_report.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e0f924aa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Segment_ID</th>\n",
       "      <th>Segment_Name</th>\n",
       "      <th>Model_Display_Name</th>\n",
       "      <th>Model_Version_ID</th>\n",
       "      <th>Segment_Column_1</th>\n",
       "      <th>Segment_Value_1</th>\n",
       "      <th>Segment_Column_2</th>\n",
       "      <th>Segment_Value_2</th>\n",
       "      <th>Segment_Column_3</th>\n",
       "      <th>Segment_Value_3</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_Type</th>\n",
       "      <th>Month</th>\n",
       "      <th>Base_Count</th>\n",
       "      <th>Actual_Count</th>\n",
       "      <th>Expected_Percentage</th>\n",
       "      <th>Actual_Percentage</th>\n",
       "      <th>PSI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>categorical</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>224718</td>\n",
       "      <td>836</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>0.596118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>categorical</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>224718</td>\n",
       "      <td>858</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>0.547015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1_LoanProduct_Appl...</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>calc_Personal_Loans_granted_contracts_amt_24M</td>\n",
       "      <td>numerical</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>180245</td>\n",
       "      <td>658</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.218863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1_LoanProduct_Appl...</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>calc_Personal_Loans_granted_contracts_amt_24M</td>\n",
       "      <td>numerical</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>180245</td>\n",
       "      <td>635</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>9.090909</td>\n",
       "      <td>0.214828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1_LoanProduct_Appl...</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>calc_ScoreRange</td>\n",
       "      <td>categorical</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>180245</td>\n",
       "      <td>658</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.185771</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1_LoanProduct_Appl...</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>calc_ScoreRange</td>\n",
       "      <td>categorical</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>180245</td>\n",
       "      <td>635</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.223081</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1_LoanProduct_Appl...</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>calc_cnt_active_contracts</td>\n",
       "      <td>numerical</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>180245</td>\n",
       "      <td>658</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.060040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>1</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1_LoanProduct_Appl...</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>calc_cnt_active_contracts</td>\n",
       "      <td>numerical</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>180245</td>\n",
       "      <td>635</td>\n",
       "      <td>16.666667</td>\n",
       "      <td>33.333333</td>\n",
       "      <td>0.084973</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1_LoanProduct_Appl...</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>calc_cnt_nongranted_contracts_3M</td>\n",
       "      <td>numerical</td>\n",
       "      <td>2025-11</td>\n",
       "      <td>180245</td>\n",
       "      <td>658</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.033295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>cic_model_sil_Trench_Trench 1_LoanProduct_Appl...</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>modelVersionId</td>\n",
       "      <td>v2</td>\n",
       "      <td>trenchCategory</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>calc_cnt_nongranted_contracts_3M</td>\n",
       "      <td>numerical</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>180245</td>\n",
       "      <td>635</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>0.023122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Segment_ID                                       Segment_Name  \\\n",
       "0           0                      cic_model_sil_Trench_Trench 1   \n",
       "1           0                      cic_model_sil_Trench_Trench 1   \n",
       "2           1  cic_model_sil_Trench_Trench 1_LoanProduct_Appl...   \n",
       "3           1  cic_model_sil_Trench_Trench 1_LoanProduct_Appl...   \n",
       "4           1  cic_model_sil_Trench_Trench 1_LoanProduct_Appl...   \n",
       "5           1  cic_model_sil_Trench_Trench 1_LoanProduct_Appl...   \n",
       "6           1  cic_model_sil_Trench_Trench 1_LoanProduct_Appl...   \n",
       "7           1  cic_model_sil_Trench_Trench 1_LoanProduct_Appl...   \n",
       "8           1  cic_model_sil_Trench_Trench 1_LoanProduct_Appl...   \n",
       "9           1  cic_model_sil_Trench_Trench 1_LoanProduct_Appl...   \n",
       "\n",
       "  Model_Display_Name Model_Version_ID Segment_Column_1 Segment_Value_1  \\\n",
       "0      cic_model_sil               v2   modelVersionId              v2   \n",
       "1      cic_model_sil               v2   modelVersionId              v2   \n",
       "2      cic_model_sil               v2   modelVersionId              v2   \n",
       "3      cic_model_sil               v2   modelVersionId              v2   \n",
       "4      cic_model_sil               v2   modelVersionId              v2   \n",
       "5      cic_model_sil               v2   modelVersionId              v2   \n",
       "6      cic_model_sil               v2   modelVersionId              v2   \n",
       "7      cic_model_sil               v2   modelVersionId              v2   \n",
       "8      cic_model_sil               v2   modelVersionId              v2   \n",
       "9      cic_model_sil               v2   modelVersionId              v2   \n",
       "\n",
       "  Segment_Column_2 Segment_Value_2   Segment_Column_3 Segment_Value_3  \\\n",
       "0   trenchCategory        Trench 1                NaN             NaN   \n",
       "1   trenchCategory        Trench 1                NaN             NaN   \n",
       "2   trenchCategory        Trench 1  loan_product_type       Appliance   \n",
       "3   trenchCategory        Trench 1  loan_product_type       Appliance   \n",
       "4   trenchCategory        Trench 1  loan_product_type       Appliance   \n",
       "5   trenchCategory        Trench 1  loan_product_type       Appliance   \n",
       "6   trenchCategory        Trench 1  loan_product_type       Appliance   \n",
       "7   trenchCategory        Trench 1  loan_product_type       Appliance   \n",
       "8   trenchCategory        Trench 1  loan_product_type       Appliance   \n",
       "9   trenchCategory        Trench 1  loan_product_type       Appliance   \n",
       "\n",
       "                                         Feature Feature_Type    Month  \\\n",
       "0                            Alpha_cic_sil_score  categorical  2025-11   \n",
       "1                            Alpha_cic_sil_score  categorical  2025-12   \n",
       "2  calc_Personal_Loans_granted_contracts_amt_24M    numerical  2025-11   \n",
       "3  calc_Personal_Loans_granted_contracts_amt_24M    numerical  2025-12   \n",
       "4                                calc_ScoreRange  categorical  2025-11   \n",
       "5                                calc_ScoreRange  categorical  2025-12   \n",
       "6                      calc_cnt_active_contracts    numerical  2025-11   \n",
       "7                      calc_cnt_active_contracts    numerical  2025-12   \n",
       "8               calc_cnt_nongranted_contracts_3M    numerical  2025-11   \n",
       "9               calc_cnt_nongranted_contracts_3M    numerical  2025-12   \n",
       "\n",
       "   Base_Count  Actual_Count  Expected_Percentage  Actual_Percentage       PSI  \n",
       "0      224718           836            10.000000          25.000000  0.596118  \n",
       "1      224718           858            10.000000          16.666667  0.547015  \n",
       "2      180245           658             9.090909           9.090909  0.218863  \n",
       "3      180245           635             9.090909           9.090909  0.214828  \n",
       "4      180245           658            10.000000          10.000000  0.185771  \n",
       "5      180245           635            10.000000          10.000000  0.223081  \n",
       "6      180245           658            16.666667          33.333333  0.060040  \n",
       "7      180245           635            16.666667          33.333333  0.084973  \n",
       "8      180245           658            20.000000          50.000000  0.033295  \n",
       "9      180245           635            20.000000          50.000000  0.023122  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_report.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "f160ea5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['calc_Personal_Loans_granted_contracts_amt_24M', 'calc_ScoreRange',\n",
       "       'calc_cnt_active_contracts', 'calc_cnt_nongranted_contracts_3M',\n",
       "       'calc_crifApplicationId', 'calc_customerId',\n",
       "       'calc_days_since_last_closed', 'calc_digitalLoanAccountId',\n",
       "       'calc_flg_zero_granted_ever', 'calc_flg_zero_non_granted_ever',\n",
       "       'calc_granted_contracts_cnt_6M', 'calc_has_ever_been_overdue',\n",
       "       'calc_ln_loan_level_user_type', 'calc_max_amt_granted_24M',\n",
       "       'calc_tot_active_contracts_util',\n",
       "       'calc_total_overdue_granted_contracts',\n",
       "       'calc_vel_contract_closed_amt_3on12',\n",
       "       'calc_vel_contract_granted_amt_6on12',\n",
       "       'calc_vel_contract_nongranted_cnt_6on12',\n",
       "       'calc_cic_Personal_Loans_granted_contracts_amt_24M',\n",
       "       'calc_cic_cnt_active_contracts',\n",
       "       'calc_cic_days_since_last_inquiry', 'calc_cic_max_amt_granted_24M',\n",
       "       'calc_cic_tot_active_contracts_util',\n",
       "       'calc_cic_vel_contract_granted_amt_12on24',\n",
       "       'calc_cic_vel_contract_nongranted_cnt_12on24',\n",
       "       'calc_cic_zero_granted_ever_flag',\n",
       "       'calc_cic_zero_non_granted_ever_flag', 'calc_run_date'],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_report['Feature'][psi_report['Segment_Value_3'] == 'Appliance'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0494b9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ============================================================================\n",
    "# # USAGE EXAMPLE\n",
    "# # ============================================================================\n",
    "\n",
    "# # Assuming you have df with concatenated Train and Test data\n",
    "\n",
    "# # Calculate Overall PSI (Overall + By Segments)\n",
    "# psi_results = calculate_psi_by_model_version(\n",
    "#     df=df.copy(),\n",
    "#     score_column='Alpha_cic_sil_score',\n",
    "#     segment_columns=['new_loan_type', 'loan_product_type', 'osType', 'trenchCategory'],\n",
    "#     month_col='Application_month',\n",
    "#     data_selection_col='Data_selection',\n",
    "#     model_version_col='modelVersionId',\n",
    "#     # trench_category_col = 'trenchCategory',\n",
    "#     model_display_name_col = 'modelDisplayName',\n",
    "#     account_id_col='digitalLoanAccountId'\n",
    "# )\n",
    "\n",
    "# print(psi_results.head(20))\n",
    "# print(f\"\\nTotal rows: {len(psi_results)}\")\n",
    "\n",
    "# # View Overall results only\n",
    "# # overall_results = psi_results[psi_results['Segment_Column'] == 'Overall']\n",
    "# # print(overall_results.head())\n",
    "\n",
    "# # # View specific model version\n",
    "# # v1_results = psi_results[psi_results['Model_Version'] == 'v1']\n",
    "# # print(v1_results)\n",
    "\n",
    "# # # View specific segment\n",
    "# # loan_type_results = psi_results[psi_results['Segment_Column'] == 'new_loan_type']\n",
    "# # print(loan_type_results)\n",
    "\n",
    "# # Save results\n",
    "# psi_results.to_csv(r'D:\\OneDrive - Tonik Financial Pte Ltd\\MyStuff\\Data Engineering\\Model_Monitoring\\PSI Monitoring\\NEW_MONITORING_DASHBOARD_20251125\\Future\\psi_results_overall_and_segments.csv', index=False)\n",
    "\n",
    "# # # ---- For Bin-Level Details ----\n",
    "# # bin_psi_results = calculate_bin_level_psi_by_model_version(\n",
    "# #     df=df.copy(),\n",
    "# #     score_column='Alpha_cic_sil_score',\n",
    "# #     segment_columns=['new_loan_type', 'loan_product_type', 'osType']\n",
    "# # )\n",
    "\n",
    "# # bin_psi_results.to_csv('bin_level_psi_results_overall_and_segments.csv', index=False)\n",
    "# # print(bin_psi_results.head(30))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3890bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results[['Model_Version', 'Trench_Category', 'Model_Display_Name', 'Feature']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "321c2f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# psi_results['modelDisplayName'] = psi_results['Model_Version'].apply(lambda x: 'cic_model_sil' if x in ['v1', 'v2'] else 'Alpha - CIC-SIL-Model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81e1997",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cic_sil_model_psi_v5\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a42b31cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae753c36",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3d05fc1-d5ef-40e4-9693-54f93a29ea42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n",
    "df1.sample(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5667ddb1-1814-45d1-9728-21b6c56b630d",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc60fd-bb44-4d67-902f-375b0f6fa556",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in ('Alpha - CIC-SIL-Model', 'cic_model_sil')\n",
    "  and modelVersionId='v1'\n",
    "  )\n",
    "SELECT distinct\n",
    "  r.customerId,r.digitalLoanAccountId,prediction Alpha_cic_sil_score\n",
    "    ,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "   loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "        when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "        when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "        else 'ios' end osType,\n",
    " 'Alpha - CIC-SIL-Model' Model_Name,\n",
    " 'SIL' as product,\n",
    " 'NA' trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Train' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime))) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId order by cast(r.start_time as datetime) desc) = 1\n",
    ";\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "097bc1b2-ea64-4349-9a08-249493d1246f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()\n",
    "df2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "787f9969-825d-4255-bd6b-629276d7899a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc2c75c-36c9-4be2-889b-687bc2de6fa1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef792b9-e81d-4678-8e1e-7bb24ca4229d",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c564e812-67ff-4651-96bb-415d4f90f035",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "# df_concat.head()\n",
    "\n",
    "# 1) Get all IDs present in Train\n",
    "train_ids = set(df2['digitalLoanAccountId'])\n",
    "\n",
    "# 2) Keep only Test rows whose ID is NOT in Train\n",
    "df1_no_dupes = df1[~df1['digitalLoanAccountId'].isin(train_ids)]\n",
    "\n",
    "# 3) Concatenate\n",
    "df_concat = pd.concat([df1_no_dupes, df2], ignore_index=True)\n",
    "\n",
    "print(f\"The shape of the concatenated dataframe is:\\t {df_concat.shape}\")\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ffe5f6-bdae-48ea-91bb-a198f2514da5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat.groupby(['Data_selection','Application_month'])['digitalLoanAccountId'].nunique().reset_index().sort_values(by=['Application_month','Data_selection'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bd3463-6e28-4b35-983a-6dc7478dfbc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585a7381-d75d-4401-b596-190b655652d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Alpha_cic_sil_score'] = pd.to_numeric(df['Alpha_cic_sil_score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17c2527-1e5d-4a15-ab8e-120eb8bef634",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b53159-01ea-491c-b436-d3b012690bc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['Alpha_cic_sil_score',  'calc_cic_Personal_Loans_granted_contracts_amt_24M',\n",
    "       'calc_cic_days_since_last_inquiry', 'calc_cic_cnt_active_contracts',\n",
    "       'calc_cic_vel_contract_nongranted_cnt_12on24',\n",
    "       'calc_cic_max_amt_granted_24M', 'calc_cic_zero_non_granted_ever_flag',\n",
    "       'calc_cic_tot_active_contracts_util',\n",
    "       'calc_cic_vel_contract_granted_amt_12on24',\n",
    "       'calc_cic_zero_granted_ever_flag']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cic_sil_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cic_sil_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de4fc68e-f1b0-460c-a9cb-53041f9ce8ce",
   "metadata": {},
   "source": [
    "#### Alpha - IncomeEstimationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918c18d-90e9-44fa-91fe-00dc1b6a002b",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "978acda3-65f7-46bb-9ddc-b93c2a28a72d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName = 'Alpha  - IncomeEstimationModel'\n",
    "    and modelVersionId='v1'\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction Alpha_Income_Estimated_score,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "  loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "    case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    "  'Alpha  - IncomeEstimationModel' Model_Name,\n",
    "  'SIL' as product,\n",
    "  'NA' trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Test' Data_selection,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_cic_credit_avg_credit_limit\") AS inc_alpha_cic_credit_avg_credit_limit,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_cic_max_active_contracts_amt\") AS inc_alpha_cic_max_active_contracts_amt,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_age\") AS inc_alpha_ln_age,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_doc_type_rolled\") AS inc_alpha_doc_type_rolled,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_brand\") AS inc_alpha_ln_brand,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_city\") AS inc_alpha_ln_city,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_cnt_dependents\") AS inc_alpha_ln_cnt_dependents,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_education_level\") AS inc_alpha_ln_education_level,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_employment_type_new\") AS inc_alpha_ln_employment_type_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_gender\") AS inc_alpha_ln_gender,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_industry_new\") AS inc_alpha_ln_industry_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_loan_prod_type\") AS inc_alpha_ln_loan_prod_type,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_marital_status_new\") AS inc_alpha_ln_marital_status_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_nature_of_work_new\") AS inc_alpha_ln_nature_of_work_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_osversion_bin\") AS inc_alpha_ln_osversion_bin,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_purpose\") AS inc_alpha_ln_purpose,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_source_of_funds_new\") AS inc_alpha_ln_source_of_funds_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_encoded_company_name_grouped\") AS inc_alpha_encoded_company_name_grouped,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId order by r.start_time desc) = 1\n",
    "  ;\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9109546f-0a51-46f9-b3dd-77ec3ba82420",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "df1 = dfd.copy()\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {df1.shape[1]}\")\n",
    "df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9ca0511-9bf0-4875-9868-9ec6f4677d54",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7040c436-b555-4654-85d2-28e5b1697f50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName = 'Alpha  - IncomeEstimationModel'\n",
    "  and modelVersionId='v1'\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction Alpha_Income_Estimated_score,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "  loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "    case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    "  'Alpha  - IncomeEstimationModel' Model_Name,\n",
    "  'SIL' as product,\n",
    "  'NA' trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Train' Data_selection,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_cic_credit_avg_credit_limit\") AS inc_alpha_cic_credit_avg_credit_limit,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_cic_max_active_contracts_amt\") AS inc_alpha_cic_max_active_contracts_amt,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_age\") AS inc_alpha_ln_age,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_doc_type_rolled\") AS inc_alpha_doc_type_rolled,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_brand\") AS inc_alpha_ln_brand,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_city\") AS inc_alpha_ln_city,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_cnt_dependents\") AS inc_alpha_ln_cnt_dependents,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_education_level\") AS inc_alpha_ln_education_level,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_employment_type_new\") AS inc_alpha_ln_employment_type_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_gender\") AS inc_alpha_ln_gender,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_industry_new\") AS inc_alpha_ln_industry_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_loan_prod_type\") AS inc_alpha_ln_loan_prod_type,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_marital_status_new\") AS inc_alpha_ln_marital_status_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_nature_of_work_new\") AS inc_alpha_ln_nature_of_work_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_osversion_bin\") AS inc_alpha_ln_osversion_bin,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_purpose\") AS inc_alpha_ln_purpose,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_ln_source_of_funds_new\") AS inc_alpha_ln_source_of_funds_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_alpha_encoded_company_name_grouped\") AS inc_alpha_encoded_company_name_grouped,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime))) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId order by cast(r.start_time as datetime) desc) = 1\n",
    "  ;\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d4ba17e-177e-4dc8-beca-a75cb412dbb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = dfd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1334aa68-5b6d-4088-b16a-53dc288128f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbd47d6-434e-4b56-8e64-73c7dd0b3d50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e65eaf-e7c9-4fcd-b148-c44bb4ab6c16",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1cac31b-ed24-4c99-979d-58c9a841e4a7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "716c58e7-8008-42cb-bc39-cbbbedf3cd44",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat.groupby(['Data_selection','Application_month'])['digitalLoanAccountId'].nunique().reset_index().sort_values(by=['Application_month','Data_selection'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f41bbaf-7e51-4a67-aea0-58371eb52c94",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec6909d7-753b-4a39-9b3a-1dc8920f27c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56bdd93b-9abe-4da0-8dd6-558ba6dfe95d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5db1bdaa-9a8b-48a6-ba18-34c26934c8d0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Alpha_Income_Estimated_score'] = pd.to_numeric(df['Alpha_Income_Estimated_score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312d5d57-afb2-452d-ac0c-f559e90b6f60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['Alpha_Income_Estimated_score', 'inc_alpha_cic_credit_avg_credit_limit',\n",
    "       'inc_alpha_cic_max_active_contracts_amt', 'inc_alpha_ln_age',\n",
    "       'inc_alpha_doc_type_rolled', 'inc_alpha_ln_brand', 'inc_alpha_ln_city',\n",
    "       'inc_alpha_ln_cnt_dependents', 'inc_alpha_ln_education_level',\n",
    "       'inc_alpha_ln_employment_type_new', 'inc_alpha_ln_gender',\n",
    "       'inc_alpha_ln_industry_new', 'inc_alpha_ln_loan_prod_type',\n",
    "       'inc_alpha_ln_marital_status_new', 'inc_alpha_ln_nature_of_work_new',\n",
    "       'inc_alpha_ln_osversion_bin', 'inc_alpha_ln_purpose',\n",
    "       'inc_alpha_ln_source_of_funds_new',\n",
    "       'inc_alpha_encoded_company_name_grouped',]\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_income_estimation_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cic_sil_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bc4b54e-5c10-4c15-8d1a-3c8a5240ea49",
   "metadata": {},
   "source": [
    "#### Alpha Sil Stack Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0003c2-4d8c-410c-a2aa-41b7d2cc5fe1",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b84589-d6df-4fbc-a55b-c6158ccde13f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in ('Alpha - StackingModel', 'alpha_stack_model_sil')\n",
    "    and modelVersionId='v1'\n",
    "  )\n",
    "SELECT distinct\n",
    " r.customerId,r.digitalLoanAccountId,prediction Sil_Alpha_Stack_score,start_time,end_time,modelDisplayName,modelVersionId,\n",
    " loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "    case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'SIL Alpha - StackingModel' Model_Name,\n",
    " 'SIL' as product,\n",
    " 'NA' trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Test' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over(partition by r.digitalLoanAccountId order by r.start_time desc)=1\n",
    "  ;\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd = dfd.drop_duplicates(keep='first')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "640b1c13-8a3a-4071-ab5b-5f86c9c359a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1986f967-0479-4539-a195-649fd636333b",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd80b143-ac6d-45fe-9ae8-8466d987fccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in ('Alpha - StackingModel', 'alpha_stack_model_sil')\n",
    "    and modelVersionId='v1'\n",
    "  )\n",
    "SELECT distinct\n",
    " r.customerId,r.digitalLoanAccountId,prediction Sil_Alpha_Stack_score,start_time,end_time,modelDisplayName,modelVersionId,\n",
    " loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "    case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'SIL Alpha - StackingModel' Model_Name,\n",
    " 'SIL' as product,\n",
    " 'NA' trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Train' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),   cast(r.start_time as datetime))) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over(partition by r.digitalLoanAccountId order by  cast(r.start_time as datetime) desc)=1\n",
    "  ;\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd = dfd.drop_duplicates(keep='first')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9793fa-c2a0-4ed5-b0e0-f951261e7587",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82ee55fc-da49-4bdf-87eb-265ff808d983",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a16ef3b-0775-41f3-8b9a-005c7b6e5797",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e9a238-4185-4c2f-a720-e41f8a0908a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'calc_beta_demo_score':'calc_sb_demo_score', 'calc_cic_score':'calc_s_cic_score', 'calc_apps_score':'calc_s_apps_score', 'calc_credo_gen_score':'calc_s_credo_score'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "783b97f5-4a0f-468c-9ef5-c67a5be5b0ac",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e4898b7-704e-48d9-a743-9277fa32bd31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa75df1-6b98-4760-9aa2-7ffae63e201b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc182561-7607-4b4b-8481-dc87cd820f79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Sil_Alpha_Stack_score'] = pd.to_numeric(df['Sil_Alpha_Stack_score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e3cecb-7d01-4ea7-9511-82341e09215d",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1c11aa-2565-4775-8e57-2ef5d9716adf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['Sil_Alpha_Stack_score',   'calc_sb_demo_score',\n",
    "       'calc_s_cic_score', 'calc_s_credo_score', 'calc_s_apps_score']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# Calculate bin-level PSI\n",
    "bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                                   'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "                                    'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                                    'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_sil_stack_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9246f40-f78d-4227-818f-3e0171132898",
   "metadata": {},
   "source": [
    "#### Beta Sil App Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6d48122-203b-4256-8714-ff32bddcdba3",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d33e60-ca0d-44d0-9557-ff68cb8ccb3d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature,\n",
    "    REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in ('Beta - AppsScoreModel', 'apps_score_model_sil')\n",
    "    and modelVersionId='v1'\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'SIL Beta - AppsScoreModel' Model_Name,\n",
    " 'SIL' as product,\n",
    "  'NA' trenchCategory,\n",
    "  'Test' Data_selection,\n",
    "  safe_cast(JSON_VALUE(prediction_clean, \"$.combined_score\") AS float64) as sil_beta_app_score,\n",
    " calcFeature calcFeatures,\n",
    "    coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "    loanmaster.disbursementDateTime,\n",
    "    format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    " FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by start_time desc) = 1\n",
    ";\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc223e9b-ca7a-41e2-94fa-da046d129d76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07d96a5d-cc54-47be-8048-677b8901de65",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6010cd9-61e2-4291-a9c7-af0ec57a4eac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature,\n",
    "    REPLACE(REPLACE(cast(prediction as string), \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in ('Beta - AppsScoreModel', 'apps_score_model_sil')\n",
    "    and modelVersionId='v1'\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'SIL Beta - AppsScoreModel' Model_Name,\n",
    " 'SIL' as product,\n",
    "  'NA' trenchCategory,\n",
    "  'Train' Data_selection,\n",
    "  coalesce(prediction, safe_cast(JSON_VALUE(prediction_clean, \"$.combined_score\") AS float64)) as sil_beta_app_score,\n",
    " calcFeature calcFeatures,\n",
    "    IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "    loanmaster.disbursementDateTime,\n",
    "    format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month,\n",
    " FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcff0506-7787-43ad-a755-f73a09adbe5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d289ed-1262-4347-80d1-3b348f2df0ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478af099-4f3e-4ca7-a5d8-6e550c242386",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62f7bb62-b067-49ba-91eb-1ac735b4f520",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01612bc-2c03-4557-8769-b528d097412b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f45eb85-5147-4a85-80ef-a9c8eed11234",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f3b8b7f-9507-41ff-94d7-8b3769ef1b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sil_beta_app_score'] = pd.to_numeric(df['sil_beta_app_score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b2f5cb-5e5e-49c7-94a5-ab4dcf86ed0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['sil_beta_app_score',\n",
    "       'calc_app_cnt_rated_for_3plus_ever', 'calc_app_cnt_education_ever',\n",
    "       'calc_app_cnt_business_ever', 'calc_app_cnt_music_and_audio_ever',\n",
    "       'calc_app_cnt_travel_and_local_ever', 'calc_app_cnt_finance_7d',\n",
    "       'calc_app_cnt_absence_tag_30d', 'calc_app_cnt_competitors_30d',\n",
    "       'calc_app_cnt_absence_tag_90d',    'calc_app_cnt_finance_90d', 'calc_app_cnt_competitors_90d',\n",
    "       'calc_app_cnt_payday_90d',      'calc_app_median_time_bw_installed_mins_30d',\n",
    "       'calc_app_first_competitors_install_to_apply_days',\n",
    "       'calc_app_first_payday_install_to_apply_days',\n",
    "       'calc_app_vel_finance_30_over_365']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_appscore_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_appscore_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b24c7c4-a85c-4b5d-b8f6-29938b5f15a6",
   "metadata": {},
   "source": [
    "#### Beta SIL Demo Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c26413fb-c7fc-4b62-bcba-91329d28a17c",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18621808-22dc-4774-b7f9-7d1e6f7450f6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" WITH cleaned AS (\n",
    "  SELECT\n",
    "  customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature_cleaned\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in  ('Beta - DemoScoreModel', 'beta_demo_model_sil')\n",
    "  and modelVersionId='v1'\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,  r.digitalLoanAccountId,start_time, prediction sil_beta_demo_score, modelDisplayName,modelVersionId,\n",
    " loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "      case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    "  case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    " 'SIL Beta - DemoScoreModel' Model_Name,\n",
    " 'SIL' as product,\n",
    " 'NA' trenchCategory,\n",
    "  r.calcFeature_cleaned calcFeatures,\n",
    "  'Test' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "\n",
    "qualify row_number() over(partition by r.customerId, r.digitalLoanAccountId order by start_time desc) = 1;\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d36e2d0-fa98-416b-841d-74b046799459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d0ec4fe-35ac-4b8f-8da0-8309b6a38673",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234ec12a-0811-47af-983f-258b18b89975",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" WITH cleaned AS (\n",
    "  SELECT\n",
    "  customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature_cleaned\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in  ('Beta - DemoScoreModel', 'beta_demo_model_sil')\n",
    "  and modelVersionId='v1'\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,  r.digitalLoanAccountId,start_time, prediction sil_beta_demo_score, modelDisplayName,modelVersionId,\n",
    " loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "      case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    "  case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    " 'SIL Beta - DemoScoreModel' Model_Name,\n",
    " 'SIL' as product,\n",
    " 'NA' trenchCategory,\n",
    "  r.calcFeature_cleaned calcFeatures,\n",
    "  'Train' Data_selection,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "\n",
    "qualify row_number() over(partition by r.customerId, r.digitalLoanAccountId order by cast(start_time as datetime) desc) = 1;\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f89ad3b1-38d1-4498-ae0f-99d7043a6f4b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305a8eb5-0007-4dd9-b0e3-8b6184375e50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cefd1f01-ad1f-435e-9d54-e51cd3b47f21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1a65858-e537-4331-a48c-25bdb581bc30",
   "metadata": {},
   "source": [
    "##### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14187409-2961-4c4a-b4aa-057f8daba6b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bce1674c-4a1f-4670-99b0-71da66cb9763",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ba2046-e2a8-40b2-b01f-55774d4dac5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sil_beta_demo_score'] = pd.to_numeric(df['sil_beta_demo_score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03bb0a81-e6e5-4f25-87d0-8f2c2172b482",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6da8a4-d6bf-4564-a72b-5a8504f94ea4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['sil_beta_demo_score',\n",
    "       'calc_beta_de_ln_vas_opted_flag',\n",
    "       'calc_beta_de_ln_doc_type_rolled', 'calc_beta_de_ln_marital_status',\n",
    "       'calc_beta_de_ln_age_bin', 'calc_beta_de_ln_province_bin',\n",
    "       'calc_beta_de_ln_ref2_type', 'calc_beta_de_ln_education_level',\n",
    "       'calc_beta_de_ln_ref1_type', 'calc_beta_de_ln_industry_new_bin',\n",
    "       'calc_beta_de_ln_appln_day_of_week',\n",
    "       'calc_beta_de_onb_name_email_match_score',\n",
    "       'calc_beta_de_ln_employment_type_new_bin', 'calc_beta_de_ln_telconame',\n",
    "       'calc_beta_de_time_bw_onb_loan_appln_mins',\n",
    "       'calc_beta_de_ln_source_of_funds_new_bin', 'calc_beta_de_ln_brand_bin',\n",
    "       'calc_beta_de_ln_email_primary_domain']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_demo_score_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_demo_score_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9ed7691-73c3-4ec2-9938-307a71b04a79",
   "metadata": {},
   "source": [
    "#### Beta SIL STACK Score Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699bf738-3377-4a6a-8d03-7bb74fed7e43",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c0406db-8da5-4d06-84f8-5e1148a10113",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in ('Beta - StackScoreModel', 'beta_stack_model_sil')\n",
    "  and modelVersionId='v1'\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    " loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    " 'SIL_Beta - StackScoreModel' Model_Name,\n",
    " 'SIL' as product,\n",
    " 'NA' trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  prediction sil_beta_stack_score,\n",
    "  'Test' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId, r.digitalLoanAccountId order by start_time desc) = 1\n",
    ";\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd888da-ae53-4c2f-bff2-7f5229e4f525",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27d9848-ab09-4ed4-b27b-eac65c20b575",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cab891f-e7fd-47cb-b758-9d6c5478bc31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM  prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in ('Beta - StackScoreModel', 'beta_stack_model_sil')\n",
    "  and modelVersionId='v1'\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    " loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    " 'SIL_Beta - StackScoreModel' Model_Name,\n",
    " 'SIL' as product,\n",
    " 'NA' trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  prediction sil_beta_stack_score,\n",
    "  'Train' Data_selection,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId, r.digitalLoanAccountId order by cast(start_time as datetime) desc) = 1\n",
    ";\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b0ee832-f7fa-4eb2-ab2a-9bcaf137d8d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3b3f60-105e-4b07-b03a-1eeb441628b5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152b89b-9672-43b9-a9ae-a01a36623781",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e65cc43-4167-41a0-953a-2e7ad035acba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'calc_apps_score':'calc_s_apps_score', 'calc_credo_gen_score':'calc_s_credo_score', 'calc_beta_demo_score':'calc_sb_demo_score'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6047c6f2-fe37-44f0-8ba2-88700ba572a5",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e116fe77-fe62-4cd2-a602-f0c871adc6ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba336331-8e9b-43df-9cf0-7954c79960cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da8a8ba-07d6-4e35-bb2f-212bd0ab4385",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sil_beta_stack_score'] = pd.to_numeric(df['sil_beta_stack_score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b4ca2b-7788-4c65-84fc-2834e1c3cfa3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['sil_beta_stack_score',\n",
    "        'calc_s_apps_score', 'calc_s_credo_score', 'calc_sb_demo_score']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_stack_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_stack_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcb39ecd-169e-46d5-99d1-e39f9b8fd01a",
   "metadata": {},
   "source": [
    "#### Beta - IncomeEstimationModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85c49073-8287-4dac-a50d-4991bca807f4",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65253129-6561-41b1-963f-d0c355575c79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in  ('Beta - IncomeEstimationModel', 'beta_income_model')\n",
    "  and modelVersionId='v1'\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction sil_beta_income_estimation_score,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_loan_type\") AS inc_beta_ln_loan_type,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_education_level\") AS inc_beta_ln_education_level,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_employment_type_new\") AS inc_beta_ln_employment_type_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_industry_new\") AS inc_beta_ln_industry_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_age\") AS inc_beta_ln_age,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_brand\") AS inc_beta_ln_brand,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_city\") AS inc_beta_ln_city,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_purpose\") AS inc_beta_ln_purpose,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_osversion_bin\") AS inc_beta_ln_osversion_bin,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_postal_code\") AS inc_beta_ln_postal_code,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_gender\") AS inc_beta_ln_gender,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_doc_type_rolled\") AS inc_beta_ln_doc_type_rolled,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_cnt_dependents\") AS inc_beta_ln_cnt_dependents,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_source_of_funds_new\") AS inc_beta_ln_source_of_funds_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_marital_status_new\") AS inc_beta_ln_marital_status_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_encoded_company_name_grouped\") AS inc_beta_encoded_company_name_grouped,\n",
    "   loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    " 'SIL Beta - IncomeEstimationModel' Model_Name,\n",
    " 'SIL' as product,\n",
    " 'NA' trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Test' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId, r.digitalLoanAccountId order by start_time desc) = 1\n",
    "  ;\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81601798-647e-4f41-905e-626e402d5d37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1 = dfd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1cae74f-085a-4172-91d0-51c8fbc5f200",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba27190-5876-4fda-865c-6a6e4747cec7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in  ('Beta - IncomeEstimationModel', 'beta_income_model')\n",
    "  )\n",
    "SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction sil_beta_income_estimation_score,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_loan_type\") AS inc_beta_ln_loan_type,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_education_level\") AS inc_beta_ln_education_level,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_employment_type_new\") AS inc_beta_ln_employment_type_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_industry_new\") AS inc_beta_ln_industry_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_age\") AS inc_beta_ln_age,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_brand\") AS inc_beta_ln_brand,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_city\") AS inc_beta_ln_city,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_purpose\") AS inc_beta_ln_purpose,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_osversion_bin\") AS inc_beta_ln_osversion_bin,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_postal_code\") AS inc_beta_ln_postal_code,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_gender\") AS inc_beta_ln_gender,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_doc_type_rolled\") AS inc_beta_ln_doc_type_rolled,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_cnt_dependents\") AS inc_beta_ln_cnt_dependents,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_source_of_funds_new\") AS inc_beta_ln_source_of_funds_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_ln_marital_status_new\") AS inc_beta_ln_marital_status_new,\n",
    "  JSON_VALUE(calcFeature, \"$.inc_beta_encoded_company_name_grouped\") AS inc_beta_encoded_company_name_grouped,\n",
    "   loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    " 'SIL Beta - IncomeEstimationModel' Model_Name,\n",
    " 'SIL' as product,\n",
    " 'NA' trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Train' Data_selection,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId, r.digitalLoanAccountId order by cast(start_time as datetime) desc) = 1\n",
    "  ;\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b87d5-aa0b-4d41-9cc9-1a485129929b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2 = dfd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d995d77-5094-4276-810d-f11da89c18a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3bfabde6-0d3e-4657-98bb-3d2979fb5157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9cf932b-710f-4e32-899c-dbed7d3e3e87",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423ecc81-dd61-45fb-b071-a0fc52ae7e02",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c464f8b2-d435-470c-8ff3-d6f23031abc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9672b6d-a60f-40da-883e-6df46249174d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['sil_beta_income_estimation_score'] = pd.to_numeric(df['sil_beta_income_estimation_score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57a014a1-f28e-418d-91af-0621b65dabf8",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efcd05d6-5edc-41d3-83c7-92eed4df8529",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['sil_beta_income_estimation_score',  'inc_beta_ln_loan_type',\n",
    "       'inc_beta_ln_education_level', 'inc_beta_ln_employment_type_new',\n",
    "       'inc_beta_ln_industry_new', 'inc_beta_ln_age', 'inc_beta_ln_brand',\n",
    "       'inc_beta_ln_city', 'inc_beta_ln_purpose', 'inc_beta_ln_osversion_bin',\n",
    "       'inc_beta_ln_postal_code', 'inc_beta_ln_gender',\n",
    "       'inc_beta_ln_doc_type_rolled', 'inc_beta_ln_cnt_dependents',\n",
    "       'inc_beta_ln_source_of_funds_new', 'inc_beta_ln_marital_status_new',\n",
    "       'inc_beta_encoded_company_name_grouped',]\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "076347c6-581f-4c5c-bf1a-7eb9dd810ac3",
   "metadata": {},
   "source": [
    "# Cash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a31edc9-9d4f-41cd-b3be-f6607a1a4e9c",
   "metadata": {},
   "source": [
    "#### Alpha-Cash-CIC-Model (All Trench)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6abd714f-cd2d-48ba-91bf-4dc8d6395063",
   "metadata": {},
   "source": [
    "##### Trench 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "452fabaa-5e3a-4b66-b8f6-9115984afea2",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200bb849-1afd-462e-a97f-1af9a6b6165b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Alpha-Cash-CIC-Model','Alpha Cash CIC Model','cic_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-CIC-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    "\"\"\" \n",
    "\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98a2eea2-d1cf-48de-8232-a182d8910ac8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11675a48-7ab2-47a0-a157-10349486f601",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30d1c4e-217e-480e-9152-fb0e59b3ad06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Alpha-Cash-CIC-Model','Alpha Cash CIC Model','cic_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-CIC-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    ";\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "725e0289-afdc-4092-b55f-0ccac292eb33",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e31bd555-044a-4b5c-8e44-eb40ff8a8f4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fe9231-7cc4-4d24-9f8f-ca6070682ecf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50160fbe-09af-46e6-a679-1ed1593d2cb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns = {\n",
    "    'calc_max_age_all_contracts_snapshot':'calc_cic_max_age_all_contracts_snapshot',\n",
    "    'calc_ratio_overdue_contracts_to_granted_contracts':'calc_cic_ratio_overdue_contracts_to_granted_contracts',\n",
    "    'calc_ScoreRange':'calc_cic_ScoreRange',\n",
    "    'calc_ln_loan_level_user_type':'calc_cic_ln_loan_level_user_type', \n",
    "    'calc_has_ever_been_overdue':'calc_cic_has_ever_been_overdue',\n",
    "    'calc_latest_granted_contract_overdue_flag':'calc_cic_latest_granted_contract_overdue_flag',\n",
    "    'calc_ratio_closed_over_new_granted_cnt_24M':'calc_cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "    'calc_ratio_risky_contracts_to_granted_contracts':'calc_cic_ratio_risky_contracts_to_granted_contracts',\n",
    "    'calc_Short_and_Term_Loans_granted_contracts_cnt_24M':'calc_cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "    'calc_flg_zero_non_granted_ever':'calc_cic_flg_zero_non_granted_ever', \n",
    "    'calc_CreditAvgCreditLimit':'calc_cic_CreditAvgCreditLimit',\n",
    "    'calc_flg_zero_granted_ever':'calc_cic_flg_zero_granted_ever', \n",
    "    'calc_ca_cic_score':'calc_cic_ca_cic_score',\n",
    "    'calc_Personal_Loans_granted_contracts_amt_24M':'calc_cic_Personal_Loans_granted_contracts_amt_24M'\n",
    "    \n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56ecc7f6-fcbb-4701-af49-252b9991351d",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd8af17a-efb2-4b4a-a03d-a583d75c648b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bf46803-b67f-4f53-b31f-23156d574593",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5766b7b4-8ef4-4d1b-bad9-b9828f00e49c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['aCicScore'] = pd.to_numeric(df['aCicScore'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f373e5-d1a8-4e24-96aa-054b8bb9ef0a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['aCicScore',\n",
    "     'calc_cic_max_age_all_contracts_snapshot',\n",
    "       'calc_cic_ratio_overdue_contracts_to_granted_contracts',\n",
    "       'calc_cic_ScoreRange', 'calc_cic_ln_loan_level_user_type',\n",
    "       'calc_cic_has_ever_been_overdue',\n",
    "       'calc_cic_latest_granted_contract_overdue_flag',\n",
    "       'calc_cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "       'calc_cic_ratio_risky_contracts_to_granted_contracts',\n",
    "       'calc_cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "       'calc_cic_flg_zero_non_granted_ever',\n",
    "       'calc_cic_Personal_Loans_granted_contracts_amt_24M',\n",
    "       'calc_cic_CreditAvgCreditLimit', 'calc_cic_flg_zero_granted_ever'\n",
    "]\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cash_cic_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91abfd9-7efc-455f-8e0c-42dd1f9f979d",
   "metadata": {},
   "source": [
    "##### Trench 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71aebd75-a043-4e84-ab29-f1df605235f5",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d2ccde4-b2ab-48f8-a841-dc469b1696be",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Alpha-Cash-CIC-Model','Alpha Cash CIC Model','cic_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-CIC-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    "\"\"\" \n",
    "\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ff7cb39-d132-449e-8c31-1c202e275284",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acbd73ec-5e61-45c7-a38c-8d5d1e616040",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ab31e50-c136-4085-be8e-1d4776f407b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Alpha-Cash-CIC-Model','Alpha Cash CIC Model','cic_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-CIC-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    ";\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07fa9d9a-db4f-42e4-9b93-4fc583dc4144",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f66dc-01c1-4b05-958f-3d75411c82c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b9eb811-da92-490e-b980-d1556beb6107",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "888ba6e1-a37e-4988-8edc-e89cd143364c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns = {\n",
    "    'calc_max_age_all_contracts_snapshot':'calc_cic_max_age_all_contracts_snapshot',\n",
    "    'calc_ratio_overdue_contracts_to_granted_contracts':'calc_cic_ratio_overdue_contracts_to_granted_contracts',\n",
    "    'calc_ScoreRange':'calc_cic_ScoreRange',\n",
    "    'calc_ln_loan_level_user_type':'calc_cic_ln_loan_level_user_type', \n",
    "    'calc_has_ever_been_overdue':'calc_cic_has_ever_been_overdue',\n",
    "    'calc_latest_granted_contract_overdue_flag':'calc_cic_latest_granted_contract_overdue_flag',\n",
    "    'calc_ratio_closed_over_new_granted_cnt_24M':'calc_cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "    'calc_ratio_risky_contracts_to_granted_contracts':'calc_cic_ratio_risky_contracts_to_granted_contracts',\n",
    "    'calc_Short_and_Term_Loans_granted_contracts_cnt_24M':'calc_cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "    'calc_flg_zero_non_granted_ever':'calc_cic_flg_zero_non_granted_ever', \n",
    "    'calc_CreditAvgCreditLimit':'calc_cic_CreditAvgCreditLimit',\n",
    "    'calc_flg_zero_granted_ever':'calc_cic_flg_zero_granted_ever', \n",
    "    'calc_ca_cic_score':'calc_cic_ca_cic_score',\n",
    "    'calc_Personal_Loans_granted_contracts_amt_24M':'calc_cic_Personal_Loans_granted_contracts_amt_24M'\n",
    "    \n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ed8f56a-a791-4bb6-95fc-d19743547ba4",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac74bca-0f0f-41c8-af1a-5c0b10eb7189",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33398fb9-7ed5-4bc8-8589-9b9ec7262e9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df['aCicScore'] = pd.to_numeric(df['aCicScore'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2068c78e-575b-4a0c-8a6d-863b9b8a2066",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['aCicScore',\n",
    "     'calc_cic_max_age_all_contracts_snapshot',\n",
    "       'calc_cic_ratio_overdue_contracts_to_granted_contracts',\n",
    "       'calc_cic_ScoreRange', 'calc_cic_ln_loan_level_user_type',\n",
    "       'calc_cic_has_ever_been_overdue',\n",
    "       'calc_cic_latest_granted_contract_overdue_flag',\n",
    "       'calc_cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "       'calc_cic_ratio_risky_contracts_to_granted_contracts',\n",
    "       'calc_cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "       'calc_cic_flg_zero_non_granted_ever',\n",
    "       'calc_cic_Personal_Loans_granted_contracts_amt_24M',\n",
    "       'calc_cic_CreditAvgCreditLimit', 'calc_cic_flg_zero_granted_ever'\n",
    "]\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cash_cic_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4b86e3-0007-403a-8be0-285a269f1042",
   "metadata": {},
   "source": [
    "##### Trench 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c0fc86b-ad33-41d9-8a1e-614e52545a29",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8738599e-19a5-41dc-a602-27fa40ccd4ba",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Alpha-Cash-CIC-Model','Alpha Cash CIC Model','cic_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-CIC-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    "\"\"\" \n",
    "\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5238d368-8d1d-4b1d-99dc-66868c7d60e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6aaf531-9fdc-45bd-a803-6fb07285131d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Alpha-Cash-CIC-Model','Alpha Cash CIC Model','cic_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-CIC-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    ";\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0974fb77-053d-4c7c-a7c9-552bce97de79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0c02e0f-b394-4c60-9ab2-b613c11948c3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc43928-5748-483e-aa93-e122fc1b98a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dcf0a6-c96b-4897-bd2f-3c3a5df28a64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns = {\n",
    "    'calc_max_age_all_contracts_snapshot':'calc_cic_max_age_all_contracts_snapshot',\n",
    "    'calc_ratio_overdue_contracts_to_granted_contracts':'calc_cic_ratio_overdue_contracts_to_granted_contracts',\n",
    "    'calc_ScoreRange':'calc_cic_ScoreRange',\n",
    "    'calc_ln_loan_level_user_type':'calc_cic_ln_loan_level_user_type', \n",
    "    'calc_has_ever_been_overdue':'calc_cic_has_ever_been_overdue',\n",
    "    'calc_latest_granted_contract_overdue_flag':'calc_cic_latest_granted_contract_overdue_flag',\n",
    "    'calc_ratio_closed_over_new_granted_cnt_24M':'calc_cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "    'calc_ratio_risky_contracts_to_granted_contracts':'calc_cic_ratio_risky_contracts_to_granted_contracts',\n",
    "    'calc_Short_and_Term_Loans_granted_contracts_cnt_24M':'calc_cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "    'calc_flg_zero_non_granted_ever':'calc_cic_flg_zero_non_granted_ever', \n",
    "    'calc_CreditAvgCreditLimit':'calc_cic_CreditAvgCreditLimit',\n",
    "    'calc_flg_zero_granted_ever':'calc_cic_flg_zero_granted_ever', \n",
    "    'calc_ca_cic_score':'calc_cic_ca_cic_score',\n",
    "    'calc_Personal_Loans_granted_contracts_amt_24M':'calc_cic_Personal_Loans_granted_contracts_amt_24M'\n",
    "    \n",
    "}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf202bb-de32-431a-9095-844de044b4f2",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1081669-9cab-47ad-a79e-2229842d16f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22214e3b-5893-42cd-96fc-25eaba7d83db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df['aCicScore'] = pd.to_numeric(df['aCicScore'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dea1908d-d3ae-491d-a526-925e1665c739",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc7d30d-63ab-4af1-b95d-dfc8e9eb37d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['aCicScore',\n",
    "     'calc_cic_max_age_all_contracts_snapshot',\n",
    "       'calc_cic_ratio_overdue_contracts_to_granted_contracts',\n",
    "       'calc_cic_ScoreRange', 'calc_cic_ln_loan_level_user_type',\n",
    "       'calc_cic_has_ever_been_overdue',\n",
    "       'calc_cic_latest_granted_contract_overdue_flag',\n",
    "       'calc_cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "       'calc_cic_ratio_risky_contracts_to_granted_contracts',\n",
    "       'calc_cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "       'calc_cic_flg_zero_non_granted_ever',\n",
    "       'calc_cic_Personal_Loans_granted_contracts_amt_24M',\n",
    "       'calc_cic_CreditAvgCreditLimit', 'calc_cic_flg_zero_granted_ever'\n",
    "]\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cash_cic_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b602784c-6d83-4b06-8acb-fd9a19fcd313",
   "metadata": {},
   "source": [
    "#### Alpha-Cash-Stack-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9eac69f-a5dd-48b7-a60f-8bfabcf2ad17",
   "metadata": {},
   "source": [
    "###### Trench 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a681f482-58a0-4a2f-ab9d-2988fcb8b93f",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a04d1-b325-4897-acd8-026460ce3819",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Alpha-Cash-Stack-Model', 'alpha_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-Stack-Model' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction aStackScore,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d465e0-8408-4bd5-9dcf-9a17707c9749",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9767d1a-72a3-4724-b693-9d4fc3300079",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afcbd64-26c5-430e-a404-b0f68592e77b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Alpha-Cash-Stack-Model', 'alpha_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aStackScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-Stack-Model' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    "\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "591c7bba-3de5-4b95-ac7a-917f57bb80f4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a42f951-b9a3-4e18-931e-ebc0330087d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cee85be8-a40d-4cef-8e3b-b1c52e20cc5a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bcb8ad4-95a0-4d29-b4c7-b99e2b9e05b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49adc7f1-a75c-4605-95b7-bcc19f871488",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67014a7c-9d47-4547-8a46-f14707cf2fb4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'calc_demo_score':'calc_c_demo_score',\n",
    "                     'calc_credo_score':'calc_c_credo_score' ,\n",
    "                    'calc_cic_score':'calc_ca_cic_score',\n",
    "                    'calc_apps_score':'calc_apps_score'\n",
    "                   }, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db30bce-b359-4d19-8c74-255dddbee35a",
   "metadata": {},
   "source": [
    "##### Concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ce484-9437-4211-9fcf-490fbc1b9b0c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "431c40fe-5f24-4d08-85f9-af8548652782",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dd.query(\"\"\"select Data_selection, count(distinct digitalLoanAccountId) cnt from df_concat where calc_apps_score is not null group by 1\"\"\").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd47a6d-d378-49ba-9cd5-1e7d90ae3bf9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df['aStackScore'] = pd.to_numeric(df['aStackScore'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc574e5-38e7-4033-b61c-47c84bfa6960",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501530db-1f0b-4b14-bd35-b467712d51fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['aStackScore',  'calc_apps_score', 'calc_c_demo_score',   'calc_c_credo_score', 'calc_ca_cic_score']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cash_stack_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5086ffd9-d5dc-4816-857b-9dfb0bf7a357",
   "metadata": {},
   "source": [
    "##### Trench 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7ea13c4-2749-42c1-9221-1dea909631ee",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57f4b08-fa79-4307-8e1e-045dedf99c53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Alpha-Cash-Stack-Model', 'alpha_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-Stack-Model' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction aStackScore,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9275800-8edb-47c8-a24e-c4e7ce1b04d2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f84f6225-6375-4815-bad4-7951d3c6b52c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f86628fa-553e-4164-a392-f860bb912689",
   "metadata": {},
   "source": [
    "##### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5cd8a9-94db-40d1-8106-c8e5969e2419",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Alpha-Cash-Stack-Model', 'alpha_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aStackScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-Stack-Model' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    "\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f6a687-bbe0-469f-b2d6-313ec9e458d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "519bed4e-ba89-47bb-98c2-2a65cd96fc6f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44c4b094-04a6-41f8-90f5-edc23c354c55",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba58413f-be20-4483-9da3-a8c41268d6d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "583bcbb5-8e11-4364-9d9a-ef2da0996ef4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'calc_demo_score':'calc_c_demo_score',\n",
    "                     'calc_credo_score':'calc_c_credo_score' ,\n",
    "                    'calc_cic_score':'calc_ca_cic_score',\n",
    "                    'calc_apps_score':'calc_apps_score'\n",
    "                   }, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b86b7de4-d69f-4e14-9ce1-a2bc3b35f165",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1b20972-0dc2-4ce2-8e94-4fdf1374dfef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e298b787-4449-44e1-bb77-fadec30c4fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df['aStackScore'] = pd.to_numeric(df['aStackScore'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42cf876c-53ed-4522-9dcb-2e1adf7d3a1b",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3821545e-8979-4e93-b89d-e8dc2d2926e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['aStackScore',  'calc_apps_score', 'calc_c_demo_score',   'calc_c_credo_score', 'calc_ca_cic_score']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cash_stack_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576e222e-6e0e-41a7-bd3a-d37de22c76e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cash_stack_model_psi_v4\"\"\"\n",
    "# dfd = client.query(sq).to_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca191cd1-ee1a-464e-aae0-367bf655f1fb",
   "metadata": {},
   "source": [
    "##### Trench 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ca85e0-6bcf-4af4-9069-6cacaf42c632",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aea57b4-d1d0-40c8-8775-9feab0a2cbb0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Alpha-Cash-Stack-Model', 'alpha_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-Stack-Model' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction aStackScore,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94f0658d-b358-45ca-8f00-d739db788ce7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "314c6e3e-bd77-40b5-904d-0d4fffdd2e73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "debcd7f1-d8ba-4671-8054-7df0f73dd074",
   "metadata": {},
   "source": [
    "##### Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c451e7e6-f844-4c07-882c-c0e2c9a1a969",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Alpha-Cash-Stack-Model', 'alpha_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aStackScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Alpha-Cash-Stack-Model' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    "\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b2e7136-5bab-45c9-8e39-1d47790036cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26784d3-fdca-4752-91b9-e19120a2c60b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c1ba8e-b6bf-4cf6-90f2-4ba31d05c6a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a30c7844-af31-4743-b197-796aa8947e15",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d89fbe2-c6b2-4be5-ae90-183df8525159",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'calc_demo_score':'calc_c_demo_score',\n",
    "                     'calc_credo_score':'calc_c_credo_score' ,\n",
    "                    'calc_cic_score':'calc_ca_cic_score',\n",
    "                    'calc_apps_score':'calc_apps_score'\n",
    "                   }, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8754804-fe9e-4ba7-a96f-95e4ece2bfbf",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f4ebb4-0c3f-423d-b849-d7d6db3cfc21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00c41e1c-03da-49d5-b382-fc737f48a7dd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df['aStackScore'] = pd.to_numeric(df['aStackScore'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0afd883-1715-423b-b01a-a8d020e3fe19",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deff358f-18b0-4ad4-a9f5-26afa40545ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['aStackScore',  'calc_apps_score', 'calc_c_demo_score',   'calc_c_credo_score', 'calc_ca_cic_score']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cash_stack_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c030f30-d779-4987-8b9d-0fb9ba5e2739",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cash_stack_model_psi_v4\"\"\"\n",
    "# dfd = client.query(sq).to_dataframe()\n",
    "# dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e86913e-3155-4187-9bce-0f15b65f0388",
   "metadata": {},
   "source": [
    "#### Beta-Cash-Demo-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b321273-cd27-4986-a212-3f02a3f83a3a",
   "metadata": {},
   "source": [
    "##### Trench 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7e93cec-e069-46bd-96ef-f579e13d7ca4",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "999be832-ddeb-4a55-9a36-9a7bcf7fcf70",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Beta-Cash-Demo-Model', 'beta_demo_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Demo-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction Beta_Cash_Demo_Score,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    ";\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a22c8c5-f757-4354-8143-343647298e79",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae1e63d-f877-431b-8ed0-492c2c9acee6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54e87c5-c06b-47dc-b675-439a7a494a85",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1641b3-05c3-4efe-b145-e53443b8d197",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Beta-Cash-Demo-Model', 'beta_demo_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Demo-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction Beta_Cash_Demo_Score,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95f15fd-0a5f-4c46-bb6a-da77ee863875",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fea77ee0-6532-46af-9379-7b57facd0885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3f3e162-7fb9-49d0-a5da-2127f303f264",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd71da4b-a886-4df9-b0f8-152d3aa57eae",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42bdbd5c-62c3-4d2c-b418-f84c37fe506e",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f7de3fe-5ab1-47d7-abad-f3eaa16a3dd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45b8938-4f90-494a-9efe-68bde3bad7bf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop(columns=['calc_Beta_Cash_Demo_Score', 'calc_ln_fspd30_flag', 'calc_ln_os_type', 'calc_ln_mature_fspd30_flag', 'calc_ln_appln_submit_datetime']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ad8b44-9674-45ee-85a7-11fcf406ade4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df['Beta_Cash_Demo_Score'] = pd.to_numeric(df['Beta_Cash_Demo_Score'], errors='coerce')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e03c2f58-55d6-49f5-bbf9-85d556453383",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e29e614-4b02-413a-b374-6a487e53bcc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['Beta_Cash_Demo_Score', 'ln_vas_opted_flag', 'ln_self_dec_income', 'ln_age',\n",
    "       'ln_source_funds_new_bin', 'ln_loan_level_user_type',\n",
    "       'ln_industry_new_cat_bin', 'ln_marital_status', 'ln_doc_type_rolled',\n",
    "       'ln_education_level', 'ln_ref2_type', 'ln_email_primary_domain',\n",
    "       'ln_province_bin']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_demo_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c212d1-da6a-4b02-828a-726787960a75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_demo_model_psi_v4\"\"\"\n",
    "# dfd = client.query(sq).to_dataframe()\n",
    "# dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc817a2-80a6-4a07-b493-9c1f27a0277c",
   "metadata": {},
   "source": [
    "##### Trench 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "381f7cab-6fd6-44c6-8b74-dc53e6f1f7cc",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3b6739-cb0e-46cd-aece-3ef0cc4b9b5d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Beta-Cash-Demo-Model', 'beta_demo_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Demo-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction Beta_Cash_Demo_Score,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    ";\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "450dd60e-fc77-4573-bed7-c6ffeeadb277",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e985e9-3d09-48fe-8dc2-be6caf4f2221",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c127b2d-5650-4e7d-97e3-7cc3abc8f9d5",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b83817d-650c-49fd-ba50-a54716d60483",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Beta-Cash-Demo-Model', 'beta_demo_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Demo-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction Beta_Cash_Demo_Score,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e80fe7e-1607-4af2-9278-b302024b817a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e82a66c-4bef-4115-8c2f-2e487cd3415e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d119db7-c042-439e-95bb-10300b86db57",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf78d1bc-28c0-4a9f-a312-954a85bfb02e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d88b0bf-0b21-4c86-998a-f38fe834e05e",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed3aa90-6f05-4db4-a4b5-d75258da023a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaa1512-a050-4be1-b002-9122464510fb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop(columns=['calc_Beta_Cash_Demo_Score', 'calc_ln_fspd30_flag', 'calc_ln_os_type', 'calc_ln_mature_fspd30_flag', 'calc_ln_appln_submit_datetime']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a51d40-57ad-4e2d-9aba-8b24a2310688",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df['Beta_Cash_Demo_Score'] = pd.to_numeric(df['Beta_Cash_Demo_Score'], errors='coerce')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a4cf788-57e4-4bb6-890f-63e9d35a28c4",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5199ee9-5d0d-48ed-abc4-a2a3281649df",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['Beta_Cash_Demo_Score', 'ln_vas_opted_flag', 'ln_self_dec_income', 'ln_age',\n",
    "       'ln_source_funds_new_bin', 'ln_loan_level_user_type',\n",
    "       'ln_industry_new_cat_bin', 'ln_marital_status', 'ln_doc_type_rolled',\n",
    "       'ln_education_level', 'ln_ref2_type', 'ln_email_primary_domain',\n",
    "        'ln_province_bin']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_demo_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1c65dc-d4be-45a3-8c9f-da726148ec5e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_demo_model_psi_v4\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e69625a0-aac6-4457-82eb-4edcf417e31b",
   "metadata": {},
   "source": [
    "##### Trench 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909c83b-8d38-4826-a7f5-77e5ee201b03",
   "metadata": {},
   "source": [
    "#### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff2cbd6-ec26-422f-97f9-535774fb268b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Beta-Cash-Demo-Model', 'beta_demo_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Demo-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction Beta_Cash_Demo_Score,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    ";\n",
    "\"\"\"\n",
    "dfd= client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcda97f6-0a4b-41d5-8490-33757bc8bb88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5885fab-1097-42df-aa02-4efe00288a73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a1092a5-9454-4d70-ab4b-1d5987d878e9",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c35d8fe-7662-44bc-94f4-7df9f5ff6b3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Beta-Cash-Demo-Model', 'beta_demo_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Demo-Model_All_Trench' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction Beta_Cash_Demo_Score,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42627194-3c9d-4361-a593-b762eba0e91b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec012c9d-7a8d-46d5-bac4-380a38f4d4b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ff533b1-891d-4779-9a4e-e0b0585bf33a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72f8050-6925-456b-bb72-caab5f0dcdfd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99349ac0-c691-4919-86c4-340d17a44b0f",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f5c3609-7020-4168-9cb8-6ae333bdf097",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb0fc10-13f1-477b-9070-5ad362f7535b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop(columns=['calc_Beta_Cash_Demo_Score']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ba1b7a7-67e2-4eb3-848a-891606afc65e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df['Beta_Cash_Demo_Score'] = pd.to_numeric(df['Beta_Cash_Demo_Score'], errors='coerce')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2202128-8c83-48ec-bc8f-db258092d9e6",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f9d83d3-579e-4d1f-93c2-17ae460ede5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['Beta_Cash_Demo_Score', 'ln_vas_opted_flag', 'ln_self_dec_income', 'ln_age',\n",
    "       'ln_source_funds_new_bin', 'ln_loan_level_user_type',\n",
    "       'ln_industry_new_cat_bin', 'ln_marital_status', 'ln_doc_type_rolled',\n",
    "       'ln_education_level', 'ln_ref2_type', 'ln_email_primary_domain',\n",
    "        'ln_province_bin']\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_demo_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ec3ede0-b0fd-4d68-bb6b-2200502e1bb7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_demo_model_psi_v4\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db25b96d-10e5-41da-bd1e-26f5ff205951",
   "metadata": {},
   "source": [
    "#### Beta-Cash-AppScore-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e6b640-46b8-4c05-9fc1-1c3ca13abd26",
   "metadata": {},
   "source": [
    "##### Trench 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f38d7db0-3696-4bce-bfba-57aaba07c4f9",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1dc55a0-6660-4e5f-b261-30b99e71ae32",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in  ('Beta-Cash-AppScore-Model', 'apps_score_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-AppScore-Model_Trench1' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  SAFE_CAST(JSON_VALUE(r.prediction_clean, \"$.combined_score\") AS Float64) AS beta_cash_app_score,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    ";\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff773829-1d1a-44a0-ba9e-69d6b9ff8b59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "432c64c9-1646-46b2-880f-aeca3ae300b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features_fixed(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61b21713-a88a-4c6b-8ee2-44a60662b062",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "087a4bef-a19b-41c2-bd92-cab43ef298e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in  ('Beta-Cash-AppScore-Model', 'apps_score_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-AppScore-Model_Trench1' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction beta_cash_app_score,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0010df80-a613-442c-9662-efdf664f9727",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a17a475-606f-4b90-b9a4-d65a9c848571",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ae38d59-0dbb-4ce2-81cf-7a42838ef623",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da50d12f-8ed1-4dfe-9278-019bea7b0622",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdcb9160-3d92-43a8-bf8c-17fb33d5b3df",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "139845b7-a9ef-47a0-9a30-34197dba58fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4533562a-33c9-459f-9538-8c1efdeebcc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop(columns=['prediction', 'calc_app_median_time_bw_installed_mins_ever', 'calc_app_avg_time_bw_installed_mins_ever', 'calc_beta_cash_app_score']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e127506-dbfc-4e7d-bbf3-3ca6929f3dd2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffab6383-a5a3-4065-982a-2f6096f2ef2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['beta_cash_app_score'] = pd.to_numeric(df['beta_cash_app_score'], errors='coerce')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc29fb-3889-4b0b-9e8b-6c239e633f49",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dca70c27-e4de-4ebb-bcd3-f7ce03526268",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['beta_cash_app_score',\n",
    "       'app_cnt_health_and_fitness_ever', 'app_cnt_shopping_ever',\n",
    "       'app_cnt_crypto_ever', 'app_cnt_driver_ever', 'app_cnt_payday_180d',\n",
    "       'app_cnt_gambling_180d', 'app_avg_time_bw_installed_mins_3d',\n",
    "       'app_median_time_bw_installed_mins_3d'\n",
    "]\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_appscore_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9dcc39-7bb9-4fae-aed2-824f52e7c435",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_appscore_model_psi_v4\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df9375-1d4a-411c-8a22-a458ace720b6",
   "metadata": {},
   "source": [
    "##### Trench 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c89a15c-11bf-42b8-93cf-505ab911bab8",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0639b94a-10c8-4321-8a33-e663146fb109",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in  ('Beta-Cash-AppScore-Model', 'apps_score_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-AppScore-Model_Trench2' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  SAFE_CAST(JSON_VALUE(r.prediction_clean, \"$.combined_score\") AS Float64) AS beta_cash_app_score,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    ";\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fd562a-5c5b-4efa-9495-1e52f058a579",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0133c1-d457-437c-9bdc-df3d4cfd2c3b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60842518-83fc-4553-bf47-d5d41bd4931c",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb5d774b-d0a8-4fad-b99d-7c66d8a0d899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in  ('Beta-Cash-AppScore-Model', 'apps_score_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-AppScore-Model_Trench2' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction beta_cash_app_score,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2c4fcaf-d8d9-45fb-a10e-5ad232a41157",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f372a27-1c03-4457-b714-7636ddf1a901",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd18f82-95ac-47b5-ba8a-09b6ef95feab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e21e0b-812e-4293-b273-716ec82d451d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb1daba-975b-46b3-b133-3361ce302a58",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06025af0-2d11-484a-8b79-0bea57b4df10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9005a6-d515-45ef-aacc-263b433035ef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop(columns=['prediction', 'calc_app_median_time_bw_installed_mins_ever', 'calc_app_avg_time_bw_installed_mins_ever', 'calc_beta_cash_app_score']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c75f923-16e4-4706-9568-230381ba716e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49f4905f-c45c-4976-b2dd-7928b8a7f563",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['beta_cash_app_score'] = pd.to_numeric(df['beta_cash_app_score'], errors='coerce')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e723650-0320-49f3-9cdd-779d0b3d5e98",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3356766-9c5e-4ee0-8a33-7dccd283cd61",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['beta_cash_app_score',\n",
    "       'app_cnt_health_and_fitness_ever', 'app_cnt_shopping_ever',\n",
    "       'app_cnt_crypto_ever', 'app_cnt_driver_ever', 'app_cnt_payday_180d',\n",
    "       'app_cnt_gambling_180d', 'app_avg_time_bw_installed_mins_3d',\n",
    "       'app_median_time_bw_installed_mins_3d'\n",
    "]\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_appscore_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8f31e9d-81a2-4a48-b00b-8237f780dcef",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_appscore_model_psi_v4\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4884f4c1-8ab3-44c6-8c23-af05d2622849",
   "metadata": {},
   "source": [
    "##### Trench 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e911dfc4-1248-46cd-8c30-66b9aab4323c",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d304eede-faa3-4ebe-9e8e-d4b007383292",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in  ('Beta-Cash-AppScore-Model', 'apps_score_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction aCicScore \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-AppScore-Model_Trench3' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  SAFE_CAST(JSON_VALUE(r.prediction_clean, \"$.combined_score\") AS Float64) AS beta_cash_app_score,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    ";\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc276c2-e941-4029-bcd8-bd4953d485d8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aacc674-5209-4a90-a276-08c2fc79ef3e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7a3363-5e82-4ee5-9906-105d5a5296ca",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e2bc97-bd24-4b12-827c-10f17d81a946",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in  ('Beta-Cash-AppScore-Model', 'apps_score_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction \n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-AppScore-Model_Trench3' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  prediction beta_cash_app_score,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7867d4ca-262b-4cf4-9ec1-c66e4a82049f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552e953a-9c4c-4aac-9452-a49298815954",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a668d805-6446-4dcb-8b62-98fd628de390",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36ecb18d-823a-4f83-b03c-511e9d74663e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a45017e-2469-44e3-9e9a-64292102a251",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "051ddaa7-0aba-4e3a-b052-01a55de8839e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b60c773-1433-4876-b591-cc469e541e31",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop(columns=['prediction', 'calc_beta_cash_app_score']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2edeabf5-8c37-4fc4-9adc-798e46baed1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa8b0fc-e787-4481-8aae-80a62eff8beb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['beta_cash_app_score'] = pd.to_numeric(df['beta_cash_app_score'], errors='coerce')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbbda16e-caaa-4f92-9214-3374c96c5b21",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e743b8-bc24-46fb-9cbe-e1c1ae24c684",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['beta_cash_app_score', 'app_cnt_health_and_fitness_ever', 'app_cnt_productivity_ever',\n",
    "       'app_cnt_rated_for_18plus_ever', 'app_cnt_books_and_reference_ever',\n",
    "       'app_cnt_gaming_180d', 'app_cnt_absence_tag_365d',\n",
    "       'app_last_payday_install_to_apply_days',\n",
    "       'app_cnt_absence_tag_365d_binned', 'app_cnt_gaming_180d_binned',\n",
    "       'app_cnt_productivity_ever_binned',\n",
    "       'app_cnt_rated_for_18plus_ever_binned',\n",
    "       'app_cnt_health_and_fitness_ever_binned',\n",
    "       'app_cnt_books_and_reference_ever_binned',\n",
    "       'app_last_payday_install_to_apply_days_binned', 'calc_ln_user_type'\n",
    "]\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_appscore_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9d06e41-ea25-40ef-b7c6-8bdd3cc4cbea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_appscore_model_psi_v4\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b215df90-4d6b-4fa6-8de3-0dadf64b60e3",
   "metadata": {},
   "source": [
    "#### Beta-Cash-Stack-Model Trench1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6760929-1090-4ede-a304-513915dd4230",
   "metadata": {},
   "source": [
    "##### Trench 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6aebf942-90bf-4b36-9dd5-6b4c40bc0bea",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f159ac3f-2f63-4d86-8179-6d2f8a7504da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Beta-Cash-Stack-Model', 'beta_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction Beta_cash_stack_score\n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Stack-Model_Trench1' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e2bb171-151e-458e-b45e-f4e80f209d98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93020863-fb5e-432e-9e3f-d41c5a754d39",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ffd157-4dbf-4e2c-989f-c2aac01da97d",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b34df95-90d0-4b7b-8dba-c587dcf48452",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Beta-Cash-Stack-Model', 'beta_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction Beta_cash_stack_score\n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Stack-Model_Trench1' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "762a5d37-6504-4cd9-b4a9-42837e617fc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07f19e1d-4b4f-47c5-bfb2-8314276a3900",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0593e7b6-8d8e-4f19-b935-6a9101ded0d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce63957-4038-4f35-8658-2d1f7fc4fcd5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22c70bde-f1f4-48d2-a9ee-954f74289416",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'calc_apps_score':'calc_app_score'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad4c57d-7f40-4cd1-8093-d6c09abb9c53",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfde157-cc0d-47b0-97e6-3e7e489f9e2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35bee06b-a382-4ae1-9e69-53f3db22abbe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat = df_concat.drop(columns=['prediction', 'calc_app_median_time_bw_installed_mins_ever', 'calc_app_avg_time_bw_installed_mins_ever', 'calc_beta_cash_app_score']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "864af338-b2ff-414a-ba22-25c51d4d278f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0070a81-7776-493f-8ee5-c879422712a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Beta_cash_stack_score'] = pd.to_numeric(df['Beta_cash_stack_score'], errors='coerce')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4f36215-b5b8-4f41-8fb8-b77752a25d16",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768de672-6dd4-487a-89a3-b997f2c7ebde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['Beta_cash_stack_score',\n",
    "       'demo_score', 'credo_score', 'app_score'\n",
    "]\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_stack_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb36c51-295d-4402-9489-b436b4d85cf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_stack_model_psi_v4\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a748e35-0633-4d9e-9667-17bd32348939",
   "metadata": {},
   "source": [
    "##### Trench 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266ec2a8-dea9-4da4-bcf0-4289fb907e41",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e4b8e2d-cd47-48a9-b5b7-d933a4e7bd34",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Beta-Cash-Stack-Model', 'beta_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction Beta_cash_stack_score\n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Stack-Model_Trench2' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7ba9136-12e1-4740-b456-006dfe01d7ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad3d6df-fb89-4160-93d3-f40d15a35abc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ccc1c3-00a5-4694-b07a-9e2bbb7b4c00",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "647cd825-1528-4330-820b-e61ad58266ea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Beta-Cash-Stack-Model', 'beta_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction Beta_cash_stack_score\n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Stack-Model_Trench2' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 2'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e88a4b-ed50-4442-98e0-2af448ac6036",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a5acd2-b178-479c-9a61-59e30af15119",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa898915-c299-4863-b9e0-6ca57eba4965",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e583f5b7-fee2-4a7e-a105-d01c6d55397d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d2451da-6662-4825-925d-25f7e6fffad9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'calc_apps_score':'calc_app_score'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67d09b81-d6e1-4628-94bd-84e3b4cefddf",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b0ff642-1c26-465e-ac9a-91a437fc3c43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7531b1c-7334-47bb-bd1d-bc68663a9670",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat = df_concat.drop(columns=['prediction', 'calc_app_median_time_bw_installed_mins_ever', 'calc_app_avg_time_bw_installed_mins_ever', 'calc_beta_cash_app_score']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d24de14-3a1c-4b36-9951-26d34208b4a3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8e0c215-e72d-4c5e-b398-2e48b5eeeffc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Beta_cash_stack_score'] = pd.to_numeric(df['Beta_cash_stack_score'], errors='coerce')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bac05c0-d2a6-441e-83e6-05463a39d67f",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce82fa2a-2578-480c-a8e2-edc28467a742",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['Beta_cash_stack_score',\n",
    "       'demo_score', 'credo_score', 'app_score'\n",
    "]\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_stack_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97560650-e1f9-432a-9efe-df43c1cbb75e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_stack_model_psi_v4\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aaeaa59-fae6-4cc1-85bd-63597b245862",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "fd00335f-eafc-41b1-9e3a-b2ea9a5ef7de",
   "metadata": {},
   "source": [
    "##### Trench 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d01a9bb-e991-4e2c-9ab3-272dad921b2a",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b49353-b906-4881-befd-9863d657c891",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName in ('Beta-Cash-Stack-Model', 'beta_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction Beta_cash_stack_score\n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Stack-Model_Trench3' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Test' Data_selection,\n",
    "  calcFeatures,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054d7d02-8d47-46ba-b0be-62a4b01633b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5958ea5a-5211-4aa1-a4bf-2f5a2bba5a8b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb65b3-259e-4d16-88f7-17358ce50228",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc216bc-315c-489f-b9db-1bf3b05e1147",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "where modelDisplayName in ('Beta-Cash-Stack-Model', 'beta_stack_model_cash')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction Beta_cash_stack_score\n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Stack-Model_Trench3' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 3'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a6fc7-807d-4f65-9737-f6a7f987cdad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# dfd.groupby(['Application_month', 'trenchCategory'])['digitalLoanAccountId'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c567ee-b9b2-446b-923c-576cd73e07d9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829aace-ad75-441b-a141-6ebe6950482a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09eeb610-90ac-4cb7-8db7-a707d4f48a60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88ef2e33-8739-499d-aca5-a3ca570544ed",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df2.rename(columns={'calc_apps_score':'calc_app_score'}, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e5927f-5626-44c7-9a2c-cfef358e2e15",
   "metadata": {},
   "source": [
    "##### concatenate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1ae2e91-10bc-49a1-9935-264d7c237f69",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4218e014-fafb-43c0-9cc7-9778089dfa72",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_concat = df_concat.drop(columns=['prediction', 'calc_app_median_time_bw_installed_mins_ever', 'calc_app_avg_time_bw_installed_mins_ever', 'calc_beta_cash_app_score']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "487400cb-9900-4188-9e57-8fc584f5b8da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df_concat.drop(columns=['calcFeatures']).copy()\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc8c3e19-2389-4e33-8610-eaef57b7740d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df['Beta_cash_stack_score'] = pd.to_numeric(df['Beta_cash_stack_score'], errors='coerce')\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52ded19-667d-43a0-96d9-273d93ea30d5",
   "metadata": {},
   "source": [
    "##### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd038799-3690-469f-99c6-9db5084e7627",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = df.copy()\n",
    "\n",
    "# Define feature list\n",
    "feature_list = ['Beta_cash_stack_score',\n",
    "       'demo_score', 'credo_score', 'app_score'\n",
    "]\n",
    "\n",
    "# Define segment columns\n",
    "segment_columns = ['new_loan_type','osType', 'loan_product_type']\n",
    "# Calculate month-on-month PSI\n",
    "psi_results = calculate_month_on_month_psi(df, feature_list, segment_columns)\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "psi_results = psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "                           'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                           'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                           'Expected_Percentage', 'Actual_Percentage', 'PSI']].copy()\n",
    "\n",
    "# # Calculate bin-level PSI\n",
    "# bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "# bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "# bin_psi_results['Model_Name'] = df['Model_Name'].iloc[0]\n",
    "# bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "# bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "# bin_psi_results = bin_psi_results[['modelDisplayName', 'Model_Name', 'modelVersionId', 'trenchCategory',\n",
    "#                                    'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value',\n",
    "#                                     'Month', 'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "#                                     'Bin', 'Bin_Range', 'Base_Percentage', 'Actual_Percentage', 'Bin_PSI']].copy()\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_stack_model_psi_v4\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# # Upload to BigQuery\n",
    "# table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_income_estimation_model_csi_v4\"\n",
    "# job_config = bigquery.LoadJobConfig(\n",
    "#     write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    "# )\n",
    "# job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "# job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de013bd0-6d32-4189-975e-3f50aa5bc0e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sq = \"\"\"select * from prj-prod-dataplatform.dap_ds_poweruser_playground.beta_cash_stack_model_psi_v4\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.groupby(['trenchCategory','Month'])['modelDisplayName'].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d28f7e69-018e-4f17-9c77-38c0e376d0b1",
   "metadata": {},
   "source": [
    "# End"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedbf9ba-9990-41b5-a087-e103c35434a9",
   "metadata": {},
   "source": [
    "# SIL V2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64840d",
   "metadata": {},
   "source": [
    "#### Alpha - CIC-SIL-Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d0c88a",
   "metadata": {},
   "source": [
    "##### Trench 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47ef48b6",
   "metadata": {},
   "source": [
    "##### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84cc9752",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is for the test period of Alpha - CIC sil model - reading the data from ml_model_run_details\n",
    "\n",
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,modelDisplayName,modelVersionId, trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in ('Alpha - CIC-SIL-Model', 'cic_model_sil')\n",
    "  and modelVersionId='v2'\n",
    "  ),\n",
    "base as\n",
    "(SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction Alpha_cic_sil_score\n",
    "    ,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "   loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "        when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "        when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "        else 'ios' end osType,\n",
    " 'Alpha - CIC-SIL-Model' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Test' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId order by r.start_time desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    ";\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()\n",
    "\n",
    "## this data is not expanded. We will have to expand and get the features from the calcFeatures column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46f47e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df1 = expanded_df.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f53d134",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cb73465",
   "metadata": {},
   "source": [
    "##### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23c4e3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is for the test period of Alpha - CIC sil model - reading the data from ml_model_run_details\n",
    "\n",
    "sq = \"\"\"\n",
    "WITH parsed as (\n",
    "select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,trenchCategory,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "WHERE modelDisplayName in ('Alpha - CIC-SIL-Model', 'cic_model_sil')\n",
    "),\n",
    "base as\n",
    "(select \n",
    " r.customerId,r.digitalLoanAccountId,prediction Alpha_cic_sil_score\n",
    " ,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type loanType,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'Cash' as product,\n",
    " 'Beta-Cash-Stack-Model_Trench1' Model_Name,\n",
    "  trenchCategory,\n",
    "   'Train' Data_selection,\n",
    "  calcFeatures,\n",
    "  IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m',IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month, \n",
    "from parsed r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid order by cast(start_time as datetime) desc) = 1\n",
    ")\n",
    "select * from base where trenchCategory = 'Trench 1'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "dfd.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d97748",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Expand the calcFeatures column\n",
    "expanded_df = expand_calc_features(dfd)\n",
    "\n",
    "# Display the result\n",
    "print(f\"Original columns: {dfd.shape[1]}\")\n",
    "print(f\"Expanded columns: {expanded_df.shape[1]}\")\n",
    "df2 = expanded_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39fe8a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.rename(columns={'loan_type':'new_loan_type'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2b07c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "667e79d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bf0355c",
   "metadata": {},
   "source": [
    "##### concatenation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cab8e7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "df_concat.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893f4c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = df_concat.drop(columns=['calc_digitalLoanAccountId', 'calc_customerId', 'calc_crifApplicationId']).copy()\n",
    "df_concat.columns = df_concat.columns.str.replace('calc_', '', regex=False)\n",
    "df_concat.columns"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "conda-base-py",
   "name": "workbench-notebooks.m128",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/workbench-notebooks:m128"
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
