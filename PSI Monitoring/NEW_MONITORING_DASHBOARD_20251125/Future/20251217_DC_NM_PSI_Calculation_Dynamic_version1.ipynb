{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e485d58",
   "metadata": {},
   "source": [
    "- In this Notebook, I am trying to integrate the training data from the backscore and rest of the period from prj-prod-dataplatform.audit_balance.ml_model_run_details table.\n",
    "- In this I will compare the training period with each month of test period."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e3f7c8e",
   "metadata": {},
   "source": [
    "**Steps to Follow**:\n",
    "\n",
    "* Read the specific model data from prj-prod-dataplatform.audit_balance.ml_model_run_details table\n",
    "* Expand the calcFeature column to extract all the features for the model\n",
    "* Read the data from specific backscore table for the training data\n",
    "* Identify the features and create a list\n",
    "* Use transform_data function to create the same structure as ml_model_run_details table\n",
    "* Insert the data to a similar training table - prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "* Read the specific model data from prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "* expand the training set from the calcFeature column\n",
    "* Concatenate both the test and train datasets\n",
    "* Calculate the PSI using the PSI function comparing it with the train set\n",
    "* Insert the result to a PSI table prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cic_sil_model_psi_v4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d30699a5",
   "metadata": {},
   "source": [
    "# **PSI - CSI Calculation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421e2a89",
   "metadata": {},
   "source": [
    "This is based on the decile split of the score and features and for categorical top 20 and rest as others "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e7e82",
   "metadata": {},
   "source": [
    "## Define Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c269fa0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "import gcsfs\n",
    "import duckdb as dd\n",
    "import pickle\n",
    "import joblib\n",
    "from typing import Union\n",
    "import io\n",
    "path = r'C:\\Users\\Dwaipayan\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"prj-prod-dataplatform\"\n",
    "\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"Display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd045879",
   "metadata": {},
   "source": [
    "## Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5936ef22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method 1: Using regex to remove all \"Calc_\" occurrences\n",
    "def clean_names_regex(name):\n",
    "    return re.sub(r'_Calc_', '_', name)\n",
    "\n",
    "# Method 2: Simple string replacement\n",
    "def clean_names_replace(name):\n",
    "    return name.replace('_Calc_', '_')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfc0567",
   "metadata": {},
   "source": [
    "## expand_calc_features_robust"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad291987",
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_calc_features_robust(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns.\n",
    "    Column names will be prefixed with modelVersionId_Calc_\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe with 'calcFeatures' column\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Dataframe with expanded features\n",
    "    \"\"\"\n",
    "    df_expanded = df.copy()\n",
    "    \n",
    "    if 'calcFeatures' not in df_expanded.columns:\n",
    "        print(\"Warning: 'calcFeatures' column not found in dataframe\")\n",
    "        return df_expanded\n",
    "    \n",
    "    # Get modelVersionId (assuming all rows have same modelVersionId in filtered dataframe)\n",
    "    if len(df_expanded) == 0:\n",
    "        return df_expanded\n",
    "    \n",
    "    model_version_id = df_expanded['modelVersionId'].iloc[0]\n",
    "    prefix = f\"{model_version_id}_Calc_\"\n",
    "    \n",
    "    all_features_data = []\n",
    "    \n",
    "    for idx, row in df_expanded.iterrows():\n",
    "        calc_features_str = row['calcFeatures']\n",
    "        row_features = {}\n",
    "        \n",
    "        if pd.isna(calc_features_str):\n",
    "            all_features_data.append(row_features)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Clean the string and parse JSON\n",
    "            features_str = str(calc_features_str)\n",
    "            # Handle common JSON issues\n",
    "            features_str = features_str.replace(\"'\", '\"').replace('None', 'null').replace('True', 'true').replace('False', 'false')\n",
    "            \n",
    "            features_dict = json.loads(features_str)\n",
    "            \n",
    "            if isinstance(features_dict, dict):\n",
    "                row_features = features_dict\n",
    "            else:\n",
    "                print(f\"Warning: calcFeatures is not a dictionary at index {idx}\")\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: JSON decode error at index {idx}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing calcFeatures at index {idx}: {e}\")\n",
    "        \n",
    "        all_features_data.append(row_features)\n",
    "    \n",
    "    # Create DataFrame from features\n",
    "    features_df = pd.DataFrame(all_features_data)\n",
    "    \n",
    "    # Add prefix to column names\n",
    "    features_df = features_df.add_prefix(prefix)\n",
    "    \n",
    "    # Combine with original dataframe\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    features_df = features_df.reset_index(drop=True)\n",
    "    \n",
    "    result_df = pd.concat([df_expanded, features_df], axis=1)\n",
    "    \n",
    "    # Optionally drop the original calcFeatures column\n",
    "    if 'calcFeatures' in result_df.columns:\n",
    "        result_df = result_df.drop('calcFeatures', axis=1)\n",
    "    \n",
    "    return result_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98dc0773",
   "metadata": {},
   "source": [
    "### dropping_duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4efc2786",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropping_duplicates(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Drop duplicates based on digitalLoanAccountId, Data_selection, and modelVersionid,\n",
    "    keeping the first occurrence based on appln_submit_datetime.\n",
    "\n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        Input dataframe\n",
    "\n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame with duplicates dropped\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.sort_values(\n",
    "        ['digitalLoanAccountId', 'Data_selection', 'modelVersionId', 'appln_submit_datetime'],\n",
    "        ascending=[True, True, True, True],\n",
    "        na_position='last'\n",
    "    )\n",
    "\n",
    "    result = df.drop_duplicates(\n",
    "        subset=['digitalLoanAccountId', 'Data_selection', 'modelVersionId'],\n",
    "        keep='first'\n",
    "    ).copy()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802dc01f",
   "metadata": {},
   "source": [
    "### PSI pipeline Version 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4195bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def calculate_psi_for_model(dfcombined: pd.DataFrame,\n",
    "                            configdf: pd.DataFrame,\n",
    "                            model_display_name: str,\n",
    "                            debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate PSI for a specific model based on configdf combinations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter configdf to only include combinations for this specific modelDisplayName\n",
    "    model_config = configdf[configdf['modelDisplayName'] == model_display_name].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting PSI Pipeline for Model: {model_display_name}\")\n",
    "    print(f\"Total combinations to process: {len(model_config)}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    if len(model_config) == 0:\n",
    "        print(f\"ERROR: No configurations found for modelDisplayName={model_display_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Process each unique combination\n",
    "    all_results = []\n",
    "    \n",
    "    for idx, config_row in model_config.iterrows():\n",
    "        model_version_id = config_row['modelVersionId']\n",
    "        trench_category = config_row['trenchCategory']\n",
    "        \n",
    "        print(f\"Processing combination {idx + 1}/{len(model_config)}: \"\n",
    "              f\"modelVersionId={model_version_id}, trenchCategory={trench_category}\")\n",
    "        \n",
    "        # Filter data from dfcombined based on modelVersionId\n",
    "        combo_df = dfcombined[dfcombined['modelVersionId'] == model_version_id].copy()\n",
    "        \n",
    "        # If trenchCategory is not 'ALL', filter by it\n",
    "        if trench_category != 'ALL':\n",
    "            combo_df = combo_df[combo_df['trenchCategory'] == trench_category].copy()\n",
    "        \n",
    "        if len(combo_df) == 0:\n",
    "            print(f\"  Warning: No data found for this combination. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Data points: {len(combo_df)}\")\n",
    "        \n",
    "        # Expand calcFeatures\n",
    "        try:\n",
    "            combo_df = expand_calc_features_robust(combo_df)\n",
    "            if debug:\n",
    "                print(f\"  Features expanded successfully. Columns: {combo_df.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error expanding features: {e}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Extract feature list (all columns starting with modelVersionId_Calc_)\n",
    "        feature_prefix = f\"{model_version_id}_Calc_\"\n",
    "        feature_list = [col for col in combo_df.columns if col.startswith(feature_prefix)]\n",
    "        \n",
    "        # Also include 'score' if it exists\n",
    "        if 'score' in combo_df.columns:\n",
    "            feature_list.append('score')\n",
    "        \n",
    "        if len(feature_list) == 0:\n",
    "            print(f\"  Warning: No features found after expansion. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Features identified: {len(feature_list)}\")\n",
    "        \n",
    "        # Define segment columns\n",
    "        segment_columns = ['new_loan_type', 'osType', 'loan_product_type', 'trenchCategory']\n",
    "        # Filter to only existing columns\n",
    "        segment_columns = [col for col in segment_columns if col in combo_df.columns]\n",
    "        \n",
    "        # Calculate PSI (overall + segments + score)\n",
    "        try:\n",
    "            psi_result = calculate_month_on_month_psi(\n",
    "                combo_df,\n",
    "                feature_list,\n",
    "                segment_columns=segment_columns\n",
    "            )\n",
    "            \n",
    "            # Add model metadata\n",
    "            psi_result['modelDisplayName'] = model_display_name\n",
    "            psi_result['modelVersionId'] = model_version_id\n",
    "            psi_result['trenchCategory'] = trench_category\n",
    "            \n",
    "            all_results.append(psi_result)\n",
    "            print(f\"  PSI calculated: {len(psi_result)} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error calculating PSI: {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        final_result = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Reorder columns to match required output\n",
    "        column_order = ['modelDisplayName', 'modelVersionId', 'trenchCategory',\n",
    "                       'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                       'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                       'Expected_Percentage', 'Actual_Percentage', 'PSI', 'PSI_Interpretation']\n",
    "        \n",
    "        # Keep only columns that exist\n",
    "        available_cols = [col for col in column_order if col in final_result.columns]\n",
    "        final_result = final_result[available_cols]\n",
    "        \n",
    "        # Sort results\n",
    "        final_result = final_result.sort_values(['modelVersionId', 'trenchCategory', \n",
    "                                                'Feature', 'Month', 'Segment_Column'])\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Pipeline Complete!\")\n",
    "        print(f\"Total rows in final output: {len(final_result)}\")\n",
    "        print(f\"Unique combinations processed: {len(final_result[['modelVersionId', 'trenchCategory']].drop_duplicates())}\")\n",
    "        print(f\"Unique features processed: {final_result['Feature'].nunique()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        return final_result\n",
    "    else:\n",
    "        print(\"No results generated. Check input data and configurations.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def expand_calc_features_robust(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns.\n",
    "    \"\"\"\n",
    "    df_expanded = df.copy()\n",
    "    \n",
    "    if 'calcFeatures' not in df_expanded.columns:\n",
    "        print(\"Warning: 'calcFeatures' column not found in dataframe\")\n",
    "        return df_expanded\n",
    "    \n",
    "    # Get modelVersionId (assuming all rows have same modelVersionId in filtered dataframe)\n",
    "    if len(df_expanded) == 0:\n",
    "        return df_expanded\n",
    "    \n",
    "    model_version_id = df_expanded['modelVersionId'].iloc[0]\n",
    "    prefix = f\"{model_version_id}_Calc_\"\n",
    "    \n",
    "    all_features_data = []\n",
    "    \n",
    "    for idx, row in df_expanded.iterrows():\n",
    "        calc_features_str = row['calcFeatures']\n",
    "        row_features = {}\n",
    "        \n",
    "        if pd.isna(calc_features_str):\n",
    "            all_features_data.append(row_features)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Clean the string and parse JSON\n",
    "            features_str = str(calc_features_str)\n",
    "            # Handle common JSON issues\n",
    "            features_str = features_str.replace(\"'\", '\"').replace('None', 'null').replace('True', 'true').replace('False', 'false')\n",
    "            \n",
    "            features_dict = json.loads(features_str)\n",
    "            \n",
    "            if isinstance(features_dict, dict):\n",
    "                row_features = features_dict\n",
    "            else:\n",
    "                print(f\"Warning: calcFeatures is not a dictionary at index {idx}\")\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: JSON decode error at index {idx}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing calcFeatures at index {idx}: {e}\")\n",
    "        \n",
    "        all_features_data.append(row_features)\n",
    "    \n",
    "    # Create DataFrame from features\n",
    "    features_df = pd.DataFrame(all_features_data)\n",
    "    \n",
    "    # Add prefix to column names\n",
    "    features_df = features_df.add_prefix(prefix)\n",
    "    \n",
    "    # Combine with original dataframe\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    features_df = features_df.reset_index(drop=True)\n",
    "    \n",
    "    result_df = pd.concat([df_expanded, features_df], axis=1)\n",
    "    \n",
    "    # Optionally drop the original calcFeatures column\n",
    "    if 'calcFeatures' in result_df.columns:\n",
    "        result_df = result_df.drop('calcFeatures', axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Identify categorical and numerical features from the feature list.\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        if feature not in df.columns:\n",
    "            print(f\"Warning: Feature {feature} not found in dataframe\")\n",
    "            continue\n",
    "        \n",
    "        # Check if column exists and has data\n",
    "        if df[feature].isnull().all():\n",
    "            print(f\"Warning: Feature {feature} is all null\")\n",
    "            continue\n",
    "        \n",
    "        # For numeric columns, check unique values\n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            unique_count = df[feature].nunique()\n",
    "            \n",
    "            # Check if it's likely categorical (few unique values and integers)\n",
    "            if unique_count <= 10:\n",
    "                sample_values = df[feature].dropna().head(100)\n",
    "                # Check if values are essentially integers\n",
    "                if all(abs(val - int(val)) < 0.0001 if not pd.isna(val) else True for val in sample_values):\n",
    "                    categorical_features.append(feature)\n",
    "                else:\n",
    "                    numerical_features.append(feature)\n",
    "            else:\n",
    "                numerical_features.append(feature)\n",
    "        else:\n",
    "            # Non-numeric columns are treated as categorical\n",
    "            categorical_features.append(feature)\n",
    "    \n",
    "    print(f\"Identified {len(numerical_features)} numerical and {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    return {\n",
    "        'categorical': categorical_features,\n",
    "        'numerical': numerical_features\n",
    "    }\n",
    "\n",
    "\n",
    "def create_bins_for_features(df: pd.DataFrame,\n",
    "                             numerical_features: List[str],\n",
    "                             categorical_features: List[str],\n",
    "                             train_period_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create bins for numerical features and categorical features based on training period.\n",
    "    \"\"\"\n",
    "    binning_info = {}\n",
    "    \n",
    "    # Process numerical features\n",
    "    for feature in numerical_features:\n",
    "        if feature not in train_period_df.columns:\n",
    "            binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "            continue\n",
    "        \n",
    "        feature_data = train_period_df[feature].dropna()\n",
    "        \n",
    "        if len(feature_data) == 0:\n",
    "            binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "            continue\n",
    "        \n",
    "        # Create bins based on percentiles\n",
    "        try:\n",
    "            # Try decile bins\n",
    "            percentiles = np.percentile(feature_data, np.arange(0, 101, 10))\n",
    "            percentiles = np.unique(percentiles)\n",
    "            \n",
    "            # Ensure we have at least 2 unique bins\n",
    "            if len(percentiles) >= 2:\n",
    "                bins = percentiles.copy()\n",
    "                # Ensure first bin starts at -inf and last at inf\n",
    "                bins[0] = -np.inf\n",
    "                bins[-1] = np.inf\n",
    "                \n",
    "                # Create bin labels and ranges\n",
    "                bin_ranges = {}\n",
    "                for i in range(len(bins)-1):\n",
    "                    bin_name = f\"Bin_{i+1}\"\n",
    "                    lower = bins[i]\n",
    "                    upper = bins[i+1]\n",
    "                    \n",
    "                    if np.isinf(lower) and np.isinf(upper):\n",
    "                        range_str = \"(-inf, inf)\"\n",
    "                    elif np.isinf(lower):\n",
    "                        range_str = f\"(-inf, {upper:.3f})\"\n",
    "                    elif np.isinf(upper):\n",
    "                        range_str = f\"[{lower:.3f}, inf)\"\n",
    "                    else:\n",
    "                        range_str = f\"[{lower:.3f}, {upper:.3f})\"\n",
    "                    \n",
    "                    bin_ranges[bin_name] = {\n",
    "                        'min': lower,\n",
    "                        'max': upper,\n",
    "                        'range_str': range_str\n",
    "                    }\n",
    "                \n",
    "                binning_info[feature] = {\n",
    "                    'type': 'numerical',\n",
    "                    'bins': bins,\n",
    "                    'bin_ranges': bin_ranges,\n",
    "                    'bin_count': len(bins) - 1\n",
    "                }\n",
    "            else:\n",
    "                # Fallback: simple min-max bins\n",
    "                min_val = feature_data.min()\n",
    "                max_val = feature_data.max()\n",
    "                \n",
    "                if min_val == max_val:\n",
    "                    # Handle constant value\n",
    "                    bins = np.array([-np.inf, min_val - 0.001, min_val, min_val + 0.001, np.inf])\n",
    "                else:\n",
    "                    bins = np.array([-np.inf, min_val, max_val, np.inf])\n",
    "                \n",
    "                bin_ranges = {\n",
    "                    'Bin_1': {'min': -np.inf, 'max': min_val, 'range_str': f'(-inf, {min_val:.3f})'},\n",
    "                    'Bin_2': {'min': min_val, 'max': max_val, 'range_str': f'[{min_val:.3f}, {max_val:.3f})'},\n",
    "                    'Bin_3': {'min': max_val, 'max': np.inf, 'range_str': f'[{max_val:.3f}, inf)'}\n",
    "                }\n",
    "                \n",
    "                binning_info[feature] = {\n",
    "                    'type': 'numerical',\n",
    "                    'bins': bins,\n",
    "                    'bin_ranges': bin_ranges,\n",
    "                    'bin_count': len(bins) - 1\n",
    "                }\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating bins for {feature}: {e}\")\n",
    "            # Fallback to simple bins\n",
    "            min_val = feature_data.min()\n",
    "            max_val = feature_data.max()\n",
    "            \n",
    "            if min_val == max_val:\n",
    "                bins = np.array([-np.inf, min_val - 0.001, min_val, min_val + 0.001, np.inf])\n",
    "            else:\n",
    "                bins = np.array([-np.inf, min_val, max_val, np.inf])\n",
    "            \n",
    "            binning_info[feature] = {\n",
    "                'type': 'numerical',\n",
    "                'bins': bins,\n",
    "                'bin_ranges': {},\n",
    "                'bin_count': len(bins) - 1\n",
    "            }\n",
    "    \n",
    "    # Process categorical features\n",
    "    for feature in categorical_features:\n",
    "        if feature not in train_period_df.columns:\n",
    "            binning_info[feature] = {'type': 'categorical', 'top_categories': [], 'bin_ranges': {}}\n",
    "            continue\n",
    "        \n",
    "        value_counts = train_period_df[feature].value_counts()\n",
    "        \n",
    "        # Get top categories (up to 10)\n",
    "        top_categories = value_counts.head(10).index.tolist()\n",
    "        \n",
    "        # Ensure 'Missing' is treated separately\n",
    "        if 'Missing' in top_categories:\n",
    "            top_categories.remove('Missing')\n",
    "        \n",
    "        binning_info[feature] = {\n",
    "            'type': 'categorical',\n",
    "            'top_categories': top_categories,\n",
    "            'all_categories': value_counts.index.tolist(),\n",
    "            'value_counts': value_counts.to_dict()\n",
    "        }\n",
    "    \n",
    "    return binning_info\n",
    "\n",
    "\n",
    "def apply_binning(df: pd.DataFrame, feature: str, binning_info: Dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply binning to a feature based on binning information.\n",
    "    \"\"\"\n",
    "    if feature not in df.columns:\n",
    "        return pd.Series(['Feature_Not_Found'] * len(df), index=df.index)\n",
    "    \n",
    "    if binning_info['type'] == 'numerical':\n",
    "        if binning_info['bins'] is None or len(binning_info['bins']) < 2:\n",
    "            # Handle missing or invalid bins\n",
    "            result = pd.Series(['No_Bins'] * len(df), index=df.index)\n",
    "            result[df[feature].isna()] = 'Missing'\n",
    "            return result\n",
    "        \n",
    "        bins = binning_info['bins']\n",
    "        # Create labels\n",
    "        labels = [f\"Bin_{i+1}\" for i in range(len(bins)-1)]\n",
    "        \n",
    "        try:\n",
    "            # Apply binning\n",
    "            binned = pd.cut(df[feature], bins=bins, labels=labels, \n",
    "                          include_lowest=True, duplicates='drop')\n",
    "            binned = binned.astype(str)\n",
    "            \n",
    "            # Handle NaN values\n",
    "            binned[df[feature].isna()] = 'Missing'\n",
    "            \n",
    "            # Handle any other issues\n",
    "            binned[binned.isna()] = 'Other'\n",
    "            \n",
    "            return binned\n",
    "        except Exception as e:\n",
    "            print(f\"Error binning {feature}: {e}\")\n",
    "            return pd.Series(['Binning_Error'] * len(df), index=df.index)\n",
    "    \n",
    "    else:  # Categorical\n",
    "        top_categories = binning_info.get('top_categories', [])\n",
    "        \n",
    "        # Convert to string and handle NaN\n",
    "        feature_data = df[feature].astype(str)\n",
    "        feature_data[df[feature].isna()] = 'Missing'\n",
    "        \n",
    "        # Apply categorization\n",
    "        def categorize_value(x):\n",
    "            if x == 'Missing':\n",
    "                return 'Missing'\n",
    "            elif x in top_categories:\n",
    "                return x\n",
    "            elif str(x) in top_categories:\n",
    "                return str(x)\n",
    "            else:\n",
    "                return 'Others'\n",
    "        \n",
    "        binned = feature_data.apply(categorize_value)\n",
    "        return binned\n",
    "\n",
    "\n",
    "def calculate_psi(expected_pct: pd.Series, actual_pct: pd.Series, \n",
    "                  epsilon: float = 1e-10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index.\n",
    "    \"\"\"\n",
    "    # Align both series on the same index\n",
    "    all_bins = set(expected_pct.index) | set(actual_pct.index)\n",
    "    \n",
    "    # Create aligned series\n",
    "    expected_aligned = pd.Series(0.0, index=list(all_bins))\n",
    "    actual_aligned = pd.Series(0.0, index=list(all_bins))\n",
    "    \n",
    "    for idx in all_bins:\n",
    "        if idx in expected_pct.index:\n",
    "            expected_aligned[idx] = expected_pct[idx]\n",
    "        if idx in actual_pct.index:\n",
    "            actual_aligned[idx] = actual_pct[idx]\n",
    "    \n",
    "    # Apply epsilon to avoid zeros\n",
    "    expected_aligned = expected_aligned.apply(lambda x: max(x, epsilon))\n",
    "    actual_aligned = actual_aligned.apply(lambda x: max(x, epsilon))\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    expected_aligned = expected_aligned / expected_aligned.sum()\n",
    "    actual_aligned = actual_aligned / actual_aligned.sum()\n",
    "    \n",
    "    # Calculate PSI\n",
    "    psi_value = np.sum((actual_aligned - expected_aligned) * \n",
    "                      np.log(actual_aligned / expected_aligned))\n",
    "    \n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def interpret_psi(psi_value: float) -> str:\n",
    "    \"\"\"\n",
    "    Interpret PSI value based on industry standards.\n",
    "    \"\"\"\n",
    "    if psi_value < 0.1:\n",
    "        return \"Very Stable\"\n",
    "    elif psi_value < 0.2:\n",
    "        return \"Stable\"\n",
    "    elif psi_value < 0.5:\n",
    "        return \"Moderate Shift\"\n",
    "    else:\n",
    "        return \"Significant Shift\"\n",
    "\n",
    "\n",
    "def calculate_month_on_month_psi(df: pd.DataFrame,\n",
    "                                 feature_list: List[str],\n",
    "                                 segment_columns: List[str] = [],\n",
    "                                 month_col: str = 'Application_month',\n",
    "                                 data_selection_col: str = 'Data_selection',\n",
    "                                 account_id_col: str = 'digitalLoanAccountId') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate PSI for each feature comparing training period vs each month,\n",
    "    with overall and segment-level breakdowns.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Separate train and test data\n",
    "    train_mask = df[data_selection_col] == 'Train'\n",
    "    test_mask = df[data_selection_col] != 'Train'\n",
    "    \n",
    "    train_df = df[train_mask].copy()\n",
    "    test_df = df[test_mask].copy()\n",
    "    \n",
    "    if len(train_df) == 0:\n",
    "        raise ValueError(\"No training data found. Check Data_selection column.\")\n",
    "    \n",
    "    if len(test_df) == 0:\n",
    "        raise ValueError(\"No test data found. Check Data_selection column.\")\n",
    "    \n",
    "    # Identify feature types\n",
    "    feature_types = identify_feature_types(df, feature_list)\n",
    "    \n",
    "    # Create bins based on training data\n",
    "    binning_info = create_bins_for_features(\n",
    "        df, \n",
    "        feature_types['numerical'], \n",
    "        feature_types['categorical'], \n",
    "        train_df\n",
    "    )\n",
    "    \n",
    "    # Get test months\n",
    "    test_months = sorted(test_df[month_col].unique())\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Create temporary binned columns\n",
    "    temp_columns = []\n",
    "    for feature in feature_list:\n",
    "        if feature not in binning_info:\n",
    "            continue\n",
    "            \n",
    "        binned_col = f'{feature}_binned'\n",
    "        df[binned_col] = apply_binning(df, feature, binning_info[feature])\n",
    "        temp_columns.append(binned_col)\n",
    "    \n",
    "    # 1. Calculate overall PSI (no segments)\n",
    "    print(\"  Calculating overall PSI...\")\n",
    "    for feature in feature_list:\n",
    "        if feature not in binning_info:\n",
    "            continue\n",
    "            \n",
    "        binned_col = f'{feature}_binned'\n",
    "        \n",
    "        # Get baseline distribution from ALL training data\n",
    "        train_baseline = df[train_mask][binned_col].value_counts(normalize=True)\n",
    "        \n",
    "        # Skip if baseline is empty\n",
    "        if len(train_baseline) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get total training count (across all training months)\n",
    "        total_train_count = train_df[account_id_col].nunique()\n",
    "        \n",
    "        for month in test_months:\n",
    "            month_mask = df[month_col] == month\n",
    "            test_month_mask = month_mask & test_mask\n",
    "            \n",
    "            # Get current month distribution\n",
    "            actual_dist = df[test_month_mask][binned_col].value_counts(normalize=True)\n",
    "            \n",
    "            # Skip if no data for this month\n",
    "            if len(actual_dist) == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate PSI\n",
    "            psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "            \n",
    "            # Get counts\n",
    "            base_count = total_train_count  # Total training data count\n",
    "            actual_count = df[test_month_mask][account_id_col].nunique()\n",
    "            \n",
    "            # Get distribution statistics\n",
    "            expected_avg_pct = (train_baseline * 100).mean() if len(train_baseline) > 0 else 0\n",
    "            actual_avg_pct = (actual_dist * 100).mean() if len(actual_dist) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'Feature': feature,\n",
    "                'Feature_Type': binning_info[feature]['type'],\n",
    "                'Segment_Column': 'Overall',\n",
    "                'Segment_Value': 'All',\n",
    "                'Month': month,\n",
    "                'Base_Month': 'All_Training_Months',\n",
    "                'Current_Month': month,\n",
    "                'Base_Count': base_count,\n",
    "                'Actual_Count': actual_count,\n",
    "                'Expected_Percentage': expected_avg_pct,\n",
    "                'Actual_Percentage': actual_avg_pct,\n",
    "                'PSI': psi_value,\n",
    "                'PSI_Interpretation': interpret_psi(psi_value)\n",
    "            })\n",
    "    \n",
    "    # 2. Calculate segment-level PSI\n",
    "    print(\"  Calculating segment-level PSI...\")\n",
    "    for segment_col in segment_columns:\n",
    "        if segment_col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        segments = df[segment_col].dropna().unique()\n",
    "        \n",
    "        for segment_val in segments:\n",
    "            segment_mask = df[segment_col] == segment_val\n",
    "            \n",
    "            # Skip if segment has no data\n",
    "            if not segment_mask.any():\n",
    "                continue\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                if feature not in binning_info:\n",
    "                    continue\n",
    "                    \n",
    "                binned_col = f'{feature}_binned'\n",
    "                \n",
    "                # Get training baseline for this segment (ALL training data for this segment)\n",
    "                train_segment_mask = train_mask & segment_mask\n",
    "                \n",
    "                if not train_segment_mask.any():\n",
    "                    continue\n",
    "                    \n",
    "                train_baseline = df[train_segment_mask][binned_col].value_counts(normalize=True)\n",
    "                \n",
    "                if len(train_baseline) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get training count for this segment\n",
    "                segment_train_count = df[train_segment_mask][account_id_col].nunique()\n",
    "                \n",
    "                for month in test_months:\n",
    "                    month_segment_mask = (df[month_col] == month) & segment_mask & test_mask\n",
    "                    \n",
    "                    if not month_segment_mask.any():\n",
    "                        continue\n",
    "                    \n",
    "                    actual_dist = df[month_segment_mask][binned_col].value_counts(normalize=True)\n",
    "                    \n",
    "                    if len(actual_dist) == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    # Calculate PSI\n",
    "                    psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "                    \n",
    "                    # Get counts\n",
    "                    base_count = segment_train_count\n",
    "                    actual_count = df[month_segment_mask][account_id_col].nunique()\n",
    "                    \n",
    "                    # Get distribution statistics\n",
    "                    expected_avg_pct = (train_baseline * 100).mean() if len(train_baseline) > 0 else 0\n",
    "                    actual_avg_pct = (actual_dist * 100).mean() if len(actual_dist) > 0 else 0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Feature': feature,\n",
    "                        'Feature_Type': binning_info[feature]['type'],\n",
    "                        'Segment_Column': segment_col,\n",
    "                        'Segment_Value': str(segment_val),\n",
    "                        'Month': month,\n",
    "                        'Base_Month': 'All_Training_Months',\n",
    "                        'Current_Month': month,\n",
    "                        'Base_Count': base_count,\n",
    "                        'Actual_Count': actual_count,\n",
    "                        'Expected_Percentage': expected_avg_pct,\n",
    "                        'Actual_Percentage': actual_avg_pct,\n",
    "                        'PSI': psi_value,\n",
    "                        'PSI_Interpretation': interpret_psi(psi_value)\n",
    "                    })\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    for col in temp_columns:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# CORRECTED VALIDATION FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def validate_psi_counts(psi_results: pd.DataFrame, dfcombined: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate the counts in PSI results against the original data.\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "    \n",
    "    for idx, row in psi_results.iterrows():\n",
    "        model_display_name = row['modelDisplayName']\n",
    "        model_version_id = row['modelVersionId']\n",
    "        config_trench = row['trenchCategory']  # This is from config: can be 'ALL' or specific\n",
    "        month = row['Month']\n",
    "        segment_col = row['Segment_Column']\n",
    "        segment_val = row['Segment_Value']\n",
    "        \n",
    "        try:\n",
    "            # Build base filters for model\n",
    "            base_filters = [\n",
    "                f\"modelDisplayName == '{model_display_name}'\",\n",
    "                f\"modelVersionId == '{model_version_id}'\"\n",
    "            ]\n",
    "            \n",
    "            # Handle trenchCategory from config\n",
    "            if config_trench != 'ALL':\n",
    "                # If config has specific trench, filter by it\n",
    "                base_filters.append(f\"trenchCategory == '{config_trench}'\")\n",
    "            # If config_trench is 'ALL', we don't filter by trenchCategory\n",
    "            \n",
    "            # For segment-specific validation\n",
    "            if segment_col != 'Overall':\n",
    "                # Add segment filter\n",
    "                base_filters.append(f\"{segment_col} == '{segment_val}'\")\n",
    "            \n",
    "            # TRAINING DATA COUNT\n",
    "            # Training data is all data with Data_selection = 'Train'\n",
    "            train_filters = base_filters + [\"Data_selection == 'Train'\"]\n",
    "            \n",
    "            # Build query for training data\n",
    "            train_query = ' & '.join(train_filters)\n",
    "            train_data = dfcombined.query(train_query)\n",
    "            actual_train_count = train_data['digitalLoanAccountId'].nunique()\n",
    "            \n",
    "            # TEST DATA COUNT\n",
    "            # Test data is for specific month and Data_selection != 'Train'\n",
    "            test_filters = base_filters + [\n",
    "                f\"Application_month == '{month}'\",\n",
    "                \"Data_selection != 'Train'\"\n",
    "            ]\n",
    "            \n",
    "            # Build query for test data\n",
    "            test_query = ' & '.join(test_filters)\n",
    "            test_data = dfcombined.query(test_query)\n",
    "            actual_test_count = test_data['digitalLoanAccountId'].nunique()\n",
    "            \n",
    "            # Compare with PSI results\n",
    "            validation = {\n",
    "                'Row_Index': idx,\n",
    "                'Model': model_display_name,\n",
    "                'ModelVersion': model_version_id,\n",
    "                'Config_Trench': config_trench,\n",
    "                'Month': month,\n",
    "                'Segment_Column': segment_col,\n",
    "                'Segment_Value': segment_val,\n",
    "                'PSI_Base_Count': row['Base_Count'],\n",
    "                'Actual_Train_Count': actual_train_count,\n",
    "                'PSI_Actual_Count': row['Actual_Count'],\n",
    "                'Actual_Test_Count': actual_test_count,\n",
    "                'Train_Match': row['Base_Count'] == actual_train_count,\n",
    "                'Test_Match': row['Actual_Count'] == actual_test_count,\n",
    "                'Train_Difference': row['Base_Count'] - actual_train_count,\n",
    "                'Test_Difference': row['Actual_Count'] - actual_test_count\n",
    "            }\n",
    "            \n",
    "            validation_results.append(validation)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error validating row {idx}: {e}\")\n",
    "            validation = {\n",
    "                'Row_Index': idx,\n",
    "                'Model': model_display_name,\n",
    "                'ModelVersion': model_version_id,\n",
    "                'Config_Trench': config_trench,\n",
    "                'Month': month,\n",
    "                'Segment_Column': segment_col,\n",
    "                'Segment_Value': segment_val,\n",
    "                'PSI_Base_Count': row['Base_Count'],\n",
    "                'Actual_Train_Count': 'ERROR',\n",
    "                'PSI_Actual_Count': row['Actual_Count'],\n",
    "                'Actual_Test_Count': 'ERROR',\n",
    "                'Train_Match': False,\n",
    "                'Test_Match': False,\n",
    "                'Train_Difference': 'ERROR',\n",
    "                'Test_Difference': 'ERROR',\n",
    "                'Error': str(e)\n",
    "            }\n",
    "            validation_results.append(validation)\n",
    "    \n",
    "    return pd.DataFrame(validation_results)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# QUICK FIX FUNCTION FOR YOUR SPECIFIC ISSUE\n",
    "# ============================================================================\n",
    "\n",
    "def debug_counts(dfcombined: pd.DataFrame, \n",
    "                 model_display_name: str,\n",
    "                 model_version_id: str,\n",
    "                 trench_category: str,\n",
    "                 month: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Debug function to check counts for a specific combination.\n",
    "    \"\"\"\n",
    "    print(f\"\\nDebugging counts for:\")\n",
    "    print(f\"  Model: {model_display_name}\")\n",
    "    print(f\"  Version: {model_version_id}\")\n",
    "    print(f\"  Trench (config): {trench_category}\")\n",
    "    print(f\"  Month: {month}\")\n",
    "    \n",
    "    # Build filters\n",
    "    filters = [\n",
    "        f\"modelDisplayName == '{model_display_name}'\",\n",
    "        f\"modelVersionId == '{model_version_id}'\"\n",
    "    ]\n",
    "    \n",
    "    # Handle trench category\n",
    "    if trench_category != 'ALL':\n",
    "        filters.append(f\"trenchCategory == '{trench_category}'\")\n",
    "    \n",
    "    # Training data count\n",
    "    train_filters = filters + [\"Data_selection == 'Train'\"]\n",
    "    train_query = ' & '.join(train_filters)\n",
    "    train_data = dfcombined.query(train_query)\n",
    "    train_count = train_data['digitalLoanAccountId'].nunique()\n",
    "    \n",
    "    print(f\"\\nTraining data query: {train_query}\")\n",
    "    print(f\"Training count: {train_count}\")\n",
    "    \n",
    "    # Test data count for specific month\n",
    "    test_filters = filters + [\n",
    "        f\"Application_month == '{month}'\",\n",
    "        \"Data_selection != 'Train'\"\n",
    "    ]\n",
    "    test_query = ' & '.join(test_filters)\n",
    "    test_data = dfcombined.query(test_query)\n",
    "    test_count = test_data['digitalLoanAccountId'].nunique()\n",
    "    \n",
    "    print(f\"\\nTest data query: {test_query}\")\n",
    "    print(f\"Test count for {month}: {test_count}\")\n",
    "    \n",
    "    # Show sample data\n",
    "    print(f\"\\nSample training data (first 5 rows):\")\n",
    "    print(train_data[['digitalLoanAccountId', 'Application_month', 'Data_selection', 'trenchCategory']].head())\n",
    "    \n",
    "    print(f\"\\nSample test data for {month} (first 5 rows):\")\n",
    "    print(test_data[['digitalLoanAccountId', 'Application_month', 'Data_selection', 'trenchCategory']].head())\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'query_type': ['training', 'test'],\n",
    "        'query': [train_query, test_query],\n",
    "        'count': [train_count, test_count]\n",
    "    })\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # USAGE EXAMPLE WITH DEBUG\n",
    "# # ============================================================================\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Example usage with debug and validation.\n",
    "#     \"\"\"\n",
    "#     # Load your data\n",
    "#     # dfcombined = pd.read_csv('your_data.csv')\n",
    "#     # configdf = pd.read_csv('your_config.csv')\n",
    "    \n",
    "#     # Example data structure (you should replace with your actual data)\n",
    "#     dfcombined = pd.DataFrame()  # Your actual dataframe\n",
    "#     configdf = pd.DataFrame()    # Your actual config\n",
    "    \n",
    "#     # First, debug a specific case\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"DEBUGGING SPECIFIC CASE\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     debug_result = debug_counts(\n",
    "#         dfcombined=dfcombined,\n",
    "#         model_display_name='cic_model_sil',\n",
    "#         model_version_id='v2',\n",
    "#         trench_category='ALL',\n",
    "#         month='2025-11'\n",
    "#     )\n",
    "    \n",
    "#     print(f\"\\nDebug result:\\n{debug_result}\")\n",
    "    \n",
    "#     # Calculate PSI\n",
    "#     print(\"\\n\" + \"=\"*80)\n",
    "#     print(\"CALCULATING PSI\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     psi_results = calculate_psi_for_model(\n",
    "#         dfcombined=dfcombined,\n",
    "#         configdf=configdf,\n",
    "#         model_display_name='cic_model_sil',\n",
    "#         debug=True\n",
    "#     )\n",
    "    \n",
    "#     if not psi_results.empty:\n",
    "#         # Save results\n",
    "#         psi_results.to_csv('psi_results.csv', index=False)\n",
    "#         print(f\"\\nResults saved to psi_results.csv\")\n",
    "        \n",
    "#         # Validate counts\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"VALIDATING COUNTS\")\n",
    "#         print(\"=\"*80)\n",
    "        \n",
    "#         validation = validate_psi_counts(psi_results, dfcombined)\n",
    "        \n",
    "#         # Check for mismatches\n",
    "#         mismatches = validation[(validation['Train_Match'] == False) | (validation['Test_Match'] == False)]\n",
    "        \n",
    "#         if len(mismatches) > 0:\n",
    "#             print(f\"\\nFound {len(mismatches)} count mismatches:\")\n",
    "#             print(mismatches.head(20))\n",
    "            \n",
    "#             # Save mismatches to CSV for detailed analysis\n",
    "#             mismatches.to_csv('psi_validation_mismatches.csv', index=False)\n",
    "#             print(\"\\nDetailed mismatches saved to psi_validation_mismatches.csv\")\n",
    "            \n",
    "#             # Show summary of mismatches\n",
    "#             print(\"\\nSummary of mismatches:\")\n",
    "#             mismatch_summary = mismatches.groupby(['Model', 'ModelVersion', 'Config_Trench']).agg({\n",
    "#                 'Train_Match': lambda x: (x == False).sum(),\n",
    "#                 'Test_Match': lambda x: (x == False).sum(),\n",
    "#                 'Train_Difference': 'mean',\n",
    "#                 'Test_Difference': 'mean'\n",
    "#             }).reset_index()\n",
    "            \n",
    "#             print(mismatch_summary)\n",
    "#         else:\n",
    "#             print(\" All counts match!\")\n",
    "        \n",
    "#         # Summary statistics\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"PSI SUMMARY\")\n",
    "#         print(\"=\"*80)\n",
    "        \n",
    "#         print(f\"Total calculations: {len(psi_results):,}\")\n",
    "#         print(f\"Average PSI: {psi_results['PSI'].mean():.4f}\")\n",
    "#         print(f\"Maximum PSI: {psi_results['PSI'].max():.4f}\")\n",
    "        \n",
    "#         # Distribution of PSI interpretations\n",
    "#         print(f\"\\nPSI Interpretation Distribution:\")\n",
    "#         print(psi_results['PSI_Interpretation'].value_counts())\n",
    "        \n",
    "#         # Features with highest PSI\n",
    "#         high_psi = psi_results[psi_results['PSI'] > 0.2]\n",
    "#         if len(high_psi) > 0:\n",
    "#             print(f\"\\nFeatures with PSI > 0.2 (moderate or significant shift): {len(high_psi):,}\")\n",
    "#             top_features = high_psi.groupby('Feature')['PSI'].max().sort_values(ascending=False).head(10)\n",
    "#             print(\"\\nTop 10 features with highest PSI:\")\n",
    "#             for feature, psi_val in top_features.items():\n",
    "#                 print(f\"  {feature}: {psi_val:.4f}\")\n",
    "    \n",
    "#     return psi_results\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Run the analysis\n",
    "#     results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "733f7fb0",
   "metadata": {},
   "source": [
    "### PSI pipeline Version 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a07695d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Optional, Union\n",
    "import warnings\n",
    "import json\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def calculate_psi_for_model(dfcombined: pd.DataFrame,\n",
    "                            configdf: pd.DataFrame,\n",
    "                            model_display_name: str,\n",
    "                            debug: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate PSI for a specific model based on configdf combinations.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter configdf to only include combinations for this specific modelDisplayName\n",
    "    model_config = configdf[configdf['modelDisplayName'] == model_display_name].copy()\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Starting PSI Pipeline for Model: {model_display_name}\")\n",
    "    print(f\"Total combinations to process: {len(model_config)}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    if len(model_config) == 0:\n",
    "        print(f\"ERROR: No configurations found for modelDisplayName={model_display_name}\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Process each unique combination\n",
    "    all_results = []\n",
    "    \n",
    "    for idx, config_row in model_config.iterrows():\n",
    "        model_version_id = config_row['modelVersionId']\n",
    "        trench_category = config_row['trenchCategory']\n",
    "        \n",
    "        print(f\"Processing combination {idx + 1}/{len(model_config)}: \"\n",
    "              f\"modelVersionId={model_version_id}, trenchCategory={trench_category}\")\n",
    "        \n",
    "        # Filter data from dfcombined based on modelVersionId\n",
    "        combo_df = dfcombined[dfcombined['modelVersionId'] == model_version_id].copy()\n",
    "        \n",
    "        # If trenchCategory is not 'ALL', filter by it\n",
    "        if trench_category != 'ALL':\n",
    "            combo_df = combo_df[combo_df['trenchCategory'] == trench_category].copy()\n",
    "        \n",
    "        if len(combo_df) == 0:\n",
    "            print(f\"  Warning: No data found for this combination. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Data points: {len(combo_df)}\")\n",
    "        \n",
    "        # Expand calcFeatures\n",
    "        try:\n",
    "            combo_df = expand_calc_features_robust(combo_df)\n",
    "            if debug:\n",
    "                print(f\"  Features expanded successfully. Columns: {combo_df.shape[1]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"  Error expanding features: {e}. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        # Extract feature list (all columns starting with modelVersionId_Calc_)\n",
    "        feature_prefix = f\"{model_version_id}_Calc_\"\n",
    "        feature_list = [col for col in combo_df.columns if col.startswith(feature_prefix)]\n",
    "        \n",
    "        # Also include 'score' if it exists\n",
    "        if 'score' in combo_df.columns:\n",
    "            feature_list.append('score')\n",
    "        \n",
    "        if len(feature_list) == 0:\n",
    "            print(f\"  Warning: No features found after expansion. Skipping...\")\n",
    "            continue\n",
    "        \n",
    "        print(f\"  Features identified: {len(feature_list)}\")\n",
    "        \n",
    "        # Define segment columns\n",
    "        segment_columns = ['new_loan_type', 'osType', 'loan_product_type']\n",
    "        # Filter to only existing columns\n",
    "        segment_columns = [col for col in segment_columns if col in combo_df.columns]\n",
    "        \n",
    "        # Calculate PSI (overall + segments + score)\n",
    "        try:\n",
    "            psi_result = calculate_month_on_month_psi(\n",
    "                combo_df,\n",
    "                feature_list,\n",
    "                segment_columns=segment_columns\n",
    "            )\n",
    "            \n",
    "            # Add model metadata\n",
    "            psi_result['modelDisplayName'] = model_display_name\n",
    "            psi_result['modelVersionId'] = model_version_id\n",
    "            psi_result['trenchCategory'] = trench_category\n",
    "            \n",
    "            all_results.append(psi_result)\n",
    "            print(f\"  PSI calculated: {len(psi_result)} rows\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  Error calculating PSI: {e}\")\n",
    "            continue\n",
    "        \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        final_result = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "        # Reorder columns to match required output\n",
    "        column_order = ['modelDisplayName', 'modelVersionId', 'trenchCategory',\n",
    "                       'Feature', 'Feature_Type', 'Segment_Column', 'Segment_Value', 'Month',\n",
    "                       'Base_Month', 'Current_Month', 'Base_Count', 'Actual_Count',\n",
    "                       'Expected_Percentage', 'Actual_Percentage', 'PSI', 'PSI_Interpretation']\n",
    "        \n",
    "        # Keep only columns that exist\n",
    "        available_cols = [col for col in column_order if col in final_result.columns]\n",
    "        final_result = final_result[available_cols]\n",
    "        \n",
    "        # Sort results\n",
    "        final_result = final_result.sort_values(['modelVersionId', 'trenchCategory', \n",
    "                                                'Feature', 'Month', 'Segment_Column'])\n",
    "        \n",
    "        print(f\"\\n{'='*80}\")\n",
    "        print(f\"Pipeline Complete!\")\n",
    "        print(f\"Total rows in final output: {len(final_result)}\")\n",
    "        print(f\"Unique combinations processed: {len(final_result[['modelVersionId', 'trenchCategory']].drop_duplicates())}\")\n",
    "        print(f\"Unique features processed: {final_result['Feature'].nunique()}\")\n",
    "        print(f\"{'='*80}\")\n",
    "        \n",
    "        return final_result\n",
    "    else:\n",
    "        print(\"No results generated. Check input data and configurations.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def expand_calc_features_robust(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Expand the calcFeatures JSON column into separate columns.\n",
    "    \"\"\"\n",
    "    df_expanded = df.copy()\n",
    "    \n",
    "    if 'calcFeatures' not in df_expanded.columns:\n",
    "        print(\"Warning: 'calcFeatures' column not found in dataframe\")\n",
    "        return df_expanded\n",
    "    \n",
    "    # Get modelVersionId (assuming all rows have same modelVersionId in filtered dataframe)\n",
    "    if len(df_expanded) == 0:\n",
    "        return df_expanded\n",
    "    \n",
    "    model_version_id = df_expanded['modelVersionId'].iloc[0]\n",
    "    prefix = f\"{model_version_id}_Calc_\"\n",
    "    \n",
    "    all_features_data = []\n",
    "    \n",
    "    for idx, row in df_expanded.iterrows():\n",
    "        calc_features_str = row['calcFeatures']\n",
    "        row_features = {}\n",
    "        \n",
    "        if pd.isna(calc_features_str):\n",
    "            all_features_data.append(row_features)\n",
    "            continue\n",
    "            \n",
    "        try:\n",
    "            # Clean the string and parse JSON\n",
    "            features_str = str(calc_features_str)\n",
    "            # Handle common JSON issues\n",
    "            features_str = features_str.replace(\"'\", '\"').replace('None', 'null').replace('True', 'true').replace('False', 'false')\n",
    "            \n",
    "            features_dict = json.loads(features_str)\n",
    "            \n",
    "            if isinstance(features_dict, dict):\n",
    "                row_features = features_dict\n",
    "            else:\n",
    "                print(f\"Warning: calcFeatures is not a dictionary at index {idx}\")\n",
    "                \n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f\"Warning: JSON decode error at index {idx}: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error processing calcFeatures at index {idx}: {e}\")\n",
    "        \n",
    "        all_features_data.append(row_features)\n",
    "    \n",
    "    # Create DataFrame from features\n",
    "    features_df = pd.DataFrame(all_features_data)\n",
    "    \n",
    "    # Add prefix to column names\n",
    "    features_df = features_df.add_prefix(prefix)\n",
    "    \n",
    "    # Combine with original dataframe\n",
    "    df_expanded = df_expanded.reset_index(drop=True)\n",
    "    features_df = features_df.reset_index(drop=True)\n",
    "    \n",
    "    result_df = pd.concat([df_expanded, features_df], axis=1)\n",
    "    \n",
    "    # Optionally drop the original calcFeatures column\n",
    "    if 'calcFeatures' in result_df.columns:\n",
    "        result_df = result_df.drop('calcFeatures', axis=1)\n",
    "    \n",
    "    return result_df\n",
    "\n",
    "\n",
    "def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, List[str]]:\n",
    "    \"\"\"\n",
    "    Identify categorical and numerical features from the feature list.\n",
    "    \"\"\"\n",
    "    categorical_features = []\n",
    "    numerical_features = []\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        if feature not in df.columns:\n",
    "            print(f\"Warning: Feature {feature} not found in dataframe\")\n",
    "            continue\n",
    "        \n",
    "        # Check if column exists and has data\n",
    "        if df[feature].isnull().all():\n",
    "            print(f\"Warning: Feature {feature} is all null\")\n",
    "            continue\n",
    "        \n",
    "        # For numeric columns, check unique values\n",
    "        if pd.api.types.is_numeric_dtype(df[feature]):\n",
    "            unique_count = df[feature].nunique()\n",
    "            \n",
    "            # Check if it's likely categorical (few unique values and integers)\n",
    "            if unique_count <= 10:\n",
    "                sample_values = df[feature].dropna().head(100)\n",
    "                # Check if values are essentially integers\n",
    "                if all(abs(val - int(val)) < 0.0001 if not pd.isna(val) else True for val in sample_values):\n",
    "                    categorical_features.append(feature)\n",
    "                else:\n",
    "                    numerical_features.append(feature)\n",
    "            else:\n",
    "                numerical_features.append(feature)\n",
    "        else:\n",
    "            # Non-numeric columns are treated as categorical\n",
    "            categorical_features.append(feature)\n",
    "    \n",
    "    print(f\"Identified {len(numerical_features)} numerical and {len(categorical_features)} categorical features\")\n",
    "    \n",
    "    return {\n",
    "        'categorical': categorical_features,\n",
    "        'numerical': numerical_features\n",
    "    }\n",
    "\n",
    "\n",
    "def create_bins_for_features(df: pd.DataFrame,\n",
    "                             numerical_features: List[str],\n",
    "                             categorical_features: List[str],\n",
    "                             train_period_df: pd.DataFrame) -> Dict:\n",
    "    \"\"\"\n",
    "    Create bins for numerical features and categorical features based on training period.\n",
    "    \n",
    "    For numerical features: Create decile bins (10 equal-frequency bins) based on training data\n",
    "    For categorical features: Use exact categories from training data\n",
    "    \"\"\"\n",
    "    binning_info = {}\n",
    "    \n",
    "    # Process numerical features - DECILE BINNING\n",
    "    for feature in numerical_features:\n",
    "        if feature not in train_period_df.columns:\n",
    "            binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "            continue\n",
    "        \n",
    "        feature_data = train_period_df[feature].dropna()\n",
    "        \n",
    "        if len(feature_data) == 0:\n",
    "            binning_info[feature] = {'type': 'numerical', 'bins': None, 'bin_ranges': {}}\n",
    "            continue\n",
    "        \n",
    "        # CREATE DECILE BINS (10 equal-frequency bins)\n",
    "        try:\n",
    "            # Calculate decile thresholds (0th to 100th percentile in steps of 10)\n",
    "            deciles = np.percentile(feature_data, np.arange(0, 101, 10))\n",
    "            \n",
    "            # Ensure unique values for bin edges\n",
    "            unique_deciles = np.unique(deciles)\n",
    "            \n",
    "            # Handle edge case where all deciles might be the same\n",
    "            if len(unique_deciles) == 1:\n",
    "                # Single value - create symmetric bins around the value\n",
    "                unique_deciles = np.array([\n",
    "                    -np.inf, \n",
    "                    unique_deciles[0] - 0.001, \n",
    "                    unique_deciles[0], \n",
    "                    unique_deciles[0] + 0.001, \n",
    "                    np.inf\n",
    "                ])\n",
    "            \n",
    "            # Create bins with -inf and inf for the boundaries\n",
    "            bins = unique_deciles.copy()\n",
    "            bins[0] = -np.inf\n",
    "            bins[-1] = np.inf\n",
    "            \n",
    "            # Create bin labels and ranges\n",
    "            bin_ranges = {}\n",
    "            for i in range(len(bins)-1):\n",
    "                bin_name = f\"Bin_{i+1}\"\n",
    "                lower = bins[i]\n",
    "                upper = bins[i+1]\n",
    "                \n",
    "                # Create descriptive range string\n",
    "                if i == 0:  # First bin\n",
    "                    range_str = f\"(-inf, {upper:.6f})\"\n",
    "                elif i == len(bins)-2:  # Last bin\n",
    "                    range_str = f\"[{lower:.6f}, inf)\"\n",
    "                else:\n",
    "                    # For middle bins, check if it's essentially a single value\n",
    "                    if abs(upper - lower) < 1e-10:\n",
    "                        range_str = f\"[{lower:.6f}]\"\n",
    "                    else:\n",
    "                        range_str = f\"[{lower:.6f}, {upper:.6f})\"\n",
    "                \n",
    "                bin_ranges[bin_name] = {\n",
    "                    'min': lower,\n",
    "                    'max': upper,\n",
    "                    'range_str': range_str\n",
    "                }\n",
    "            \n",
    "            # Store binning information\n",
    "            binning_info[feature] = {\n",
    "                'type': 'numerical',\n",
    "                'bins': bins,\n",
    "                'bin_ranges': bin_ranges,\n",
    "                'bin_count': len(bins) - 1,\n",
    "                'decile_values': deciles.tolist()  # Store decile values for debugging\n",
    "            }\n",
    "            \n",
    "            if len(feature_data) > 0:\n",
    "                print(f\"    Created {len(bins)-1} decile bins for {feature} \"\n",
    "                      f\"(min={feature_data.min():.4f}, max={feature_data.max():.4f})\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error creating decile bins for {feature}: {e}\")\n",
    "            # Fallback to simple min-max bins\n",
    "            try:\n",
    "                min_val = feature_data.min()\n",
    "                max_val = feature_data.max()\n",
    "                \n",
    "                if min_val == max_val:\n",
    "                    bins = np.array([-np.inf, min_val - 0.001, min_val, min_val + 0.001, np.inf])\n",
    "                    bin_count = 4\n",
    "                else:\n",
    "                    # Create 10 equal-width bins as fallback\n",
    "                    bins = np.linspace(min_val, max_val, 11)\n",
    "                    bins[0] = -np.inf\n",
    "                    bins[-1] = np.inf\n",
    "                    bin_count = 10\n",
    "                \n",
    "                binning_info[feature] = {\n",
    "                    'type': 'numerical',\n",
    "                    'bins': bins,\n",
    "                    'bin_ranges': {},\n",
    "                    'bin_count': bin_count\n",
    "                }\n",
    "                print(f\"    Used fallback bins for {feature}\")\n",
    "            except:\n",
    "                binning_info[feature] = {\n",
    "                    'type': 'numerical',\n",
    "                    'bins': None,\n",
    "                    'bin_ranges': {},\n",
    "                    'bin_count': 0\n",
    "                }\n",
    "    \n",
    "    # Process categorical features - EXACT CATEGORY MATCHING\n",
    "    for feature in categorical_features:\n",
    "        if feature not in train_period_df.columns:\n",
    "            binning_info[feature] = {'type': 'categorical', 'categories': [], 'value_counts': {}}\n",
    "            continue\n",
    "        \n",
    "        # Get value counts from training data\n",
    "        train_values = train_period_df[feature].dropna()\n",
    "        value_counts = train_values.value_counts()\n",
    "        \n",
    "        # Store ALL unique categories from training data\n",
    "        all_categories = value_counts.index.tolist()\n",
    "        \n",
    "        # Also store value counts for reference\n",
    "        value_counts_dict = value_counts.to_dict()\n",
    "        \n",
    "        binning_info[feature] = {\n",
    "            'type': 'categorical',\n",
    "            'categories': all_categories,  # Store all categories from training\n",
    "            'value_counts': value_counts_dict,\n",
    "            'category_count': len(all_categories)\n",
    "        }\n",
    "        \n",
    "        print(f\"    Found {len(all_categories)} unique categories for {feature}\")\n",
    "    \n",
    "    return binning_info\n",
    "\n",
    "\n",
    "def apply_binning(df: pd.DataFrame, feature: str, binning_info: Dict) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Apply binning to a feature based on binning information.\n",
    "    \n",
    "    For numerical: Use decile bins created from training data\n",
    "    For categorical: Use exact categories from training data, mark new values in test as 'Others'\n",
    "    \"\"\"\n",
    "    if feature not in df.columns:\n",
    "        return pd.Series(['Feature_Not_Found'] * len(df), index=df.index)\n",
    "    \n",
    "    if binning_info['type'] == 'numerical':\n",
    "        if binning_info['bins'] is None or len(binning_info['bins']) < 2:\n",
    "            # Handle missing or invalid bins\n",
    "            result = pd.Series(['No_Bins'] * len(df), index=df.index)\n",
    "            result[df[feature].isna()] = 'Missing'\n",
    "            return result\n",
    "        \n",
    "        bins = binning_info['bins']\n",
    "        # Create labels for decile bins\n",
    "        labels = [f\"Bin_{i+1}\" for i in range(len(bins)-1)]\n",
    "        \n",
    "        try:\n",
    "            # Apply binning using training bins\n",
    "            binned = pd.cut(df[feature], bins=bins, labels=labels, \n",
    "                          include_lowest=True, right=False, duplicates='drop')\n",
    "            binned = binned.astype(str)\n",
    "            \n",
    "            # Handle NaN values\n",
    "            binned[df[feature].isna()] = 'Missing'\n",
    "            \n",
    "            # Handle any values that couldn't be binned\n",
    "            binned[binned.isna()] = 'Out_of_Range'\n",
    "            \n",
    "            return binned\n",
    "        except Exception as e:\n",
    "            print(f\"Error binning {feature}: {e}\")\n",
    "            return pd.Series(['Binning_Error'] * len(df), index=df.index)\n",
    "    \n",
    "    else:  # Categorical\n",
    "        train_categories = set(binning_info.get('categories', []))\n",
    "        \n",
    "        # Convert to string and handle NaN\n",
    "        feature_data = df[feature].astype(str)\n",
    "        feature_data[df[feature].isna()] = 'Missing'\n",
    "        \n",
    "        # Apply exact category matching\n",
    "        def categorize_value(x):\n",
    "            if x == 'Missing':\n",
    "                return 'Missing'\n",
    "            elif x in train_categories or str(x) in train_categories:\n",
    "                # Return exact value if it exists in training data\n",
    "                return x\n",
    "            else:\n",
    "                # Mark as 'Others' if not in training categories\n",
    "                return 'Others'\n",
    "        \n",
    "        binned = feature_data.apply(categorize_value)\n",
    "        return binned\n",
    "\n",
    "\n",
    "def calculate_psi(expected_pct: pd.Series, actual_pct: pd.Series, \n",
    "                  epsilon: float = 1e-10) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index.\n",
    "    \"\"\"\n",
    "    # Align both series on the same index\n",
    "    all_bins = set(expected_pct.index) | set(actual_pct.index)\n",
    "    \n",
    "    # Create aligned series\n",
    "    expected_aligned = pd.Series(0.0, index=list(all_bins))\n",
    "    actual_aligned = pd.Series(0.0, index=list(all_bins))\n",
    "    \n",
    "    for idx in all_bins:\n",
    "        if idx in expected_pct.index:\n",
    "            expected_aligned[idx] = expected_pct[idx]\n",
    "        if idx in actual_pct.index:\n",
    "            actual_aligned[idx] = actual_pct[idx]\n",
    "    \n",
    "    # Apply epsilon to avoid zeros\n",
    "    expected_aligned = expected_aligned.apply(lambda x: max(x, epsilon))\n",
    "    actual_aligned = actual_aligned.apply(lambda x: max(x, epsilon))\n",
    "    \n",
    "    # Normalize to sum to 1\n",
    "    expected_aligned = expected_aligned / expected_aligned.sum()\n",
    "    actual_aligned = actual_aligned / actual_aligned.sum()\n",
    "    \n",
    "    # Calculate PSI\n",
    "    psi_value = np.sum((actual_aligned - expected_aligned) * \n",
    "                      np.log(actual_aligned / expected_aligned))\n",
    "    \n",
    "    return psi_value\n",
    "\n",
    "\n",
    "def interpret_psi(psi_value: float) -> str:\n",
    "    \"\"\"\n",
    "    Interpret PSI value based on industry standards.\n",
    "    \"\"\"\n",
    "    if psi_value < 0.1:\n",
    "        return \"Very Stable\"\n",
    "    elif psi_value < 0.2:\n",
    "        return \"Stable\"\n",
    "    elif psi_value < 0.5:\n",
    "        return \"Moderate Shift\"\n",
    "    else:\n",
    "        return \"Significant Shift\"\n",
    "\n",
    "\n",
    "def calculate_month_on_month_psi(df: pd.DataFrame,\n",
    "                                 feature_list: List[str],\n",
    "                                 segment_columns: List[str] = [],\n",
    "                                 month_col: str = 'Application_month',\n",
    "                                 data_selection_col: str = 'Data_selection',\n",
    "                                 account_id_col: str = 'digitalLoanAccountId') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate PSI for each feature comparing training period vs each month,\n",
    "    with overall and segment-level breakdowns.\n",
    "    \n",
    "    Uses decile binning for numerical features and exact category matching\n",
    "    for categorical features from training to test.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Separate train and test data\n",
    "    train_mask = df[data_selection_col] == 'Train'\n",
    "    test_mask = df[data_selection_col] != 'Train'\n",
    "    \n",
    "    train_df = df[train_mask].copy()\n",
    "    test_df = df[test_mask].copy()\n",
    "    \n",
    "    if len(train_df) == 0:\n",
    "        raise ValueError(\"No training data found. Check Data_selection column.\")\n",
    "    \n",
    "    if len(test_df) == 0:\n",
    "        raise ValueError(\"No test data found. Check Data_selection column.\")\n",
    "    \n",
    "    # Identify feature types\n",
    "    feature_types = identify_feature_types(df, feature_list)\n",
    "    \n",
    "    # Create bins based on training data (DECILE BINNING for numerical, EXACT CATEGORIES for categorical)\n",
    "    print(\"  Creating decile bins from training data...\")\n",
    "    binning_info = create_bins_for_features(\n",
    "        df, \n",
    "        feature_types['numerical'], \n",
    "        feature_types['categorical'], \n",
    "        train_df\n",
    "    )\n",
    "    \n",
    "    # Get test months\n",
    "    test_months = sorted(test_df[month_col].unique())\n",
    "    \n",
    "    # Store results\n",
    "    results = []\n",
    "    \n",
    "    # Create temporary binned columns for all data (train + test)\n",
    "    # This ensures consistent binning using training bins\n",
    "    temp_columns = []\n",
    "    for feature in feature_list:\n",
    "        if feature not in binning_info:\n",
    "            continue\n",
    "            \n",
    "        binned_col = f'{feature}_binned'\n",
    "        df[binned_col] = apply_binning(df, feature, binning_info[feature])\n",
    "        temp_columns.append(binned_col)\n",
    "    \n",
    "    # 1. Calculate overall PSI (no segments)\n",
    "    print(\"  Calculating overall PSI...\")\n",
    "    for feature in feature_list:\n",
    "        if feature not in binning_info:\n",
    "            continue\n",
    "            \n",
    "        binned_col = f'{feature}_binned'\n",
    "        \n",
    "        # Get baseline distribution from ALL training data\n",
    "        train_baseline = df[train_mask][binned_col].value_counts(normalize=True)\n",
    "        \n",
    "        # Skip if baseline is empty\n",
    "        if len(train_baseline) == 0:\n",
    "            continue\n",
    "        \n",
    "        # Get total training count (across all training months)\n",
    "        total_train_count = train_df[account_id_col].nunique()\n",
    "        \n",
    "        # Calculate distribution percentages for training data\n",
    "        train_counts = df[train_mask][binned_col].value_counts()\n",
    "        train_total = train_counts.sum()\n",
    "        \n",
    "        for month in test_months:\n",
    "            month_mask = df[month_col] == month\n",
    "            test_month_mask = month_mask & test_mask\n",
    "            \n",
    "            # Skip if no test data for this month\n",
    "            if not test_month_mask.any():\n",
    "                continue\n",
    "            \n",
    "            # Get current month distribution\n",
    "            actual_counts = df[test_month_mask][binned_col].value_counts()\n",
    "            actual_total = actual_counts.sum()\n",
    "            \n",
    "            # Skip if no data for this month\n",
    "            if actual_total == 0:\n",
    "                continue\n",
    "            \n",
    "            # Calculate normalized percentages\n",
    "            actual_dist = actual_counts / actual_total\n",
    "            \n",
    "            # Calculate PSI using the distributions\n",
    "            psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "            \n",
    "            # Get distribution statistics\n",
    "            expected_avg_pct = (train_baseline * 100).mean() if len(train_baseline) > 0 else 0\n",
    "            actual_avg_pct = (actual_dist * 100).mean() if len(actual_dist) > 0 else 0\n",
    "            \n",
    "            results.append({\n",
    "                'Feature': feature,\n",
    "                'Feature_Type': binning_info[feature]['type'],\n",
    "                'Segment_Column': 'Overall',\n",
    "                'Segment_Value': 'All',\n",
    "                'Month': month,\n",
    "                'Base_Month': 'All_Training_Months',\n",
    "                'Current_Month': month,\n",
    "                'Base_Count': total_train_count,\n",
    "                'Actual_Count': df[test_month_mask][account_id_col].nunique(),\n",
    "                'Expected_Percentage': expected_avg_pct,\n",
    "                'Actual_Percentage': actual_avg_pct,\n",
    "                'PSI': psi_value,\n",
    "                'PSI_Interpretation': interpret_psi(psi_value),\n",
    "                'Bin_Count': binning_info[feature].get('bin_count', \n",
    "                          binning_info[feature].get('category_count', 0))\n",
    "            })\n",
    "    \n",
    "    # 2. Calculate segment-level PSI\n",
    "    print(\"  Calculating segment-level PSI...\")\n",
    "    for segment_col in segment_columns:\n",
    "        if segment_col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        segments = df[segment_col].dropna().unique()\n",
    "        \n",
    "        for segment_val in segments:\n",
    "            segment_mask = df[segment_col] == segment_val\n",
    "            \n",
    "            # Skip if segment has no data\n",
    "            if not segment_mask.any():\n",
    "                continue\n",
    "            \n",
    "            for feature in feature_list:\n",
    "                if feature not in binning_info:\n",
    "                    continue\n",
    "                    \n",
    "                binned_col = f'{feature}_binned'\n",
    "                \n",
    "                # Get training baseline for this segment (ALL training data for this segment)\n",
    "                train_segment_mask = train_mask & segment_mask\n",
    "                \n",
    "                if not train_segment_mask.any():\n",
    "                    continue\n",
    "                    \n",
    "                train_baseline = df[train_segment_mask][binned_col].value_counts(normalize=True)\n",
    "                \n",
    "                if len(train_baseline) == 0:\n",
    "                    continue\n",
    "                \n",
    "                # Get training count for this segment\n",
    "                segment_train_count = df[train_segment_mask][account_id_col].nunique()\n",
    "                \n",
    "                for month in test_months:\n",
    "                    month_segment_mask = (df[month_col] == month) & segment_mask & test_mask\n",
    "                    \n",
    "                    if not month_segment_mask.any():\n",
    "                        continue\n",
    "                    \n",
    "                    actual_counts = df[month_segment_mask][binned_col].value_counts()\n",
    "                    actual_total = actual_counts.sum()\n",
    "                    \n",
    "                    if actual_total == 0:\n",
    "                        continue\n",
    "                    \n",
    "                    actual_dist = actual_counts / actual_total\n",
    "                    \n",
    "                    # Calculate PSI\n",
    "                    psi_value = calculate_psi(train_baseline, actual_dist)\n",
    "                    \n",
    "                    # Get counts\n",
    "                    base_count = segment_train_count\n",
    "                    actual_count = df[month_segment_mask][account_id_col].nunique()\n",
    "                    \n",
    "                    # Get distribution statistics\n",
    "                    expected_avg_pct = (train_baseline * 100).mean() if len(train_baseline) > 0 else 0\n",
    "                    actual_avg_pct = (actual_dist * 100).mean() if len(actual_dist) > 0 else 0\n",
    "                    \n",
    "                    results.append({\n",
    "                        'Feature': feature,\n",
    "                        'Feature_Type': binning_info[feature]['type'],\n",
    "                        'Segment_Column': segment_col,\n",
    "                        'Segment_Value': str(segment_val),\n",
    "                        'Month': month,\n",
    "                        'Base_Month': 'All_Training_Months',\n",
    "                        'Current_Month': month,\n",
    "                        'Base_Count': base_count,\n",
    "                        'Actual_Count': actual_count,\n",
    "                        'Expected_Percentage': expected_avg_pct,\n",
    "                        'Actual_Percentage': actual_avg_pct,\n",
    "                        'PSI': psi_value,\n",
    "                        'PSI_Interpretation': interpret_psi(psi_value),\n",
    "                        'Bin_Count': binning_info[feature].get('bin_count', \n",
    "                                  binning_info[feature].get('category_count', 0))\n",
    "                    })\n",
    "    \n",
    "    # Clean up temporary columns\n",
    "    for col in temp_columns:\n",
    "        if col in df.columns:\n",
    "            df.drop(col, axis=1, inplace=True)\n",
    "    \n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# ADDITIONAL HELPER FUNCTIONS FOR DECILE BINNING ANALYSIS\n",
    "# ============================================================================\n",
    "\n",
    "def analyze_decile_bins(df: pd.DataFrame, feature: str, train_mask: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze decile bins for a numerical feature.\n",
    "    Returns detailed information about the decile bins.\n",
    "    \"\"\"\n",
    "    train_data = df[train_mask][feature].dropna()\n",
    "    \n",
    "    if len(train_data) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Calculate deciles\n",
    "    deciles = np.percentile(train_data, np.arange(0, 101, 10))\n",
    "    \n",
    "    # Create bins\n",
    "    bins = np.unique(deciles)\n",
    "    bins[0] = -np.inf\n",
    "    bins[-1] = np.inf\n",
    "    \n",
    "    # Calculate bin statistics\n",
    "    bin_stats = []\n",
    "    for i in range(len(bins)-1):\n",
    "        lower = bins[i]\n",
    "        upper = bins[i+1]\n",
    "        \n",
    "        if i == 0:\n",
    "            mask = train_data < upper\n",
    "        elif i == len(bins)-2:\n",
    "            mask = train_data >= lower\n",
    "        else:\n",
    "            mask = (train_data >= lower) & (train_data < upper)\n",
    "        \n",
    "        bin_data = train_data[mask]\n",
    "        \n",
    "        bin_stats.append({\n",
    "            'bin': i+1,\n",
    "            'lower_bound': lower,\n",
    "            'upper_bound': upper,\n",
    "            'count': len(bin_data),\n",
    "            'percentage': len(bin_data) / len(train_data) * 100,\n",
    "            'min_in_bin': bin_data.min() if len(bin_data) > 0 else None,\n",
    "            'max_in_bin': bin_data.max() if len(bin_data) > 0 else None,\n",
    "            'mean_in_bin': bin_data.mean() if len(bin_data) > 0 else None\n",
    "        })\n",
    "    \n",
    "    return {\n",
    "        'feature': feature,\n",
    "        'decile_values': deciles.tolist(),\n",
    "        'bins': bins.tolist(),\n",
    "        'bin_stats': bin_stats,\n",
    "        'total_count': len(train_data),\n",
    "        'min_value': train_data.min(),\n",
    "        'max_value': train_data.max(),\n",
    "        'mean_value': train_data.mean()\n",
    "    }\n",
    "\n",
    "\n",
    "def validate_categorical_matching(df: pd.DataFrame, feature: str, \n",
    "                                  train_mask: pd.Series, test_mask: pd.Series) -> Dict:\n",
    "    \"\"\"\n",
    "    Validate categorical feature matching between train and test.\n",
    "    \"\"\"\n",
    "    train_cats = set(df[train_mask][feature].dropna().unique())\n",
    "    test_cats = set(df[test_mask][feature].dropna().unique())\n",
    "    \n",
    "    categories_only_in_test = test_cats - train_cats\n",
    "    categories_only_in_train = train_cats - test_cats\n",
    "    common_categories = train_cats.intersection(test_cats)\n",
    "    \n",
    "    return {\n",
    "        'feature': feature,\n",
    "        'train_categories_count': len(train_cats),\n",
    "        'test_categories_count': len(test_cats),\n",
    "        'common_categories_count': len(common_categories),\n",
    "        'categories_only_in_test': list(categories_only_in_test),\n",
    "        'categories_only_in_train': list(categories_only_in_train),\n",
    "        'categories_only_in_test_count': len(categories_only_in_test),\n",
    "        'categories_only_in_train_count': len(categories_only_in_train)\n",
    "    }\n",
    "\n",
    "\n",
    "def debug_binning_strategy(dfcombined: pd.DataFrame, \n",
    "                          model_display_name: str,\n",
    "                          model_version_id: str,\n",
    "                          feature_name: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Debug function to analyze binning strategy for a specific feature.\n",
    "    \"\"\"\n",
    "    print(f\"\\nDebugging binning strategy for:\")\n",
    "    print(f\"  Model: {model_display_name}\")\n",
    "    print(f\"  Version: {model_version_id}\")\n",
    "    print(f\"  Feature: {feature_name}\")\n",
    "    \n",
    "    # Filter data\n",
    "    df = dfcombined[dfcombined['modelVersionId'] == model_version_id].copy()\n",
    "    df = expand_calc_features_robust(df)\n",
    "    \n",
    "    if feature_name not in df.columns:\n",
    "        print(f\"  ERROR: Feature {feature_name} not found in expanded data\")\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    # Separate train and test\n",
    "    train_mask = df['Data_selection'] == 'Train'\n",
    "    test_mask = df['Data_selection'] != 'Train'\n",
    "    \n",
    "    train_data = df[train_mask][feature_name].dropna()\n",
    "    test_data = df[test_mask][feature_name].dropna()\n",
    "    \n",
    "    print(f\"\\n  Data Summary:\")\n",
    "    print(f\"    Training samples: {len(train_data):,}\")\n",
    "    print(f\"    Test samples: {len(test_data):,}\")\n",
    "    print(f\"    Training min: {train_data.min():.4f}\")\n",
    "    print(f\"    Training max: {train_data.max():.4f}\")\n",
    "    print(f\"    Test min: {test_data.min():.4f}\")\n",
    "    print(f\"    Test max: {test_data.max():.4f}\")\n",
    "    \n",
    "    # Identify feature type\n",
    "    feature_types = identify_feature_types(df, [feature_name])\n",
    "    \n",
    "    if feature_name in feature_types['numerical']:\n",
    "        print(f\"  Feature Type: Numerical\")\n",
    "        # Analyze decile bins\n",
    "        bin_info = analyze_decile_bins(df, feature_name, train_mask)\n",
    "        \n",
    "        if bin_info:\n",
    "            print(f\"  Decile Bins Analysis:\")\n",
    "            print(f\"    Number of bins: {len(bin_info['bins'])-1}\")\n",
    "            print(f\"    Decile values: {[f'{x:.4f}' for x in bin_info['decile_values']]}\")\n",
    "            \n",
    "            print(f\"\\n    Bin Statistics:\")\n",
    "            for stat in bin_info['bin_stats']:\n",
    "                print(f\"      Bin {stat['bin']}: [{stat['lower_bound']:.4f}, {stat['upper_bound']:.4f}) - \"\n",
    "                      f\"{stat['count']} samples ({stat['percentage']:.1f}%)\")\n",
    "        \n",
    "        # Apply binning to test data\n",
    "        binning_info = create_bins_for_features(\n",
    "            df, \n",
    "            [feature_name], \n",
    "            [], \n",
    "            df[train_mask]\n",
    "        )\n",
    "        \n",
    "        if feature_name in binning_info:\n",
    "            df['binned'] = apply_binning(df, feature_name, binning_info[feature_name])\n",
    "            \n",
    "            print(f\"\\n  Test Data Binning Results:\")\n",
    "            test_binned = df[test_mask]['binned'].value_counts().sort_index()\n",
    "            for bin_name, count in test_binned.items():\n",
    "                print(f\"    {bin_name}: {count} samples\")\n",
    "            \n",
    "            # Check for out-of-range values\n",
    "            out_of_range = (df[test_mask]['binned'] == 'Out_of_Range').sum()\n",
    "            if out_of_range > 0:\n",
    "                print(f\"\\n  WARNING: {out_of_range} test samples fell outside training range!\")\n",
    "                \n",
    "    else:\n",
    "        print(f\"  Feature Type: Categorical\")\n",
    "        # Analyze categorical matching\n",
    "        matching_info = validate_categorical_matching(df, feature_name, train_mask, test_mask)\n",
    "        \n",
    "        print(f\"  Categorical Matching Analysis:\")\n",
    "        print(f\"    Training categories: {matching_info['train_categories_count']}\")\n",
    "        print(f\"    Test categories: {matching_info['test_categories_count']}\")\n",
    "        print(f\"    Common categories: {matching_info['common_categories_count']}\")\n",
    "        print(f\"    Categories only in test: {matching_info['categories_only_in_test_count']}\")\n",
    "        \n",
    "        if matching_info['categories_only_in_test_count'] > 0:\n",
    "            print(f\"\\n    Categories only in test data:\")\n",
    "            for cat in matching_info['categories_only_in_test'][:10]:  # Show first 10\n",
    "                print(f\"      - {cat}\")\n",
    "            if len(matching_info['categories_only_in_test']) > 10:\n",
    "                print(f\"      ... and {len(matching_info['categories_only_in_test']) - 10} more\")\n",
    "    \n",
    "    return pd.DataFrame([{\n",
    "        'model': model_display_name,\n",
    "        'version': model_version_id,\n",
    "        'feature': feature_name,\n",
    "        'train_samples': len(train_data),\n",
    "        'test_samples': len(test_data)\n",
    "    }])\n",
    "\n",
    "\n",
    "# ============================================================================\n",
    "# VALIDATION FUNCTION (UNCHANGED)\n",
    "# ============================================================================\n",
    "\n",
    "def validate_psi_counts(psi_results: pd.DataFrame, dfcombined: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Validate the counts in PSI results against the original data.\n",
    "    \"\"\"\n",
    "    validation_results = []\n",
    "    \n",
    "    for idx, row in psi_results.iterrows():\n",
    "        model_display_name = row['modelDisplayName']\n",
    "        model_version_id = row['modelVersionId']\n",
    "        config_trench = row['trenchCategory']  # This is from config: can be 'ALL' or specific\n",
    "        month = row['Month']\n",
    "        segment_col = row['Segment_Column']\n",
    "        segment_val = row['Segment_Value']\n",
    "        \n",
    "        try:\n",
    "            # Build base filters for model\n",
    "            base_filters = [\n",
    "                f\"modelDisplayName == '{model_display_name}'\",\n",
    "                f\"modelVersionId == '{model_version_id}'\"\n",
    "            ]\n",
    "            \n",
    "            # Handle trenchCategory from config\n",
    "            if config_trench != 'ALL':\n",
    "                # If config has specific trench, filter by it\n",
    "                base_filters.append(f\"trenchCategory == '{config_trench}'\")\n",
    "            # If config_trench is 'ALL', we don't filter by trenchCategory\n",
    "            \n",
    "            # For segment-specific validation\n",
    "            if segment_col != 'Overall':\n",
    "                # Add segment filter\n",
    "                base_filters.append(f\"{segment_col} == '{segment_val}'\")\n",
    "            \n",
    "            # TRAINING DATA COUNT\n",
    "            # Training data is all data with Data_selection = 'Train'\n",
    "            train_filters = base_filters + [\"Data_selection == 'Train'\"]\n",
    "            \n",
    "            # Build query for training data\n",
    "            train_query = ' & '.join(train_filters)\n",
    "            train_data = dfcombined.query(train_query)\n",
    "            actual_train_count = train_data['digitalLoanAccountId'].nunique()\n",
    "            \n",
    "            # TEST DATA COUNT\n",
    "            # Test data is for specific month and Data_selection != 'Train'\n",
    "            test_filters = base_filters + [\n",
    "                f\"Application_month == '{month}'\",\n",
    "                \"Data_selection != 'Train'\"\n",
    "            ]\n",
    "            \n",
    "            # Build query for test data\n",
    "            test_query = ' & '.join(test_filters)\n",
    "            test_data = dfcombined.query(test_query)\n",
    "            actual_test_count = test_data['digitalLoanAccountId'].nunique()\n",
    "            \n",
    "            # Compare with PSI results\n",
    "            validation = {\n",
    "                'Row_Index': idx,\n",
    "                'Model': model_display_name,\n",
    "                'ModelVersion': model_version_id,\n",
    "                'Config_Trench': config_trench,\n",
    "                'Month': month,\n",
    "                'Segment_Column': segment_col,\n",
    "                'Segment_Value': segment_val,\n",
    "                'PSI_Base_Count': row['Base_Count'],\n",
    "                'Actual_Train_Count': actual_train_count,\n",
    "                'PSI_Actual_Count': row['Actual_Count'],\n",
    "                'Actual_Test_Count': actual_test_count,\n",
    "                'Train_Match': row['Base_Count'] == actual_train_count,\n",
    "                'Test_Match': row['Actual_Count'] == actual_test_count,\n",
    "                'Train_Difference': row['Base_Count'] - actual_train_count,\n",
    "                'Test_Difference': row['Actual_Count'] - actual_test_count\n",
    "            }\n",
    "            \n",
    "            validation_results.append(validation)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error validating row {idx}: {e}\")\n",
    "            validation = {\n",
    "                'Row_Index': idx,\n",
    "                'Model': model_display_name,\n",
    "                'ModelVersion': model_version_id,\n",
    "                'Config_Trench': config_trench,\n",
    "                'Month': month,\n",
    "                'Segment_Column': segment_col,\n",
    "                'Segment_Value': segment_val,\n",
    "                'PSI_Base_Count': row['Base_Count'],\n",
    "                'Actual_Train_Count': 'ERROR',\n",
    "                'PSI_Actual_Count': row['Actual_Count'],\n",
    "                'Actual_Test_Count': 'ERROR',\n",
    "                'Train_Match': False,\n",
    "                'Test_Match': False,\n",
    "                'Train_Difference': 'ERROR',\n",
    "                'Test_Difference': 'ERROR',\n",
    "                'Error': str(e)\n",
    "            }\n",
    "            validation_results.append(validation)\n",
    "    \n",
    "    return pd.DataFrame(validation_results)\n",
    "\n",
    "\n",
    "# # ============================================================================\n",
    "# # MAIN EXECUTION (OPTIONAL - FOR TESTING)\n",
    "# # ============================================================================\n",
    "\n",
    "# def main():\n",
    "#     \"\"\"\n",
    "#     Example usage with decile binning.\n",
    "#     \"\"\"\n",
    "#     # Load your data\n",
    "#     # dfcombined = pd.read_csv('your_data.csv')\n",
    "#     # configdf = pd.read_csv('your_config.csv')\n",
    "    \n",
    "#     # Example data structure\n",
    "#     dfcombined = pd.DataFrame()  # Your actual dataframe\n",
    "#     configdf = pd.DataFrame()    # Your actual config\n",
    "    \n",
    "#     # Calculate PSI with decile binning\n",
    "#     print(\"=\"*80)\n",
    "#     print(\"CALCULATING PSI WITH DECILE BINNING\")\n",
    "#     print(\"=\"*80)\n",
    "    \n",
    "#     psi_results = calculate_psi_for_model(\n",
    "#         dfcombined=dfcombined,\n",
    "#         configdf=configdf,\n",
    "#         model_display_name='cic_model_sil',\n",
    "#         debug=True\n",
    "#     )\n",
    "    \n",
    "#     if not psi_results.empty:\n",
    "#         # Save results\n",
    "#         psi_results.to_csv('psi_results_decile_binning.csv', index=False)\n",
    "#         print(f\"\\nResults saved to psi_results_decile_binning.csv\")\n",
    "        \n",
    "#         # Summary of decile binning\n",
    "#         print(\"\\n\" + \"=\"*80)\n",
    "#         print(\"DECILE BINNING SUMMARY\")\n",
    "#         print(\"=\"*80)\n",
    "        \n",
    "#         numerical_features = psi_results[psi_results['Feature_Type'] == 'numerical']['Feature'].unique()\n",
    "#         categorical_features = psi_results[psi_results['Feature_Type'] == 'categorical']['Feature'].unique()\n",
    "        \n",
    "#         print(f\"Numerical features with decile binning: {len(numerical_features)}\")\n",
    "#         print(f\"Categorical features with exact matching: {len(categorical_features)}\")\n",
    "        \n",
    "#         # Show features with highest PSI\n",
    "#         high_psi = psi_results[psi_results['PSI'] > 0.2]\n",
    "#         if len(high_psi) > 0:\n",
    "#             print(f\"\\nFeatures with PSI > 0.2 (moderate or significant shift):\")\n",
    "#             top_features = high_psi.groupby(['Feature', 'Feature_Type'])['PSI'].max().sort_values(ascending=False).head(10)\n",
    "#             for (feature, ftype), psi_val in top_features.items():\n",
    "#                 print(f\"  {feature} ({ftype}): {psi_val:.4f}\")\n",
    "    \n",
    "#     return psi_results\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Run the analysis with decile binning\n",
    "#     results = main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91d8d055",
   "metadata": {},
   "source": [
    "# Config query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1104625b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelDisplayName</th>\n",
       "      <th>modelVersionId</th>\n",
       "      <th>trenchCategory</th>\n",
       "      <th>product_category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>alpha_income_model</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>beta_income_model</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>ALL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>alpha_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>alpha_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>alpha_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>alpha_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>alpha_stack_model_cash</td>\n",
       "      <td>v1.1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>alpha_stack_model_cash</td>\n",
       "      <td>v1.1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>alpha_stack_model_cash</td>\n",
       "      <td>v1.1</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>apps_score_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>apps_score_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>apps_score_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>apps_score_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>apps_score_cash</td>\n",
       "      <td>v1.1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>apps_score_cash</td>\n",
       "      <td>v1.1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>apps_score_cash</td>\n",
       "      <td>v1.1</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>beta_demo_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>beta_demo_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>beta_demo_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>beta_demo_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>beta_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>beta_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>beta_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>beta_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>beta_stack_model_cash</td>\n",
       "      <td>v1.1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>beta_stack_model_cash</td>\n",
       "      <td>v1.1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>beta_stack_model_cash</td>\n",
       "      <td>v1.1</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>cic_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>cic_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>cic_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>cic_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>gamma_demo_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>gamma_demo_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>gamma_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>gamma_stack_model_cash</td>\n",
       "      <td>v1</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>CASH</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>alpha_stack_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>alpha_stack_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>alpha_stack_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>alpha_stack_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>alpha_stack_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>apps_score_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>apps_score_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>apps_score_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>apps_score_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>apps_score_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>beta_demo_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>beta_demo_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>beta_demo_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>beta_demo_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>beta_demo_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>beta_stack_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>beta_stack_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>beta_stack_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>beta_stack_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>beta_stack_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>ALL</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>SIL</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          modelDisplayName modelVersionId trenchCategory product_category\n",
       "0       alpha_income_model             v1            ALL              ALL\n",
       "1        beta_income_model             v1            ALL              ALL\n",
       "2   alpha_stack_model_cash             v1            ALL             CASH\n",
       "3   alpha_stack_model_cash             v1       Trench 1             CASH\n",
       "4   alpha_stack_model_cash             v1       Trench 2             CASH\n",
       "5   alpha_stack_model_cash             v1       Trench 3             CASH\n",
       "6   alpha_stack_model_cash           v1.1            ALL             CASH\n",
       "7   alpha_stack_model_cash           v1.1       Trench 1             CASH\n",
       "8   alpha_stack_model_cash           v1.1       Trench 2             CASH\n",
       "9          apps_score_cash             v1            ALL             CASH\n",
       "10         apps_score_cash             v1       Trench 1             CASH\n",
       "11         apps_score_cash             v1       Trench 2             CASH\n",
       "12         apps_score_cash             v1       Trench 3             CASH\n",
       "13         apps_score_cash           v1.1            ALL             CASH\n",
       "14         apps_score_cash           v1.1       Trench 1             CASH\n",
       "15         apps_score_cash           v1.1       Trench 2             CASH\n",
       "16    beta_demo_model_cash             v1            ALL             CASH\n",
       "17    beta_demo_model_cash             v1       Trench 1             CASH\n",
       "18    beta_demo_model_cash             v1       Trench 2             CASH\n",
       "19    beta_demo_model_cash             v1       Trench 3             CASH\n",
       "20   beta_stack_model_cash             v1            ALL             CASH\n",
       "21   beta_stack_model_cash             v1       Trench 1             CASH\n",
       "22   beta_stack_model_cash             v1       Trench 2             CASH\n",
       "23   beta_stack_model_cash             v1       Trench 3             CASH\n",
       "24   beta_stack_model_cash           v1.1            ALL             CASH\n",
       "25   beta_stack_model_cash           v1.1       Trench 1             CASH\n",
       "26   beta_stack_model_cash           v1.1       Trench 2             CASH\n",
       "27          cic_model_cash             v1            ALL             CASH\n",
       "28          cic_model_cash             v1       Trench 1             CASH\n",
       "29          cic_model_cash             v1       Trench 2             CASH\n",
       "30          cic_model_cash             v1       Trench 3             CASH\n",
       "31   gamma_demo_model_cash             v1            ALL             CASH\n",
       "32   gamma_demo_model_cash             v1       Trench 1             CASH\n",
       "33  gamma_stack_model_cash             v1            ALL             CASH\n",
       "34  gamma_stack_model_cash             v1       Trench 1             CASH\n",
       "35   alpha_stack_model_sil             v1            ALL              SIL\n",
       "36   alpha_stack_model_sil             v2            ALL              SIL\n",
       "37   alpha_stack_model_sil             v2       Trench 1              SIL\n",
       "38   alpha_stack_model_sil             v2       Trench 2              SIL\n",
       "39   alpha_stack_model_sil             v2       Trench 3              SIL\n",
       "40    apps_score_model_sil             v1            ALL              SIL\n",
       "41    apps_score_model_sil             v2            ALL              SIL\n",
       "42    apps_score_model_sil             v2       Trench 1              SIL\n",
       "43    apps_score_model_sil             v2       Trench 2              SIL\n",
       "44    apps_score_model_sil             v2       Trench 3              SIL\n",
       "45     beta_demo_model_sil             v1            ALL              SIL\n",
       "46     beta_demo_model_sil             v2            ALL              SIL\n",
       "47     beta_demo_model_sil             v2       Trench 1              SIL\n",
       "48     beta_demo_model_sil             v2       Trench 2              SIL\n",
       "49     beta_demo_model_sil             v2       Trench 3              SIL\n",
       "50    beta_stack_model_sil             v1            ALL              SIL\n",
       "51    beta_stack_model_sil             v2            ALL              SIL\n",
       "52    beta_stack_model_sil             v2       Trench 1              SIL\n",
       "53    beta_stack_model_sil             v2       Trench 2              SIL\n",
       "54    beta_stack_model_sil             v2       Trench 3              SIL\n",
       "55           cic_model_sil             v1            ALL              SIL\n",
       "56           cic_model_sil             v2            ALL              SIL\n",
       "57           cic_model_sil             v2       Trench 1              SIL\n",
       "58           cic_model_sil             v2       Trench 2              SIL\n",
       "59           cic_model_sil             v2       Trench 3              SIL"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq = \"\"\" WITH base AS (\n",
    "  -- First part with actual trenchCategory\n",
    "  SELECT \n",
    "    modelDisplayName, \n",
    "    modelVersionId, \n",
    "    CASE \n",
    "      WHEN trenchCategory IS NULL THEN 'ALL'\n",
    "      WHEN trenchCategory = '' THEN 'ALL'\n",
    "      ELSE trenchCategory \n",
    "    END AS trenchCategory,\n",
    "(CASE \n",
    "      WHEN modelDisplayName LIKE '%sil%' THEN 'SIL'\n",
    "      WHEN modelDisplayName LIKE '%cash%' THEN 'CASH'\n",
    "      ELSE 'ALL' \n",
    "    END) AS product_category,\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "\n",
    "  \n",
    "  UNION ALL\n",
    "  \n",
    "  -- Second part with 'ALL' trenchCategory\n",
    "  SELECT \n",
    "    modelDisplayName, \n",
    "    modelVersionId, \n",
    "    'ALL' AS trenchCategory,\n",
    "    (CASE \n",
    "      WHEN modelDisplayName LIKE '%sil%' THEN 'SIL'\n",
    "      WHEN modelDisplayName LIKE '%cash%' THEN 'CASH'\n",
    "      ELSE 'ALL' \n",
    "    END) AS product_category,\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE trenchCategory IS NOT NULL\n",
    "\n",
    ")\n",
    "SELECT distinct\n",
    "  modelDisplayName, \n",
    "  modelVersionId, \n",
    "  trenchCategory, \n",
    "  product_category,\n",
    "FROM base\n",
    "ORDER BY 4, 1, 2, 3;\"\"\"\n",
    "\n",
    "configdf = client.query(sq).to_dataframe()\n",
    "configdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "85f3977c",
   "metadata": {},
   "outputs": [],
   "source": [
    "configdf.to_csv('configdf.csv', index = False)\n",
    "configdf.to_pickle('configdf.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af9a2909",
   "metadata": {},
   "source": [
    "## SIL"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b28e8ed2",
   "metadata": {},
   "source": [
    "### Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7665c5",
   "metadata": {},
   "source": [
    "### cic_model_sil"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a91bc986",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f6541c62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe is: (107557, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>digitalLoanAccountId</th>\n",
       "      <th>score</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>modelDisplayName</th>\n",
       "      <th>modelVersionId</th>\n",
       "      <th>new_loan_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>loan_product_type</th>\n",
       "      <th>osType</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>product</th>\n",
       "      <th>trenchCategory</th>\n",
       "      <th>calcFeatures</th>\n",
       "      <th>Data_selection</th>\n",
       "      <th>appln_submit_datetime</th>\n",
       "      <th>disbursementDateTime</th>\n",
       "      <th>Application_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2402440</td>\n",
       "      <td>b0e95c66-5ce7-4ece-aeff-4d3b28dad01c</td>\n",
       "      <td>0.11136085020144332</td>\n",
       "      <td>2025-11-07 09:10:35.213600</td>\n",
       "      <td>2025-11-07 09:10:35.219746</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL Competitor</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1762473600000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-11-07 17:10:23</td>\n",
       "      <td>2025-11-07 17:40:00</td>\n",
       "      <td>2025-11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2539836</td>\n",
       "      <td>6c6703c4-5f24-4d57-b983-d63b977f3619</td>\n",
       "      <td>0.14965652480021718</td>\n",
       "      <td>2025-05-14 05:50:15.640022</td>\n",
       "      <td>2025-05-14 05:50:15.646151</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL ZERO</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1747180800000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-05-14 13:50:07</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2610988</td>\n",
       "      <td>3cd006dd-1f7c-473b-8459-b2ec2dac4720</td>\n",
       "      <td>0.1619340616351965</td>\n",
       "      <td>2025-09-09 09:23:58.214000</td>\n",
       "      <td>2025-09-09 09:23:58.220394</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL Competitor</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1757376000000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-09-09 17:23:49</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2684028</td>\n",
       "      <td>24f1f58f-6262-480c-8db2-31177064bd0e</td>\n",
       "      <td>0.17056241682938522</td>\n",
       "      <td>2025-10-05 04:44:08.379022</td>\n",
       "      <td>2025-10-05 04:44:08.385506</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>F</td>\n",
       "      <td>Mall</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1759622400000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-10-05 12:43:57</td>\n",
       "      <td>2025-10-10 15:05:19</td>\n",
       "      <td>2025-10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2698089</td>\n",
       "      <td>5756ec8e-eb89-4b01-a595-b8feb6f5fe2f</td>\n",
       "      <td>0.11963830395919038</td>\n",
       "      <td>2025-07-27 10:32:02.703559</td>\n",
       "      <td>2025-07-27 10:32:02.709721</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"run_date\":1753574400000,\"cic_Personal_Loans_...</td>\n",
       "      <td>Test</td>\n",
       "      <td>2025-07-27 18:31:54</td>\n",
       "      <td>2025-07-27 18:35:26</td>\n",
       "      <td>2025-07</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customerId                  digitalLoanAccountId                score  \\\n",
       "0    2402440  b0e95c66-5ce7-4ece-aeff-4d3b28dad01c  0.11136085020144332   \n",
       "1    2539836  6c6703c4-5f24-4d57-b983-d63b977f3619  0.14965652480021718   \n",
       "2    2610988  3cd006dd-1f7c-473b-8459-b2ec2dac4720   0.1619340616351965   \n",
       "3    2684028  24f1f58f-6262-480c-8db2-31177064bd0e  0.17056241682938522   \n",
       "4    2698089  5756ec8e-eb89-4b01-a595-b8feb6f5fe2f  0.11963830395919038   \n",
       "\n",
       "                  start_time                   end_time modelDisplayName  \\\n",
       "0 2025-11-07 09:10:35.213600 2025-11-07 09:10:35.219746    cic_model_sil   \n",
       "1 2025-05-14 05:50:15.640022 2025-05-14 05:50:15.646151    cic_model_sil   \n",
       "2 2025-09-09 09:23:58.214000 2025-09-09 09:23:58.220394    cic_model_sil   \n",
       "3 2025-10-05 04:44:08.379022 2025-10-05 04:44:08.385506    cic_model_sil   \n",
       "4 2025-07-27 10:32:02.703559 2025-07-27 10:32:02.709721    cic_model_sil   \n",
       "\n",
       "  modelVersionId   new_loan_type gender loan_product_type   osType  \\\n",
       "0             v1  SIL Competitor      F         Appliance  android   \n",
       "1             v1        SIL ZERO      M         Appliance  android   \n",
       "2             v1  SIL Competitor      M         Appliance  android   \n",
       "3             v1     SIL-Instore      F              Mall  android   \n",
       "4             v1     SIL-Instore      M         Appliance  android   \n",
       "\n",
       "      Model_Name product trenchCategory  \\\n",
       "0  cic_model_sil     SIL            ALL   \n",
       "1  cic_model_sil     SIL            ALL   \n",
       "2  cic_model_sil     SIL            ALL   \n",
       "3  cic_model_sil     SIL            ALL   \n",
       "4  cic_model_sil     SIL            ALL   \n",
       "\n",
       "                                        calcFeatures Data_selection  \\\n",
       "0  {\"run_date\":1762473600000,\"cic_Personal_Loans_...           Test   \n",
       "1  {\"run_date\":1747180800000,\"cic_Personal_Loans_...           Test   \n",
       "2  {\"run_date\":1757376000000,\"cic_Personal_Loans_...           Test   \n",
       "3  {\"run_date\":1759622400000,\"cic_Personal_Loans_...           Test   \n",
       "4  {\"run_date\":1753574400000,\"cic_Personal_Loans_...           Test   \n",
       "\n",
       "  appln_submit_datetime disbursementDateTime Application_month  \n",
       "0   2025-11-07 17:10:23  2025-11-07 17:40:00           2025-11  \n",
       "1   2025-05-14 13:50:07                  NaT           2025-05  \n",
       "2   2025-09-09 17:23:49                  NaT           2025-09  \n",
       "3   2025-10-05 12:43:57  2025-10-10 15:05:19           2025-10  \n",
       "4   2025-07-27 18:31:54  2025-07-27 18:35:26           2025-07  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## This is for the test period of Alpha - CIC sil model - reading the data from ml_model_run_details\n",
    "\n",
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "        case when modelDisplayName = 'Alpha - CIC-SIL-Model' then 'cic_model_sil' else modelDisplayName end as modelDisplayName    \n",
    "    ,modelVersionId,\n",
    "    case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory='' then 'ALL'    \n",
    "    else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in ('Alpha - CIC-SIL-Model', 'cic_model_sil')\n",
    "  and prediction is not null\n",
    "  ),\n",
    "base as\n",
    "(SELECT distinct\n",
    "\n",
    "--Alpha_cic_sil_score\n",
    "  r.customerId,r.digitalLoanAccountId,prediction score\n",
    "    ,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "   loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "        when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "        when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "        else 'ios' end osType,\n",
    " 'cic_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Test' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId, modelVersionId \n",
    "order by coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) desc) = 1\n",
    ")\n",
    "select * from base\n",
    ";\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()\n",
    "\n",
    "## this data is not expanded. We will have to expand and get the features from the calcFeatures column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c01488af",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44754fed",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "42695c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the dataframe is: (450861, 19)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>digitalLoanAccountId</th>\n",
       "      <th>score</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>modelDisplayName</th>\n",
       "      <th>modelVersionId</th>\n",
       "      <th>new_loan_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>loan_product_type</th>\n",
       "      <th>osType</th>\n",
       "      <th>Model_Name</th>\n",
       "      <th>product</th>\n",
       "      <th>trenchCategory</th>\n",
       "      <th>calcFeatures</th>\n",
       "      <th>Data_selection</th>\n",
       "      <th>appln_submit_datetime</th>\n",
       "      <th>disbursementDateTime</th>\n",
       "      <th>Application_month</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1074411</td>\n",
       "      <td>90da4ddc-c0eb-4b46-9a39-a9e872a0cac6</td>\n",
       "      <td>0.225131</td>\n",
       "      <td>2025-12-13T11:41:46.108896</td>\n",
       "      <td>2025-12-13T11:41:46.108896</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>F</td>\n",
       "      <td>Mall</td>\n",
       "      <td>android</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>{\"ScoreRange\": \"Bi\", \"ln_loan_level_user_type\"...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2024-12-29 15:47:35</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2024-12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1779527</td>\n",
       "      <td>e5578538-61b5-44e4-bedd-94824be363d6</td>\n",
       "      <td>0.360614</td>\n",
       "      <td>2025-12-13T11:41:46.899618</td>\n",
       "      <td>2025-12-13T11:41:46.899618</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>ios</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>Trench 2</td>\n",
       "      <td>{\"ScoreRange\": \"NH_Hi\", \"ln_loan_level_user_ty...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2025-08-21 15:15:40</td>\n",
       "      <td>2025-08-23 16:44:14</td>\n",
       "      <td>2025-08</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1963733</td>\n",
       "      <td>cb444a6c-1165-4f7a-a393-72b576ab7a7c</td>\n",
       "      <td>0.156348</td>\n",
       "      <td>2025-12-13T11:19:39.915690</td>\n",
       "      <td>2025-12-13T11:19:39.915690</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>ios</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"cic_days_since_last_inquiry\": 330.0, \"cic_ve...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2023-03-27 19:55:24</td>\n",
       "      <td>2023-03-27 20:04:58</td>\n",
       "      <td>2023-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1967303</td>\n",
       "      <td>3ea0aa8c-f9b6-4898-9cb4-4d75410811da</td>\n",
       "      <td>0.143880</td>\n",
       "      <td>2025-12-13T11:19:12.930842</td>\n",
       "      <td>2025-12-13T11:19:12.930842</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>M</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"cic_days_since_last_inquiry\": 61.0, \"cic_vel...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2023-03-29 18:54:45</td>\n",
       "      <td>2023-03-29 18:58:01</td>\n",
       "      <td>2023-03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1994673</td>\n",
       "      <td>a58514f4-dacd-4ded-9744-776d00aa0904</td>\n",
       "      <td>0.124429</td>\n",
       "      <td>2025-12-13T11:19:15.036968</td>\n",
       "      <td>2025-12-13T11:19:15.036968</td>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>F</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>android</td>\n",
       "      <td>Alpha - CIC-SIL-Model</td>\n",
       "      <td>SIL</td>\n",
       "      <td>ALL</td>\n",
       "      <td>{\"cic_Personal_Loans_granted_contracts_amt_24M...</td>\n",
       "      <td>Train</td>\n",
       "      <td>2023-04-14 13:47:08</td>\n",
       "      <td>2023-04-14 17:34:58</td>\n",
       "      <td>2023-04</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   customerId                  digitalLoanAccountId     score  \\\n",
       "0     1074411  90da4ddc-c0eb-4b46-9a39-a9e872a0cac6  0.225131   \n",
       "1     1779527  e5578538-61b5-44e4-bedd-94824be363d6  0.360614   \n",
       "2     1963733  cb444a6c-1165-4f7a-a393-72b576ab7a7c  0.156348   \n",
       "3     1967303  3ea0aa8c-f9b6-4898-9cb4-4d75410811da  0.143880   \n",
       "4     1994673  a58514f4-dacd-4ded-9744-776d00aa0904  0.124429   \n",
       "\n",
       "                   start_time                    end_time modelDisplayName  \\\n",
       "0  2025-12-13T11:41:46.108896  2025-12-13T11:41:46.108896    cic_model_sil   \n",
       "1  2025-12-13T11:41:46.899618  2025-12-13T11:41:46.899618    cic_model_sil   \n",
       "2  2025-12-13T11:19:39.915690  2025-12-13T11:19:39.915690    cic_model_sil   \n",
       "3  2025-12-13T11:19:12.930842  2025-12-13T11:19:12.930842    cic_model_sil   \n",
       "4  2025-12-13T11:19:15.036968  2025-12-13T11:19:15.036968    cic_model_sil   \n",
       "\n",
       "  modelVersionId new_loan_type gender loan_product_type   osType  \\\n",
       "0             v2   SIL-Instore      F              Mall  android   \n",
       "1             v2   SIL-Instore      M         Appliance      ios   \n",
       "2             v1   SIL-Instore      M         Appliance      ios   \n",
       "3             v1   SIL-Instore      M         Appliance  android   \n",
       "4             v1   SIL-Instore      F         Appliance  android   \n",
       "\n",
       "              Model_Name product trenchCategory  \\\n",
       "0  Alpha - CIC-SIL-Model     SIL       Trench 2   \n",
       "1  Alpha - CIC-SIL-Model     SIL       Trench 2   \n",
       "2  Alpha - CIC-SIL-Model     SIL            ALL   \n",
       "3  Alpha - CIC-SIL-Model     SIL            ALL   \n",
       "4  Alpha - CIC-SIL-Model     SIL            ALL   \n",
       "\n",
       "                                        calcFeatures Data_selection  \\\n",
       "0  {\"ScoreRange\": \"Bi\", \"ln_loan_level_user_type\"...          Train   \n",
       "1  {\"ScoreRange\": \"NH_Hi\", \"ln_loan_level_user_ty...          Train   \n",
       "2  {\"cic_days_since_last_inquiry\": 330.0, \"cic_ve...          Train   \n",
       "3  {\"cic_days_since_last_inquiry\": 61.0, \"cic_vel...          Train   \n",
       "4  {\"cic_Personal_Loans_granted_contracts_amt_24M...          Train   \n",
       "\n",
       "  appln_submit_datetime disbursementDateTime Application_month  \n",
       "0   2024-12-29 15:47:35                  NaT           2024-12  \n",
       "1   2025-08-21 15:15:40  2025-08-23 16:44:14           2025-08  \n",
       "2   2023-03-27 19:55:24  2023-03-27 20:04:58           2023-03  \n",
       "3   2023-03-29 18:54:45  2023-03-29 18:58:01           2023-03  \n",
       "4   2023-04-14 13:47:08  2023-04-14 17:34:58           2023-04  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq = \"\"\"WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "    \n",
    "    case when modelDisplayName = 'Alpha - CIC-SIL-Model' then 'cic_model_sil' else modelDisplayName end as modelDisplayName\n",
    "    \n",
    "    ,modelVersionId,\n",
    "        case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory = '' then 'ALL'\n",
    "    else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in ('Alpha - CIC-SIL-Model', 'cic_model_sil')\n",
    "  ),\n",
    "base as \n",
    "(SELECT distinct\n",
    "  r.customerId,r.digitalLoanAccountId,prediction score\n",
    "    ,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "   loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "        when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "        when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "        else 'ios' end osType,\n",
    " 'Alpha - CIC-SIL-Model' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Train' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime))) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId, modelVersionId \n",
    "order by   coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) desc) = 1\n",
    ")\n",
    "select * from base\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "93fca1e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dfd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "65f0ad71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the concatenated dataframe is: (558418, 19)\n"
     ]
    }
   ],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6503e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of the concatenated dataframe is: (558418, 19)\n",
      "The shape of the dataframe after dropping duplicates is: (558418, 19)\n"
     ]
    }
   ],
   "source": [
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")\n",
    "df_combined = dropping_duplicates(df_concat)\n",
    "print(f\"The shape of the dataframe after dropping duplicates is: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9eaa482a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['score'] = pd.to_numeric(df_combined['score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "30c8ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined.sample(100).to_csv('sample_data.csv', index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72bfe0a4",
   "metadata": {},
   "source": [
    "### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "297b8c25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "Starting PSI Pipeline for Model: cic_model_sil\n",
      "Total combinations to process: 5\n",
      "================================================================================\n",
      "\n",
      "Processing combination 56/5: modelVersionId=v1, trenchCategory=ALL\n",
      "  Data points: 307884\n",
      "  Features identified: 11\n",
      "Identified 9 numerical and 2 categorical features\n",
      "  Creating decile bins from training data...\n",
      "    Created 9 decile bins for v1_Calc_cic_days_since_last_inquiry (min=0.0000, max=11649.0000)\n",
      "    Created 3 decile bins for v1_Calc_cic_cnt_active_contracts (min=1.0000, max=101.0000)\n",
      "    Created 10 decile bins for v1_Calc_cic_max_amt_granted_24M (min=0.0000, max=8000000.0000)\n",
      "    Created 10 decile bins for v1_Calc_cic_tot_active_contracts_util (min=-1.0000, max=352.8822)\n",
      "    Created 5 decile bins for v1_Calc_cic_vel_contract_granted_amt_12on24 (min=0.0000, max=29.2579)\n",
      "    Created 10 decile bins for v1_Calc_cic_Personal_Loans_granted_contracts_amt_24M (min=0.0000, max=6042680.0000)\n",
      "    Created 4 decile bins for v1_Calc_cic_vel_contract_nongranted_cnt_12on24 (min=0.1990, max=2.0120)\n",
      "    Created 10 decile bins for score (min=0.0165, max=0.6687)\n",
      "    Found 2 unique categories for v1_Calc_cic_zero_non_granted_ever_flag\n",
      "    Found 2 unique categories for v1_Calc_cic_zero_granted_ever_flag\n",
      "  Calculating overall PSI...\n",
      "  Calculating segment-level PSI...\n",
      "  PSI calculated: 847 rows\n",
      "Processing combination 57/5: modelVersionId=v2, trenchCategory=ALL\n",
      "  Data points: 250534\n",
      "  Features identified: 20\n",
      "Identified 12 numerical and 8 categorical features\n",
      "  Creating decile bins from training data...\n",
      "    Created 3 decile bins for v2_Calc_total_overdue_granted_contracts (min=0.0000, max=196.0000)\n",
      "    Created 2 decile bins for v2_Calc_cnt_nongranted_contracts_3M (min=1.0000, max=26.0000)\n",
      "    Created 10 decile bins for v2_Calc_max_amt_granted_24M (min=0.0000, max=11842000.0000)\n",
      "    Created 10 decile bins for v2_Calc_days_since_last_closed (min=-2057.0000, max=10188.0000)\n",
      "    Created 4 decile bins for v2_Calc_vel_contract_nongranted_cnt_6on12 (min=0.1291, max=2.0120)\n",
      "    Created 7 decile bins for v2_Calc_vel_contract_closed_amt_3on12 (min=0.0000, max=59.0992)\n",
      "    Created 4 decile bins for v2_Calc_granted_contracts_cnt_6M (min=1.0000, max=155.0000)\n",
      "    Created 4 decile bins for v2_Calc_cnt_active_contracts (min=1.0000, max=115.0000)\n",
      "    Created 10 decile bins for v2_Calc_tot_active_contracts_util (min=-1.0000, max=302.5533)\n",
      "    Created 7 decile bins for v2_Calc_vel_contract_granted_amt_6on12 (min=0.0000, max=30.7601)\n",
      "    Created 10 decile bins for v2_Calc_Personal_Loans_granted_contracts_amt_24M (min=0.0000, max=6594776.0000)\n",
      "    Created 10 decile bins for score (min=0.0143, max=0.7110)\n",
      "    Found 19 unique categories for v2_Calc_ScoreRange\n",
      "    Found 2 unique categories for v2_Calc_ln_loan_level_user_type\n",
      "    Found 2 unique categories for v2_Calc_flg_zero_non_granted_ever\n",
      "    Found 2 unique categories for v2_Calc_flg_zero_granted_ever\n",
      "    Found 2 unique categories for v2_Calc_has_ever_been_overdue\n",
      "    Found 0 unique categories for v2_Calc_digitalLoanAccountId\n",
      "    Found 0 unique categories for v2_Calc_customerId\n",
      "    Found 0 unique categories for v2_Calc_crifApplicationId\n",
      "  Calculating overall PSI...\n",
      "  Calculating segment-level PSI...\n",
      "  PSI calculated: 320 rows\n",
      "Processing combination 58/5: modelVersionId=v2, trenchCategory=Trench 1\n",
      "  Data points: 228294\n",
      "  Features identified: 20\n",
      "Identified 12 numerical and 8 categorical features\n",
      "  Creating decile bins from training data...\n",
      "    Created 2 decile bins for v2_Calc_total_overdue_granted_contracts (min=0.0000, max=99.0000)\n",
      "    Created 2 decile bins for v2_Calc_cnt_nongranted_contracts_3M (min=1.0000, max=26.0000)\n",
      "    Created 10 decile bins for v2_Calc_max_amt_granted_24M (min=0.0000, max=11842000.0000)\n",
      "    Created 10 decile bins for v2_Calc_days_since_last_closed (min=-2057.0000, max=10188.0000)\n",
      "    Created 4 decile bins for v2_Calc_vel_contract_nongranted_cnt_6on12 (min=0.1291, max=2.0120)\n",
      "    Created 7 decile bins for v2_Calc_vel_contract_closed_amt_3on12 (min=0.0000, max=59.0992)\n",
      "    Created 4 decile bins for v2_Calc_granted_contracts_cnt_6M (min=1.0000, max=155.0000)\n",
      "    Created 4 decile bins for v2_Calc_cnt_active_contracts (min=1.0000, max=115.0000)\n",
      "    Created 10 decile bins for v2_Calc_tot_active_contracts_util (min=-1.0000, max=302.5533)\n",
      "    Created 7 decile bins for v2_Calc_vel_contract_granted_amt_6on12 (min=0.0000, max=26.1568)\n",
      "    Created 10 decile bins for v2_Calc_Personal_Loans_granted_contracts_amt_24M (min=0.0000, max=6594776.0000)\n",
      "    Created 10 decile bins for score (min=0.0288, max=0.7110)\n",
      "    Found 19 unique categories for v2_Calc_ScoreRange\n",
      "    Found 1 unique categories for v2_Calc_ln_loan_level_user_type\n",
      "    Found 2 unique categories for v2_Calc_flg_zero_non_granted_ever\n",
      "    Found 2 unique categories for v2_Calc_flg_zero_granted_ever\n",
      "    Found 2 unique categories for v2_Calc_has_ever_been_overdue\n",
      "    Found 0 unique categories for v2_Calc_digitalLoanAccountId\n",
      "    Found 0 unique categories for v2_Calc_customerId\n",
      "    Found 0 unique categories for v2_Calc_crifApplicationId\n",
      "  Calculating overall PSI...\n",
      "  Calculating segment-level PSI...\n",
      "  PSI calculated: 320 rows\n",
      "Processing combination 59/5: modelVersionId=v2, trenchCategory=Trench 2\n",
      "  Data points: 10533\n",
      "  Features identified: 20\n",
      "Identified 11 numerical and 9 categorical features\n",
      "  Creating decile bins from training data...\n",
      "    Created 4 decile bins for v2_Calc_total_overdue_granted_contracts (min=0.0000, max=196.0000)\n",
      "    Created 10 decile bins for v2_Calc_days_since_last_closed (min=-1460.0000, max=5230.0000)\n",
      "    Created 7 decile bins for v2_Calc_vel_contract_closed_amt_3on12 (min=0.0000, max=21.4947)\n",
      "    Created 10 decile bins for v2_Calc_Personal_Loans_granted_contracts_amt_24M (min=0.0000, max=3766629.0000)\n",
      "    Created 5 decile bins for v2_Calc_cnt_active_contracts (min=1.0000, max=63.0000)\n",
      "    Created 10 decile bins for v2_Calc_max_amt_granted_24M (min=0.0000, max=3954340.0000)\n",
      "    Created 10 decile bins for v2_Calc_tot_active_contracts_util (min=-1.0000, max=60.0146)\n",
      "    Created 7 decile bins for v2_Calc_vel_contract_nongranted_cnt_6on12 (min=0.2220, max=2.0120)\n",
      "    Created 5 decile bins for v2_Calc_granted_contracts_cnt_6M (min=1.0000, max=74.0000)\n",
      "    Created 7 decile bins for v2_Calc_vel_contract_granted_amt_6on12 (min=0.0000, max=15.4717)\n",
      "    Created 10 decile bins for score (min=0.0318, max=0.6990)\n",
      "    Found 18 unique categories for v2_Calc_ScoreRange\n",
      "    Found 1 unique categories for v2_Calc_ln_loan_level_user_type\n",
      "    Found 2 unique categories for v2_Calc_flg_zero_non_granted_ever\n",
      "    Found 2 unique categories for v2_Calc_flg_zero_granted_ever\n",
      "    Found 2 unique categories for v2_Calc_has_ever_been_overdue\n",
      "    Found 8 unique categories for v2_Calc_cnt_nongranted_contracts_3M\n",
      "    Found 0 unique categories for v2_Calc_digitalLoanAccountId\n",
      "    Found 0 unique categories for v2_Calc_customerId\n",
      "    Found 0 unique categories for v2_Calc_crifApplicationId\n",
      "  Calculating overall PSI...\n",
      "  Calculating segment-level PSI...\n",
      "  PSI calculated: 320 rows\n",
      "Processing combination 60/5: modelVersionId=v2, trenchCategory=Trench 3\n",
      "  Data points: 11707\n",
      "  Features identified: 20\n",
      "Identified 11 numerical and 9 categorical features\n",
      "  Creating decile bins from training data...\n",
      "    Created 2 decile bins for v2_Calc_total_overdue_granted_contracts (min=0.0000, max=44.0000)\n",
      "    Created 3 decile bins for v2_Calc_cnt_active_contracts (min=1.0000, max=39.0000)\n",
      "    Created 10 decile bins for v2_Calc_max_amt_granted_24M (min=0.0000, max=4800317.0000)\n",
      "    Created 10 decile bins for v2_Calc_tot_active_contracts_util (min=-1.0000, max=55.2875)\n",
      "    Created 10 decile bins for v2_Calc_days_since_last_closed (min=-1220.0000, max=5621.0000)\n",
      "    Created 10 decile bins for v2_Calc_Personal_Loans_granted_contracts_amt_24M (min=0.0000, max=1907882.0000)\n",
      "    Created 3 decile bins for v2_Calc_granted_contracts_cnt_6M (min=1.0000, max=43.0000)\n",
      "    Created 6 decile bins for v2_Calc_vel_contract_granted_amt_6on12 (min=0.0000, max=30.7601)\n",
      "    Created 5 decile bins for v2_Calc_vel_contract_closed_amt_3on12 (min=0.0001, max=20.4969)\n",
      "    Created 4 decile bins for v2_Calc_vel_contract_nongranted_cnt_6on12 (min=0.2864, max=2.0120)\n",
      "    Created 10 decile bins for score (min=0.0143, max=0.5227)\n",
      "    Found 18 unique categories for v2_Calc_ScoreRange\n",
      "    Found 1 unique categories for v2_Calc_ln_loan_level_user_type\n",
      "    Found 2 unique categories for v2_Calc_flg_zero_non_granted_ever\n",
      "    Found 2 unique categories for v2_Calc_flg_zero_granted_ever\n",
      "    Found 2 unique categories for v2_Calc_has_ever_been_overdue\n",
      "    Found 7 unique categories for v2_Calc_cnt_nongranted_contracts_3M\n",
      "    Found 0 unique categories for v2_Calc_digitalLoanAccountId\n",
      "    Found 0 unique categories for v2_Calc_customerId\n",
      "    Found 0 unique categories for v2_Calc_crifApplicationId\n",
      "  Calculating overall PSI...\n",
      "  Calculating segment-level PSI...\n",
      "  PSI calculated: 320 rows\n",
      "\n",
      "================================================================================\n",
      "Pipeline Complete!\n",
      "Total rows in final output: 2127\n",
      "Unique combinations processed: 5\n",
      "Unique features processed: 30\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Calculate PSI for a specific model\n",
    "psi_results = calculate_psi_for_model(\n",
    "    dfcombined=df_combined,\n",
    "    configdf=configdf,\n",
    "    model_display_name='cic_model_sil',  # Your model name\n",
    "    debug=False  # Set to False for production\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fc903efd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelDisplayName</th>\n",
       "      <th>modelVersionId</th>\n",
       "      <th>trenchCategory</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_Type</th>\n",
       "      <th>Segment_Column</th>\n",
       "      <th>Segment_Value</th>\n",
       "      <th>Month</th>\n",
       "      <th>Base_Month</th>\n",
       "      <th>Current_Month</th>\n",
       "      <th>Base_Count</th>\n",
       "      <th>Actual_Count</th>\n",
       "      <th>Expected_Percentage</th>\n",
       "      <th>Actual_Percentage</th>\n",
       "      <th>PSI</th>\n",
       "      <th>PSI_Interpretation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>Overall</td>\n",
       "      <td>All</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>204453</td>\n",
       "      <td>180</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.149367</td>\n",
       "      <td>Stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>163711</td>\n",
       "      <td>158</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.135054</td>\n",
       "      <td>Stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Mall</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>9507</td>\n",
       "      <td>22</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.5</td>\n",
       "      <td>2.207262</td>\n",
       "      <td>Significant Shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>new_loan_type</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>185552</td>\n",
       "      <td>176</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.154039</td>\n",
       "      <td>Stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>new_loan_type</td>\n",
       "      <td>SIL ZERO</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>10397</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>17.809518</td>\n",
       "      <td>Significant Shift</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    modelDisplayName modelVersionId trenchCategory Feature Feature_Type  \\\n",
       "100    cic_model_sil             v1            ALL   score    numerical   \n",
       "727    cic_model_sil             v1            ALL   score    numerical   \n",
       "837    cic_model_sil             v1            ALL   score    numerical   \n",
       "210    cic_model_sil             v1            ALL   score    numerical   \n",
       "320    cic_model_sil             v1            ALL   score    numerical   \n",
       "\n",
       "        Segment_Column Segment_Value    Month           Base_Month  \\\n",
       "100            Overall           All  2025-03  All_Training_Months   \n",
       "727  loan_product_type     Appliance  2025-03  All_Training_Months   \n",
       "837  loan_product_type          Mall  2025-03  All_Training_Months   \n",
       "210      new_loan_type   SIL-Instore  2025-03  All_Training_Months   \n",
       "320      new_loan_type      SIL ZERO  2025-03  All_Training_Months   \n",
       "\n",
       "    Current_Month  Base_Count  Actual_Count  Expected_Percentage  \\\n",
       "100       2025-03      204453           180                 10.0   \n",
       "727       2025-03      163711           158                 10.0   \n",
       "837       2025-03        9507            22                 10.0   \n",
       "210       2025-03      185552           176                 10.0   \n",
       "320       2025-03       10397             4                 10.0   \n",
       "\n",
       "     Actual_Percentage        PSI PSI_Interpretation  \n",
       "100               10.0   0.149367             Stable  \n",
       "727               10.0   0.135054             Stable  \n",
       "837               12.5   2.207262  Significant Shift  \n",
       "210               10.0   0.154039             Stable  \n",
       "320               50.0  17.809518  Significant Shift  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3436d26",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['All_Training_Months'], dtype=object)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_results['Base_Month'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "5da990eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All counts match!\n"
     ]
    }
   ],
   "source": [
    "# After calculating PSI, validate the counts\n",
    "validation_results = validate_psi_counts(psi_results, df_combined)\n",
    "\n",
    "# Check for mismatches\n",
    "mismatches = validation_results[\n",
    "    (validation_results['Train_Match'] == False) | \n",
    "    (validation_results['Test_Match'] == False)\n",
    "]\n",
    "\n",
    "if len(mismatches) > 0:\n",
    "    print(\"Mismatches found:\")\n",
    "    print(mismatches)\n",
    "else:\n",
    "    print(\"All counts match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "46d1dadc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "psi_results['Feature'] = psi_results['Feature'].str.replace('_Calc_', '_', regex=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5a4deec0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Feature\n",
       "score                                              141\n",
       "v1_cic_vel_contract_granted_amt_12on24              77\n",
       "v1_cic_Personal_Loans_granted_contracts_amt_24M     77\n",
       "v1_cic_zero_non_granted_ever_flag                   77\n",
       "v1_cic_zero_granted_ever_flag                       77\n",
       "v1_cic_vel_contract_nongranted_cnt_12on24           77\n",
       "v1_run_date                                         77\n",
       "v1_cic_tot_active_contracts_util                    77\n",
       "v1_cic_max_amt_granted_24M                          77\n",
       "v1_cic_days_since_last_inquiry                      77\n",
       "v1_cic_cnt_active_contracts                         77\n",
       "v2_flg_zero_non_granted_ever                        64\n",
       "v2_vel_contract_granted_amt_6on12                   64\n",
       "v2_vel_contract_closed_amt_3on12                    64\n",
       "v2_total_overdue_granted_contracts                  64\n",
       "v2_tot_active_contracts_util                        64\n",
       "v2_max_amt_granted_24M                              64\n",
       "v2_ln_loan_level_user_type                          64\n",
       "v2_has_ever_been_overdue                            64\n",
       "v2_granted_contracts_cnt_6M                         64\n",
       "v2_crifApplicationId                                64\n",
       "v2_flg_zero_granted_ever                            64\n",
       "v2_digitalLoanAccountId                             64\n",
       "v2_days_since_last_closed                           64\n",
       "v2_customerId                                       64\n",
       "v2_cnt_nongranted_contracts_3M                      64\n",
       "v2_cnt_active_contracts                             64\n",
       "v2_ScoreRange                                       64\n",
       "v2_Personal_Loans_granted_contracts_amt_24M         64\n",
       "v2_vel_contract_nongranted_cnt_6on12                64\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_results['Feature'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3023594",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of features to remove\n",
    "remove_features = ['v2_customerId', 'v2_digitalLoanAccountId','v2_crifApplicationId', 'v1_run_date']\n",
    "\n",
    "# Drop rows where feature is in the list\n",
    "psi_results = psi_results[~psi_results['Feature'].isin(remove_features)]\n",
    "\n",
    "# Replace 'score' with 'Alpha_cic_sil_score' in the Feature column\n",
    "psi_results['Feature'] = psi_results['Feature'].replace('score', 'Alpha_cic_sil_score')\n",
    "\n",
    "# Replace values starting with 'calc_' by removing the prefix\n",
    "# psi_results['Feature'] = psi_results['Feature'].apply(\n",
    "#     lambda x: x[5:] if x.startswith('calc_') else x\n",
    "# )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aa872691",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "modelVersionId  Feature                                        \n",
       "v1              Alpha_cic_sil_score                                77\n",
       "                v1_cic_vel_contract_granted_amt_12on24             77\n",
       "                v1_cic_Personal_Loans_granted_contracts_amt_24M    77\n",
       "                v1_cic_zero_granted_ever_flag                      77\n",
       "                v1_cic_vel_contract_nongranted_cnt_12on24          77\n",
       "                v1_cic_zero_non_granted_ever_flag                  77\n",
       "                v1_cic_tot_active_contracts_util                   77\n",
       "                v1_cic_max_amt_granted_24M                         77\n",
       "                v1_cic_days_since_last_inquiry                     77\n",
       "                v1_cic_cnt_active_contracts                        77\n",
       "v2              v2_granted_contracts_cnt_6M                        64\n",
       "                v2_vel_contract_granted_amt_6on12                  64\n",
       "                v2_vel_contract_closed_amt_3on12                   64\n",
       "                v2_total_overdue_granted_contracts                 64\n",
       "                v2_tot_active_contracts_util                       64\n",
       "                v2_max_amt_granted_24M                             64\n",
       "                v2_ln_loan_level_user_type                         64\n",
       "                v2_has_ever_been_overdue                           64\n",
       "                v2_cnt_active_contracts                            64\n",
       "                v2_flg_zero_non_granted_ever                       64\n",
       "                v2_flg_zero_granted_ever                           64\n",
       "                v2_days_since_last_closed                          64\n",
       "                v2_cnt_nongranted_contracts_3M                     64\n",
       "                v2_ScoreRange                                      64\n",
       "                v2_Personal_Loans_granted_contracts_amt_24M        64\n",
       "                Alpha_cic_sil_score                                64\n",
       "                v2_vel_contract_nongranted_cnt_6on12               64\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_results[['modelVersionId','Feature']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "5b87afa4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>modelDisplayName</th>\n",
       "      <th>modelVersionId</th>\n",
       "      <th>trenchCategory</th>\n",
       "      <th>Feature</th>\n",
       "      <th>Feature_Type</th>\n",
       "      <th>Segment_Column</th>\n",
       "      <th>Segment_Value</th>\n",
       "      <th>Month</th>\n",
       "      <th>Base_Month</th>\n",
       "      <th>Current_Month</th>\n",
       "      <th>Base_Count</th>\n",
       "      <th>Actual_Count</th>\n",
       "      <th>Expected_Percentage</th>\n",
       "      <th>Actual_Percentage</th>\n",
       "      <th>PSI</th>\n",
       "      <th>PSI_Interpretation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>Overall</td>\n",
       "      <td>All</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>204453</td>\n",
       "      <td>180</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.149367</td>\n",
       "      <td>Stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>727</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Appliance</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>163711</td>\n",
       "      <td>158</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.135054</td>\n",
       "      <td>Stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>837</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>loan_product_type</td>\n",
       "      <td>Mall</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>9507</td>\n",
       "      <td>22</td>\n",
       "      <td>10.0</td>\n",
       "      <td>12.500000</td>\n",
       "      <td>2.207262</td>\n",
       "      <td>Significant Shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>210</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>new_loan_type</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>185552</td>\n",
       "      <td>176</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.154039</td>\n",
       "      <td>Stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>320</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v1</td>\n",
       "      <td>ALL</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>new_loan_type</td>\n",
       "      <td>SIL ZERO</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-03</td>\n",
       "      <td>10397</td>\n",
       "      <td>4</td>\n",
       "      <td>10.0</td>\n",
       "      <td>50.000000</td>\n",
       "      <td>17.809518</td>\n",
       "      <td>Significant Shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1886</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>new_loan_type</td>\n",
       "      <td>SIL-Instore</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>8439</td>\n",
       "      <td>90</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.249805</td>\n",
       "      <td>Moderate Shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>new_loan_type</td>\n",
       "      <td>SIL Competitor</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>2136</td>\n",
       "      <td>42</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.425207</td>\n",
       "      <td>Moderate Shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1966</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>new_loan_type</td>\n",
       "      <td>SIL ZERO</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>940</td>\n",
       "      <td>9</td>\n",
       "      <td>10.0</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>10.525592</td>\n",
       "      <td>Significant Shift</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2006</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>osType</td>\n",
       "      <td>android</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>10318</td>\n",
       "      <td>130</td>\n",
       "      <td>10.0</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>0.148006</td>\n",
       "      <td>Stable</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2046</th>\n",
       "      <td>cic_model_sil</td>\n",
       "      <td>v2</td>\n",
       "      <td>Trench 3</td>\n",
       "      <td>Alpha_cic_sil_score</td>\n",
       "      <td>numerical</td>\n",
       "      <td>osType</td>\n",
       "      <td>ios</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>All_Training_Months</td>\n",
       "      <td>2025-12</td>\n",
       "      <td>1197</td>\n",
       "      <td>29</td>\n",
       "      <td>10.0</td>\n",
       "      <td>11.111111</td>\n",
       "      <td>2.002576</td>\n",
       "      <td>Significant Shift</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>141 rows  16 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     modelDisplayName modelVersionId trenchCategory              Feature  \\\n",
       "100     cic_model_sil             v1            ALL  Alpha_cic_sil_score   \n",
       "727     cic_model_sil             v1            ALL  Alpha_cic_sil_score   \n",
       "837     cic_model_sil             v1            ALL  Alpha_cic_sil_score   \n",
       "210     cic_model_sil             v1            ALL  Alpha_cic_sil_score   \n",
       "320     cic_model_sil             v1            ALL  Alpha_cic_sil_score   \n",
       "...               ...            ...            ...                  ...   \n",
       "1886    cic_model_sil             v2       Trench 3  Alpha_cic_sil_score   \n",
       "1926    cic_model_sil             v2       Trench 3  Alpha_cic_sil_score   \n",
       "1966    cic_model_sil             v2       Trench 3  Alpha_cic_sil_score   \n",
       "2006    cic_model_sil             v2       Trench 3  Alpha_cic_sil_score   \n",
       "2046    cic_model_sil             v2       Trench 3  Alpha_cic_sil_score   \n",
       "\n",
       "     Feature_Type     Segment_Column   Segment_Value    Month  \\\n",
       "100     numerical            Overall             All  2025-03   \n",
       "727     numerical  loan_product_type       Appliance  2025-03   \n",
       "837     numerical  loan_product_type            Mall  2025-03   \n",
       "210     numerical      new_loan_type     SIL-Instore  2025-03   \n",
       "320     numerical      new_loan_type        SIL ZERO  2025-03   \n",
       "...           ...                ...             ...      ...   \n",
       "1886    numerical      new_loan_type     SIL-Instore  2025-12   \n",
       "1926    numerical      new_loan_type  SIL Competitor  2025-12   \n",
       "1966    numerical      new_loan_type        SIL ZERO  2025-12   \n",
       "2006    numerical             osType         android  2025-12   \n",
       "2046    numerical             osType             ios  2025-12   \n",
       "\n",
       "               Base_Month Current_Month  Base_Count  Actual_Count  \\\n",
       "100   All_Training_Months       2025-03      204453           180   \n",
       "727   All_Training_Months       2025-03      163711           158   \n",
       "837   All_Training_Months       2025-03        9507            22   \n",
       "210   All_Training_Months       2025-03      185552           176   \n",
       "320   All_Training_Months       2025-03       10397             4   \n",
       "...                   ...           ...         ...           ...   \n",
       "1886  All_Training_Months       2025-12        8439            90   \n",
       "1926  All_Training_Months       2025-12        2136            42   \n",
       "1966  All_Training_Months       2025-12         940             9   \n",
       "2006  All_Training_Months       2025-12       10318           130   \n",
       "2046  All_Training_Months       2025-12        1197            29   \n",
       "\n",
       "      Expected_Percentage  Actual_Percentage        PSI PSI_Interpretation  \n",
       "100                  10.0          10.000000   0.149367             Stable  \n",
       "727                  10.0          10.000000   0.135054             Stable  \n",
       "837                  10.0          12.500000   2.207262  Significant Shift  \n",
       "210                  10.0          10.000000   0.154039             Stable  \n",
       "320                  10.0          50.000000  17.809518  Significant Shift  \n",
       "...                   ...                ...        ...                ...  \n",
       "1886                 10.0          10.000000   0.249805     Moderate Shift  \n",
       "1926                 10.0          10.000000   0.425207     Moderate Shift  \n",
       "1966                 10.0          20.000000  10.525592  Significant Shift  \n",
       "2006                 10.0          10.000000   0.148006             Stable  \n",
       "2046                 10.0          11.111111   2.002576  Significant Shift  \n",
       "\n",
       "[141 rows x 16 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "psi_results[psi_results['Feature'] == 'Alpha_cic_sil_score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "930b5067",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LoadJob<project=prj-prod-dataplatform, location=asia-southeast1, id=91b1d8b6-99d0-4e64-abe0-eedc27df48d6>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_cic_sil_model_psi_v6\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f097f61c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9566423",
   "metadata": {},
   "source": [
    "### Alpha Sil Stack Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82e8a157",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c7678a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is for the test period of Alpha - CIC sil model - reading the data from ml_model_run_details\n",
    "# Sil_Alpha_Stack_score\n",
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "        case when modelDisplayName = 'Alpha - StackingModel' then 'alpha_stack_model_sil' else modelDisplayName end as modelDisplayName    \n",
    "    ,modelVersionId,\n",
    "    case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory='' then 'ALL'    \n",
    "    else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in ('Alpha - StackingModel', 'alpha_stack_model_sil')\n",
    "  ),\n",
    "base as\n",
    "(SELECT distinct\n",
    "  r.customerId,r.digitalLoanAccountId,prediction score\n",
    "    ,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "   loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "        when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "        when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "        else 'ios' end osType,\n",
    " 'alpha_stack_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Test' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId, modelVersionId \n",
    "order by coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) desc) = 1\n",
    ")\n",
    "select * from base\n",
    ";\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()\n",
    "\n",
    "## this data is not expanded. We will have to expand and get the features from the calcFeatures column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c93e45be",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76768305",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "066c9072",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "    \n",
    "    case when modelDisplayName = 'Alpha - StackingModel' then 'alpha_stack_model_sil' else modelDisplayName end as modelDisplayName \n",
    "     ,modelVersionId,\n",
    "        case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory = '' then 'ALL'\n",
    "    else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in ('Alpha - StackingModel', 'alpha_stack_model_sil')\n",
    "  ),\n",
    "base as \n",
    "(SELECT distinct\n",
    "  r.customerId,r.digitalLoanAccountId,prediction score\n",
    "    ,start_time,end_time,modelDisplayName,modelVersionId,\n",
    "   loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "        when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "        when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "        else 'ios' end osType,\n",
    " 'alpha_stack_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  r.calcFeature calcFeatures,\n",
    "  'Train' Data_selection,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime))) as Application_month,\n",
    "FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId, modelVersionId \n",
    "order by   coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) desc) = 1\n",
    ")\n",
    "select * from base\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10dd7d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dfd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e9f689e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58a77475",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")\n",
    "df_combined = dropping_duplicates(df_concat)\n",
    "print(f\"The shape of the dataframe after dropping duplicates is: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2101c28c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['score'] = pd.to_numeric(df_combined['score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd7d738",
   "metadata": {},
   "source": [
    "### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f40e7ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate PSI for a specific model\n",
    "psi_results = calculate_psi_for_model(\n",
    "    dfcombined=df_combined,\n",
    "    configdf=configdf,\n",
    "    model_display_name='alpha_stack_model_sil',  # Your model name\n",
    "    debug=False  # Set to False for production\n",
    ")\n",
    "psi_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108deb0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results['Feature'] = psi_results['Feature'].str.replace('_Calc_', '_', regex=False)\n",
    "psi_results['Feature'].value_counts(dropna=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6e551e",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results['Feature'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43ef1d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of features to remove, '\n",
    "remove_features = [ 'v2_trench_category',\n",
    "       'v2_ln_os_type', 'v2_is_android', 'v2_sb_demo_score_norm',\n",
    "       'v2_apps_score_norm', 'v2_s_credo_score_norm',\n",
    "       'v2_sa_cic_score_norm',\n",
    "                       ]\n",
    "# Drop rows where feature is in the list\n",
    "psi_results = psi_results[~psi_results['Feature'].isin(remove_features)]\n",
    "\n",
    "# Replace 'score' with 'Sil_Alpha_Stack_score' in the Feature column\n",
    "psi_results['Feature'] = psi_results['Feature'].replace('score', 'Sil_Alpha_Stack_score')\n",
    "\n",
    "# # Replace values starting with 'calc_' by removing the prefix\n",
    "# psi_results['Feature'] = psi_results['Feature'].apply(\n",
    "#     lambda x: x[5:] if x.startswith('calc_') else x\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc2d94d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results[['modelVersionId','Feature']].value_counts()\n",
    "# df2.rename(columns={'calc_beta_demo_score':'calc_sb_demo_score',\n",
    "#                      'calc_cic_score':'calc_s_cic_score',\n",
    "#                       'calc_apps_score':'calc_s_apps_score',\n",
    "#                         'calc_credo_gen_score':'calc_s_credo_score'}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f0c802",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.alpha_sil_stack_model_psi_v6\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3ebf1fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "037461c6",
   "metadata": {},
   "source": [
    "### Beta Sil App Score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9f0febf",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5008906",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "    case when modelDisplayName = 'Beta - AppsScoreModel' then 'apps_score_model_sil' else modelDisplayName end as modelDisplayName ,   \n",
    "    modelVersionId,\n",
    "    case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory='' then 'ALL'    \n",
    "            else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature,\n",
    "    REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in ('Beta - AppsScoreModel', 'apps_score_model_sil')\n",
    "    \n",
    "  ),\n",
    "base as \n",
    "(SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'apps_score_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  'Test' Data_selection,\n",
    "  -- sil_beta_app_score\n",
    "  safe_cast(JSON_VALUE(prediction_clean, \"$.combined_score\") AS float64) as score,\n",
    " calcFeature calcFeatures,\n",
    "    coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "    loanmaster.disbursementDateTime,\n",
    "    format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    " FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid,  modelVersionId  order by coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) desc) = 1\n",
    "  )\n",
    "select * from base where lower(new_loan_type) like '%sil%'\n",
    ";\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe is: {dfd.shape}\")\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda49ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0ea227d",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e20848fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "    case when modelDisplayName = 'Beta - AppsScoreModel' then 'apps_score_model_sil' else modelDisplayName end as modelDisplayName \n",
    "     ,modelVersionId,\n",
    "        case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory = '' then 'ALL'\n",
    "    else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature,\n",
    "    REPLACE(REPLACE(cast(prediction as string), \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in ('Beta - AppsScoreModel', 'apps_score_model_sil')\n",
    "      ),\n",
    "base as \n",
    "(SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'apps_score_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  'Train' Data_selection,\n",
    "  coalesce(prediction, safe_cast(JSON_VALUE(prediction_clean, \"$.combined_score\") AS float64)) as score,\n",
    " calcFeature calcFeatures,\n",
    "    IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "    loanmaster.disbursementDateTime,\n",
    "    format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month,\n",
    " FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    " qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId, modelVersionId \n",
    "order by   coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) desc) = 1\n",
    ")\n",
    "select * from base where lower(new_loan_type) like '%sil%'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe dfd is:\\{dfd.shape}\")\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3668980",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dfd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65c4c933",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96858ff4",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")\n",
    "df_combined = dropping_duplicates(df_concat)\n",
    "print(f\"The shape of the dataframe after dropping duplicates is: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd58e98",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['score'] = pd.to_numeric(df_combined['score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23f2a4a3",
   "metadata": {},
   "source": [
    "### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "543ee735",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example:\n",
    "# Calculate PSI for a specific model\n",
    "psi_results = calculate_psi_for_model(\n",
    "    dfcombined=df_combined,\n",
    "    configdf=configdf,\n",
    "    model_display_name='apps_score_model_sil',  # Your model name\n",
    "    debug=False  # Set to False for production\n",
    ")\n",
    "psi_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "971f0d23",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "psi_results['Feature'] = psi_results['Feature'].str.replace('_Calc_', '_', regex=False)\n",
    "psi_results[['modelVersionId','Feature']].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e109c96",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of features to remove, '\n",
    "remove_features = ['v2_appScoreModel'                       ]\n",
    "# Drop rows where feature is in the list\n",
    "psi_results = psi_results[~psi_results['Feature'].isin(remove_features)]\n",
    "\n",
    "# Replace 'score' with 'Sil_Alpha_Stack_score' in the Feature column\n",
    "psi_results['Feature'] = psi_results['Feature'].replace('score', 'sil_beta_app_score')\n",
    "\n",
    "# # Replace values starting with 'calc_' by removing the prefix\n",
    "# psi_results['Feature'] = psi_results['Feature'].apply(\n",
    "#     lambda x: x[5:] if x.startswith('calc_') else x\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50795272",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results[['modelVersionId','Feature']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320105cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_sil_appscore_model_psi_v6\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82952448",
   "metadata": {},
   "source": [
    "### Beta SIL Demo Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290d16db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bd20aa23",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78e07a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "    case when modelDisplayName = 'Beta - DemoScoreModel' then 'beta_demo_model_sil' else modelDisplayName end as modelDisplayName ,   \n",
    "    modelVersionId,\n",
    "    case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory='' then 'ALL'    \n",
    "            else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature,\n",
    "    REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in  ('Beta - DemoScoreModel', 'beta_demo_model_sil')\n",
    "  ),\n",
    "base as \n",
    "(SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'beta_demo_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  'Test' Data_selection,\n",
    "  -- sil_beta_demo_score\n",
    "  prediction as score,\n",
    " calcFeature calcFeatures,\n",
    "    coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "    loanmaster.disbursementDateTime,\n",
    "    format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    " FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid,  modelVersionId  order by coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) desc) = 1\n",
    "  )\n",
    "select * from base where lower(new_loan_type) like '%sil%'\n",
    ";\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe for test is :\\{dfd.shape}\")\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440b891a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd.query(\"\"\"select digitalLoanAccountId, count(digitalLoanAccountId)cnt from dfd group by digitalLoanAccountId having count(digitalLoanAccountId)> 1 order by 2 desc\"\"\").to_df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a0bf83",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfd[dfd['digitalLoanAccountId'] == 'c00e08a9-b103-4904-b3c2-b0be39be5af2']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6b82a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72a82f63",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86db1f7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "    case when modelDisplayName = 'Beta - DemoScoreModel' then 'beta_demo_model_sil' else modelDisplayName end as modelDisplayName    \n",
    "     ,modelVersionId,\n",
    "        case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory = '' then 'ALL'\n",
    "    else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature,\n",
    "    REPLACE(REPLACE(cast(prediction as string), \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in  ('Beta - DemoScoreModel', 'beta_demo_model_sil')\n",
    "      ),\n",
    "base as \n",
    "(SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'beta_demo_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  'Train' Data_selection,\n",
    "  prediction as score,\n",
    " calcFeature calcFeatures,\n",
    "    IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "    loanmaster.disbursementDateTime,\n",
    "    format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month,\n",
    " FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    " qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId, modelVersionId \n",
    "order by   coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) desc) = 1\n",
    ")\n",
    "select * from base where lower(new_loan_type) like '%sil%'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "print(f\"The shape of the dataframe for train is :\\{dfd.shape}\")\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e8db0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dfd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67598f48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624f6cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")\n",
    "df_combined = dropping_duplicates(df_concat)\n",
    "print(f\"The shape of the dataframe after dropping duplicates is: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0e894c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['score'] = pd.to_numeric(df_combined['score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67461603",
   "metadata": {},
   "source": [
    "### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "903b1684",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example:\n",
    "# Calculate PSI for a specific model\n",
    "psi_results = calculate_psi_for_model(\n",
    "    dfcombined=df_combined,\n",
    "    configdf=configdf,\n",
    "    model_display_name='beta_demo_model_sil',  # Your model name\n",
    "    debug=False  # Set to False for production\n",
    ")\n",
    "psi_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7271124",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After calculating PSI, validate the counts\n",
    "validation_results = validate_psi_counts(psi_results, df_combined)\n",
    "\n",
    "# Check for mismatches\n",
    "mismatches = validation_results[\n",
    "    (validation_results['Train_Match'] == False) | \n",
    "    (validation_results['Test_Match'] == False)\n",
    "]\n",
    "\n",
    "if len(mismatches) > 0:\n",
    "    print(\"Mismatches found:\")\n",
    "    print(mismatches)\n",
    "else:\n",
    "    print(\"All counts match!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee016df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "psi_results['Feature'] = psi_results['Feature'].str.replace('_Calc_', '_', regex=False)\n",
    "psi_results[['modelVersionId','Feature']].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac835b30",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of features to remove, '\n",
    "# remove_features = ['v2_appScoreModel'                       ]\n",
    "# Drop rows where feature is in the list\n",
    "psi_results = psi_results[~psi_results['Feature'].isin(remove_features)]\n",
    "\n",
    "# Replace 'score' with 'Sil_Alpha_Stack_score' in the Feature column\n",
    "psi_results['Feature'] = psi_results['Feature'].replace('score', 'sil_beta_demo_score')\n",
    "\n",
    "# # Replace values starting with 'calc_' by removing the prefix\n",
    "# psi_results['Feature'] = psi_results['Feature'].apply(\n",
    "#     lambda x: x[5:] if x.startswith('calc_') else x\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c9a47cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results[['modelVersionId','Feature']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de32e231",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_demo_score_model_psi_v6\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27d1db40",
   "metadata": {},
   "source": [
    "## Beta SIL STACK Score Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa6c7ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "48bdac2e",
   "metadata": {},
   "source": [
    "### Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5b000f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\"\n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "    case when modelDisplayName = ''Beta - StackScoreModel' then 'beta_stack_model_sil' else modelDisplayName end as modelDisplayName ,   \n",
    "    modelVersionId,\n",
    "    case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory='' then 'ALL'    \n",
    "            else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature,\n",
    "    REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "  FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "  WHERE modelDisplayName in ('Beta - StackScoreModel', 'beta_stack_model_sil')\n",
    "  ),\n",
    "base as \n",
    "(SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'beta_stack_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  'Test' Data_selection,\n",
    "  -- sil_beta_stack_score\n",
    "  prediction as score,\n",
    " calcFeature calcFeatures,\n",
    "    coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "    loanmaster.disbursementDateTime,\n",
    "    format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month,\n",
    " FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    "  qualify row_number() over(partition by r.customerId, r.digitalLoanAccountid,  modelVersionId  order by coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) desc) = 1\n",
    "  )\n",
    "select * from base\n",
    ";\n",
    "\"\"\"\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19609101",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = dfd.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55754274",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed97466",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = \"\"\" \n",
    "WITH cleaned AS (\n",
    "  SELECT\n",
    "    customerId,digitalLoanAccountId,prediction,start_time,end_time,\n",
    "    case when modelDisplayName = 'Beta - DemoScoreModel' then 'beta_demo_model_sil' else modelDisplayName end as modelDisplayName    \n",
    "     ,modelVersionId,\n",
    "        case when trenchCategory is null then 'ALL' \n",
    "         when trenchCategory = '' then 'ALL'\n",
    "    else trenchCategory end trenchCategory,\n",
    "    REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeature,\n",
    "    REPLACE(REPLACE(cast(prediction as string), \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "  FROM prj-prod-dataplatform.dap_ds_poweruser_playground.ml_training_model_run_details\n",
    "  WHERE modelDisplayName in  ('Beta - DemoScoreModel', 'beta_demo_model_sil')\n",
    "      ),\n",
    "base as \n",
    "(SELECT\n",
    "  r.customerId,r.digitalLoanAccountId,prediction,start_time,end_time,\n",
    "  modelDisplayName,modelVersionId,\n",
    "     loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "    case when loanmaster.loantype='BNPL' and sil_category.store_type =1 then 'Appliance'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =2 then 'Mobile'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type =3 then 'Mall'\n",
    "    when loanmaster.loantype='BNPL' and sil_category.store_type not in (1,2,3) then store_tagging\n",
    "    else 'not applicable' end as loan_product_type,\n",
    "     case when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%andro%' then 'android'\n",
    "    when lower(coalesce(loanmaster.osversion_v2, loanmaster.osVersion)) like '%os%' then 'ios'\n",
    "    when lower(loanmaster.deviceType) like '%andro%' then 'android'\n",
    "    else 'ios' end osType,\n",
    " 'beta_demo_model_sil' Model_Name,\n",
    " 'SIL' as product,\n",
    "  trenchCategory,\n",
    "  'Train' Data_selection,\n",
    "  prediction as score,\n",
    " calcFeature calcFeatures,\n",
    "    IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime) AS appln_submit_datetime,\n",
    "    loanmaster.disbursementDateTime,\n",
    "    format_date('%Y-%m', IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime)) as Application_month,\n",
    " FROM cleaned r\n",
    "left join risk_credit_mis.loan_master_table loanmaster\n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " left join(SELECT DISTINCT mer_refferal_code, mer_name mer_name,store_type,store_tagging FROM `dl_loans_db_raw.tdbk_merchant_refferal_mtb`\n",
    "  left join worktable_datachampions.TARGET_SPLIT P on P.STORE_NAME = mer_name\n",
    " qualify row_number() over(partition by mer_refferal_code order by  created_dt desc)=1) sil_category on loanmaster.purpleKey=sil_category.mer_refferal_code\n",
    " qualify row_number() over (partition by r.customerId,r.digitalLoanAccountId, modelVersionId \n",
    "order by   coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  cast(r.start_time as datetime)) desc) = 1\n",
    ")\n",
    "select * from base\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "dfd = client.query(sq).to_dataframe()\n",
    "dfd.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd59ac4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = dfd.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc3f67f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_concat = pd.concat([df1, df2], ignore_index=True)\n",
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20033b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"The shape of the concatenated dataframe is: {df_concat.shape}\")\n",
    "df_combined = dropping_duplicates(df_concat)\n",
    "print(f\"The shape of the dataframe after dropping duplicates is: {df_combined.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "504dd56f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_combined['score'] = pd.to_numeric(df_combined['score'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af4d747f",
   "metadata": {},
   "source": [
    "### PSI calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bacb1b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage Example:\n",
    "psi_results = calculate_psi_for_model(\n",
    "    dfcombined=df_combined,\n",
    "    configdf=configdf,\n",
    "    model_display_name='beta_demo_model_sil'\n",
    ")\n",
    "psi_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a05bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "psi_results['Feature'] = psi_results['Feature'].str.replace('_Calc_', '_', regex=False)\n",
    "psi_results[['modelVersionId','Feature']].value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23cffd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# List of features to remove, '\n",
    "remove_features = ['v2_appScoreModel'                       ]\n",
    "# Drop rows where feature is in the list\n",
    "psi_results = psi_results[~psi_results['Feature'].isin(remove_features)]\n",
    "\n",
    "# Replace 'score' with 'Sil_Alpha_Stack_score' in the Feature column\n",
    "psi_results['Feature'] = psi_results['Feature'].replace('score', 'sil_beta_demo_score')\n",
    "\n",
    "# Replace values starting with 'calc_' by removing the prefix\n",
    "psi_results['Feature'] = psi_results['Feature'].apply(\n",
    "    lambda x: x[5:] if x.startswith('calc_') else x\n",
    ")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94997da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results[['modelVersionId','Feature']].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a25f5a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.beta_demo_score_model_psi_v5\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2d61d3c",
   "metadata": {},
   "source": [
    "# End"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
