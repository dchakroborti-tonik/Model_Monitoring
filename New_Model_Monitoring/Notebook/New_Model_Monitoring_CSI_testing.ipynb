{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6ff0b5a",
   "metadata": {},
   "source": [
    "# Define Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "849f5787",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "import gcsfs\n",
    "import duckdb as dd\n",
    "import pickle\n",
    "import joblib\n",
    "from typing import Union\n",
    "import io\n",
    "\n",
    "path = r'C:\\Users\\Dwaipayan\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"prj-prod-dataplatform\"\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"Display.max_rows\", 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf37b3ea",
   "metadata": {},
   "source": [
    "# Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4804809",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict, Tuple, Union\n",
    "\n",
    "def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Identify whether features are categorical or numerical\n",
    "    \"\"\"\n",
    "    feature_types = {}\n",
    "    \n",
    "    for feature in feature_list:\n",
    "        # Check if feature exists in dataframe\n",
    "        if feature not in df.columns:\n",
    "            feature_types[feature] = 'unknown'\n",
    "            continue\n",
    "            \n",
    "        # Check data type and unique values\n",
    "        unique_count = df[feature].nunique()\n",
    "        \n",
    "        # For features with specific patterns in name or known categoricals\n",
    "        if (feature in ['cic_ScoreRange', 'cic_ln_loan_level_user_type'] or \n",
    "            any(keyword in feature.lower() for keyword in ['flag', 'flg', 'type', 'category', 'range'])):\n",
    "            feature_types[feature] = 'categorical'\n",
    "        # For features that are clearly numerical\n",
    "        elif ('ratio' in feature.lower() or 'amt' in feature.lower() or \n",
    "              'cnt' in feature.lower() or 'age' in feature.lower() or \n",
    "              'limit' in feature.lower() or 'max' in feature.lower()):\n",
    "            feature_types[feature] = 'numerical'\n",
    "        # Fallback based on data characteristics\n",
    "        elif unique_count <= 20 or df[feature].dtype == 'object':\n",
    "            feature_types[feature] = 'categorical'\n",
    "        else:\n",
    "            feature_types[feature] = 'numerical'\n",
    "            \n",
    "    return feature_types\n",
    "\n",
    "def create_bins_numerical(df: pd.DataFrame, feature: str, num_bins: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create bins for numerical features using quantiles with robust error handling\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    if df_copy[feature].isna().any():\n",
    "        df_copy[f'{feature}_bin'] = 'Missing'\n",
    "        non_missing = df_copy[feature].notna()\n",
    "    else:\n",
    "        non_missing = pd.Series([True] * len(df_copy))\n",
    "    \n",
    "    if non_missing.any():\n",
    "        feature_data = df_copy.loc[non_missing, feature]\n",
    "        \n",
    "        # Check if we have enough data for binning\n",
    "        if len(feature_data) < 2:\n",
    "            df_copy.loc[non_missing, f'{feature}_bin'] = 'All'\n",
    "            return df_copy\n",
    "            \n",
    "        # Check if all values are the same\n",
    "        if feature_data.nunique() == 1:\n",
    "            df_copy.loc[non_missing, f'{feature}_bin'] = 'All'\n",
    "            return df_copy\n",
    "            \n",
    "        try:\n",
    "            # Use quantile-based binning with error handling\n",
    "            bins = pd.qcut(feature_data, q=min(num_bins, len(feature_data)), \n",
    "                          duplicates='drop', labels=False)\n",
    "            df_copy.loc[non_missing, f'{feature}_bin'] = bins.astype(str)\n",
    "        except (ValueError, TypeError) as e:\n",
    "            try:\n",
    "                # Fallback to equal-width binning\n",
    "                bins = pd.cut(feature_data, bins=min(num_bins, len(feature_data)), \n",
    "                            labels=False)\n",
    "                df_copy.loc[non_missing, f'{feature}_bin'] = bins.astype(str)\n",
    "            except:\n",
    "                # Final fallback - use value ranges\n",
    "                unique_vals = feature_data.unique()\n",
    "                if len(unique_vals) <= 10:\n",
    "                    df_copy.loc[non_missing, f'{feature}_bin'] = feature_data.astype(str)\n",
    "                else:\n",
    "                    df_copy.loc[non_missing, f'{feature}_bin'] = 'All'\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def create_bins_categorical(df: pd.DataFrame, feature: str, top_n: int = 6) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create bins for categorical features - top N categories and 'Others'\n",
    "    \"\"\"\n",
    "    df_copy = df.copy()\n",
    "    \n",
    "    # Handle missing values\n",
    "    if df_copy[feature].isna().any():\n",
    "        df_copy[f'{feature}_bin'] = 'Missing'\n",
    "        non_missing = df_copy[feature].notna()\n",
    "    else:\n",
    "        non_missing = pd.Series([True] * len(df_copy))\n",
    "    \n",
    "    if non_missing.any():\n",
    "        feature_data = df_copy.loc[non_missing, feature]\n",
    "        \n",
    "        # If no data or only one value\n",
    "        if len(feature_data) == 0:\n",
    "            df_copy.loc[non_missing, f'{feature}_bin'] = 'No Data'\n",
    "            return df_copy\n",
    "            \n",
    "        # Get value counts\n",
    "        value_counts = feature_data.value_counts()\n",
    "        \n",
    "        if len(value_counts) == 0:\n",
    "            df_copy.loc[non_missing, f'{feature}_bin'] = 'No Data'\n",
    "        elif len(value_counts) <= top_n:\n",
    "            # If fewer categories than top_n, use all\n",
    "            df_copy.loc[non_missing, f'{feature}_bin'] = feature_data.astype(str)\n",
    "        else:\n",
    "            # Keep top N categories, group rest as 'Others'\n",
    "            top_categories = value_counts.head(top_n).index\n",
    "            df_copy.loc[non_missing, f'{feature}_bin'] = feature_data.apply(\n",
    "                lambda x: str(x) if x in top_categories else 'Others'\n",
    "            )\n",
    "    \n",
    "    return df_copy\n",
    "\n",
    "def calculate_psi(expected: pd.Series, actual: pd.Series) -> float:\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index between expected and actual distributions\n",
    "    \"\"\"\n",
    "    # Handle empty series\n",
    "    if len(expected) == 0 or len(actual) == 0:\n",
    "        return np.nan\n",
    "        \n",
    "    # Combine all possible categories\n",
    "    all_categories = set(expected.unique()) | set(actual.unique())\n",
    "    \n",
    "    psi = 0\n",
    "    for category in all_categories:\n",
    "        exp_count = (expected == category).sum()\n",
    "        act_count = (actual == category).sum()\n",
    "        \n",
    "        exp_perc = exp_count / len(expected) if len(expected) > 0 else 0\n",
    "        act_perc = act_count / len(actual) if len(actual) > 0 else 0\n",
    "        \n",
    "        # Add small epsilon to avoid division by zero and log(0)\n",
    "        eps = 1e-6\n",
    "        exp_perc_adj = exp_perc + eps\n",
    "        act_perc_adj = act_perc + eps\n",
    "        \n",
    "        if exp_perc_adj > 0 and act_perc_adj > 0:\n",
    "            psi_component = (act_perc_adj - exp_perc_adj) * np.log(act_perc_adj / exp_perc_adj)\n",
    "            psi += psi_component\n",
    "    \n",
    "    return psi\n",
    "\n",
    "def calculate_feature_psi(df: pd.DataFrame, feature: str, feature_type: str, \n",
    "                         month_column: str, base_month: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate PSI for a single feature across months\n",
    "    \"\"\"\n",
    "    if base_month is None:\n",
    "        base_month = df[month_column].min()\n",
    "    \n",
    "    # Check if we have data for base month\n",
    "    base_data = df[df[month_column] == base_month]\n",
    "    if len(base_data) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Create bins based on feature type\n",
    "        if feature_type == 'numerical':\n",
    "            df_binned = create_bins_numerical(df, feature)\n",
    "        else:  # categorical\n",
    "            df_binned = create_bins_categorical(df, feature)\n",
    "        \n",
    "        # Get base distribution\n",
    "        base_data = df_binned[df_binned[month_column] == base_month]\n",
    "        expected_dist = base_data[f'{feature}_bin']\n",
    "        \n",
    "        # Calculate PSI for each month\n",
    "        results = []\n",
    "        months = sorted(df_binned[month_column].unique())\n",
    "        \n",
    "        for month in months:\n",
    "            if month == base_month:\n",
    "                psi = 0.0  # PSI with itself is 0\n",
    "            else:\n",
    "                current_data = df_binned[df_binned[month_column] == month]\n",
    "                if len(current_data) == 0:\n",
    "                    psi = np.nan\n",
    "                else:\n",
    "                    actual_dist = current_data[f'{feature}_bin']\n",
    "                    psi = calculate_psi(expected_dist, actual_dist)\n",
    "            \n",
    "            month_data = df_binned[df_binned[month_column] == month]\n",
    "            results.append({\n",
    "                'month': month,\n",
    "                'feature': feature,\n",
    "                'psi': psi,\n",
    "                'base_month': base_month,\n",
    "                'sample_size': len(month_data)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating PSI for feature {feature}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_segment_psi(df: pd.DataFrame, feature: str, feature_type: str,\n",
    "                        segment_columns: List[str], month_column: str, \n",
    "                        base_month: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate PSI for each segment\n",
    "    \"\"\"\n",
    "    if base_month is None:\n",
    "        base_month = df[month_column].min()\n",
    "    \n",
    "    all_results = []\n",
    "    \n",
    "    # Overall PSI (no segmentation)\n",
    "    overall_psi = calculate_feature_psi(df, feature, feature_type, month_column, base_month)\n",
    "    if not overall_psi.empty:\n",
    "        overall_psi['segment_name'] = 'overall'\n",
    "        overall_psi['segment_value'] = 'overall'\n",
    "        all_results.append(overall_psi)\n",
    "    \n",
    "    # PSI for each segment\n",
    "    for segment_col in segment_columns:\n",
    "        if segment_col not in df.columns:\n",
    "            continue\n",
    "            \n",
    "        for segment_value in df[segment_col].dropna().unique():\n",
    "            segment_data = df[df[segment_col] == segment_value]\n",
    "            if len(segment_data) == 0:\n",
    "                continue\n",
    "                \n",
    "            segment_psi = calculate_feature_psi(segment_data, feature, feature_type, month_column, base_month)\n",
    "            if not segment_psi.empty:\n",
    "                segment_psi['segment_name'] = segment_col\n",
    "                segment_psi['segment_value'] = str(segment_value)\n",
    "                all_results.append(segment_psi)\n",
    "    \n",
    "    # Combine all results\n",
    "    if all_results:\n",
    "        return pd.concat(all_results, ignore_index=True)\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def calculate_bin_level_psi(df: pd.DataFrame, feature: str, feature_type: str,\n",
    "                          segment_columns: List[str], month_column: str,\n",
    "                          base_month: str = None) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate bin-level PSI contributions\n",
    "    \"\"\"\n",
    "    if base_month is None:\n",
    "        base_month = df[month_column].min()\n",
    "    \n",
    "    # Check if we have base data\n",
    "    base_data = df[df[month_column] == base_month]\n",
    "    if len(base_data) == 0:\n",
    "        return pd.DataFrame()\n",
    "    \n",
    "    try:\n",
    "        # Create bins\n",
    "        if feature_type == 'numerical':\n",
    "            df_binned = create_bins_numerical(df, feature)\n",
    "        else:\n",
    "            df_binned = create_bins_categorical(df, feature)\n",
    "        \n",
    "        bin_results = []\n",
    "        \n",
    "        # Helper function to calculate bin distributions\n",
    "        def get_bin_distribution(data, bin_col):\n",
    "            if len(data) == 0:\n",
    "                return {}\n",
    "            dist = data[bin_col].value_counts(normalize=True).to_dict()\n",
    "            return {str(k): v for k, v in dist.items()}\n",
    "        \n",
    "        # Base distribution (overall)\n",
    "        base_data = df_binned[df_binned[month_column] == base_month]\n",
    "        base_dist_overall = get_bin_distribution(base_data, f'{feature}_bin')\n",
    "        \n",
    "        # Overall bin-level PSI\n",
    "        months = sorted(df_binned[month_column].unique())\n",
    "        for month in months:\n",
    "            if month == base_month:\n",
    "                continue\n",
    "                \n",
    "            current_data = df_binned[df_binned[month_column] == month]\n",
    "            current_dist = get_bin_distribution(current_data, f'{feature}_bin')\n",
    "            \n",
    "            # Calculate PSI contribution for each bin\n",
    "            all_bins = set(base_dist_overall.keys()) | set(current_dist.keys())\n",
    "            for bin_name in all_bins:\n",
    "                exp_perc = base_dist_overall.get(bin_name, 1e-6)\n",
    "                act_perc = current_dist.get(bin_name, 1e-6)\n",
    "                \n",
    "                # Add epsilon to avoid log(0)\n",
    "                eps = 1e-6\n",
    "                exp_perc_adj = exp_perc + eps\n",
    "                act_perc_adj = act_perc + eps\n",
    "                \n",
    "                psi_contribution = (act_perc_adj - exp_perc_adj) * np.log(act_perc_adj / exp_perc_adj)\n",
    "                \n",
    "                bin_results.append({\n",
    "                    'month': month,\n",
    "                    'segment_name': 'overall',\n",
    "                    'segment_value': 'overall',\n",
    "                    'feature': feature,\n",
    "                    'bin': bin_name,\n",
    "                    'expected_percentage': exp_perc,\n",
    "                    'actual_percentage': act_perc,\n",
    "                    'psi_contribution': psi_contribution,\n",
    "                    'base_month': base_month\n",
    "                })\n",
    "        \n",
    "        # Segment-level bin PSI\n",
    "        for segment_col in segment_columns:\n",
    "            if segment_col not in df_binned.columns:\n",
    "                continue\n",
    "                \n",
    "            for segment_value in df_binned[segment_col].dropna().unique():\n",
    "                segment_base_data = base_data[base_data[segment_col] == segment_value]\n",
    "                if len(segment_base_data) == 0:\n",
    "                    continue\n",
    "                    \n",
    "                segment_base_dist = get_bin_distribution(segment_base_data, f'{feature}_bin')\n",
    "                \n",
    "                for month in months:\n",
    "                    if month == base_month:\n",
    "                        continue\n",
    "                        \n",
    "                    segment_current_data = df_binned[\n",
    "                        (df_binned[month_column] == month) & \n",
    "                        (df_binned[segment_col] == segment_value)\n",
    "                    ]\n",
    "                    if len(segment_current_data) == 0:\n",
    "                        continue\n",
    "                        \n",
    "                    segment_current_dist = get_bin_distribution(segment_current_data, f'{feature}_bin')\n",
    "                    \n",
    "                    all_bins = set(segment_base_dist.keys()) | set(segment_current_dist.keys())\n",
    "                    for bin_name in all_bins:\n",
    "                        exp_perc = segment_base_dist.get(bin_name, 1e-6)\n",
    "                        act_perc = segment_current_dist.get(bin_name, 1e-6)\n",
    "                        \n",
    "                        eps = 1e-6\n",
    "                        exp_perc_adj = exp_perc + eps\n",
    "                        act_perc_adj = act_perc + eps\n",
    "                        \n",
    "                        psi_contribution = (act_perc_adj - exp_perc_adj) * np.log(act_perc_adj / exp_perc_adj)\n",
    "                        \n",
    "                        bin_results.append({\n",
    "                            'month': month,\n",
    "                            'segment_name': segment_col,\n",
    "                            'segment_value': str(segment_value),\n",
    "                            'feature': feature,\n",
    "                            'bin': bin_name,\n",
    "                            'expected_percentage': exp_perc,\n",
    "                            'actual_percentage': act_perc,\n",
    "                            'psi_contribution': psi_contribution,\n",
    "                            'base_month': base_month\n",
    "                        })\n",
    "        \n",
    "        return pd.DataFrame(bin_results)\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating bin-level PSI for feature {feature}: {str(e)}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def compute_population_stability_index(df: pd.DataFrame, \n",
    "                                    feature_list: List[str],\n",
    "                                    segment_columns: List[str],\n",
    "                                    month_column: str = 'Application_month') -> Dict[str, pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Main function to compute Population Stability Index\n",
    "    \"\"\"\n",
    "    \n",
    "    # Identify feature types\n",
    "    feature_types = identify_feature_types(df, feature_list)\n",
    "    \n",
    "    # Filter features that exist in dataframe\n",
    "    valid_features = [f for f in feature_list if f in df.columns and feature_types[f] != 'unknown']\n",
    "    print(f\"Processing {len(valid_features)} valid features: {valid_features}\")\n",
    "    \n",
    "    # Calculate base month (minimum month)\n",
    "    base_month = df[month_column].min()\n",
    "    print(f\"Using base month: {base_month}\")\n",
    "    \n",
    "    # Initialize results dataframes\n",
    "    all_feature_psi = []\n",
    "    all_bin_psi = []\n",
    "    \n",
    "    # Calculate PSI for each feature\n",
    "    for feature in valid_features:\n",
    "        print(f\"Processing feature: {feature} ({feature_types[feature]})\")\n",
    "        \n",
    "        try:\n",
    "            # Feature-level PSI\n",
    "            feature_psi_df = calculate_segment_psi(\n",
    "                df, feature, feature_types[feature], segment_columns, month_column, base_month\n",
    "            )\n",
    "            if not feature_psi_df.empty:\n",
    "                all_feature_psi.append(feature_psi_df)\n",
    "            \n",
    "            # Bin-level PSI\n",
    "            bin_psi_df = calculate_bin_level_psi(\n",
    "                df, feature, feature_types[feature], segment_columns, month_column, base_month\n",
    "            )\n",
    "            if not bin_psi_df.empty:\n",
    "                all_bin_psi.append(bin_psi_df)\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Error processing feature {feature}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    # Combine all results\n",
    "    feature_psi_result = pd.concat(all_feature_psi, ignore_index=True) if all_feature_psi else pd.DataFrame()\n",
    "    bin_psi_result = pd.concat(all_bin_psi, ignore_index=True) if all_bin_psi else pd.DataFrame()\n",
    "    \n",
    "    return {\n",
    "        'feature_psi': feature_psi_result,\n",
    "        'bin_psi': bin_psi_result\n",
    "    }\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3555fa9d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "49b71a07",
   "metadata": {},
   "source": [
    "## AFter removing the error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "026d51eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae99e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sq = \"\"\"drop table if exists prj-prod-dataplatform.dap_ds_poweruser_playground.temp_csi_new_monitoring_data;\"\"\"\n",
    "\n",
    "# job = client.query(sq)\n",
    "# job.result()  # Wait for job to complete\n",
    "# print(f\"Table  prj-prod-dataplatform.dap_ds_poweruser_playground.temp_csi_new_monitoring_data dropped successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4500642e",
   "metadata": {},
   "source": [
    "# Queries"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de38264",
   "metadata": {},
   "source": [
    "## Alpha-Cash-CIC-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c39218e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dwaipayan\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\cloud\\bigquery\\table.py:1900: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "customerId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "digitalLoanAccountId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "prediction",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "start_time",
         "rawType": "datetime64[us]",
         "type": "unknown"
        },
        {
         "name": "end_time",
         "rawType": "datetime64[us]",
         "type": "unknown"
        },
        {
         "name": "modelDisplayName",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "modelVersionId",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "new_loan_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "gender",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "osType",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "loanType",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "trenchCategory",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "aStackScore",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "aCicScore",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cic_max_age_all_contracts_snapshot",
         "rawType": "Int64",
         "type": "integer"
        },
        {
         "name": "cic_ratio_overdue_contracts_to_granted_contracts",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "cic_ScoreRange",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cic_ln_loan_level_user_type",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cic_has_ever_been_overdue",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cic_latest_granted_contract_overdue_flag",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cic_ratio_closed_over_new_granted_cnt_24M",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cic_ratio_risky_contracts_to_granted_contracts",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cic_Short_and_Term_Loans_granted_contracts_cnt_24M",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cic_flg_zero_non_granted_ever",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cic_Personal_Loans_granted_contracts_amt_24M",
         "rawType": "object",
         "type": "unknown"
        },
        {
         "name": "cic_CreditAvgCreditLimit",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "cic_flg_zero_granted_ever",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "appln_submit_datetime",
         "rawType": "datetime64[us]",
         "type": "unknown"
        },
        {
         "name": "disbursementDateTime",
         "rawType": "datetime64[us]",
         "type": "unknown"
        },
        {
         "name": "Application_month",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "dataselection",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "0fd9674c-73af-4573-9659-272be6ce771c",
       "rows": [
        [
         "0",
         "3711327",
         "e3b782e3-0ddd-487b-99ac-877479e0b9f4",
         "0.7408754443271659",
         "2025-10-01 07:32:57.111020",
         "2025-10-01 07:32:57.246689",
         "Alpha-Cash-CIC-Model",
         "v1",
         "Quick",
         "M",
         "android",
         "Quick",
         "Trench 1",
         "0.7180017796745426",
         "0.7408754443271659",
         null,
         "0.6666666667",
         "Ai",
         "2_New Applicant",
         "1.0",
         "1.0",
         null,
         "0.3333333333",
         null,
         "1",
         null,
         "40000",
         "0",
         "2025-10-01 15:32:47",
         null,
         "2025-10",
         "Test"
        ],
        [
         "1",
         "3734296",
         "b91a7ba7-352a-4965-8527-87a21ecefcf7",
         "0.7506778180294458",
         "2025-10-09 15:29:42.308355",
         "2025-10-09 15:29:42.434107",
         "Alpha-Cash-CIC-Model",
         "v1",
         "Quick",
         "F",
         "android",
         "Quick",
         "Trench 1",
         "0.6651406202517616",
         "0.7506778180294458",
         "7",
         "1.0",
         "Ai",
         "2_New Applicant",
         "1.0",
         "1.0",
         "1.0",
         "0.0",
         null,
         "0",
         "2000.0",
         "0",
         "0",
         "2025-10-09 23:29:32",
         null,
         "2025-10",
         "Test"
        ],
        [
         "2",
         "3731772",
         "3d50eb87-0075-4e6c-968a-3fc303b0238c",
         "0.3228720506435742",
         "2025-10-08 08:39:21.660847",
         "2025-10-08 08:39:21.860095",
         "Alpha-Cash-CIC-Model",
         "v1",
         "Quick",
         "F",
         "android",
         "Quick",
         "Trench 1",
         "0.19894251778705083",
         "0.3228720506435742",
         null,
         "0.0357142857",
         "Hi",
         "2_New Applicant",
         "1.0",
         "0.0",
         "1.0",
         "0.0",
         "4.0",
         "0",
         "18610.0",
         "92500",
         "0",
         "2025-10-08 16:39:05",
         "2025-10-09 08:25:52",
         "2025-10",
         "Test"
        ],
        [
         "3",
         "3703708",
         "c1d96cd9-0410-45bd-9c2d-b3ca7f4be45d",
         "0.6040735881116797",
         "2025-09-24 23:45:56.812265",
         "2025-09-24 23:45:56.890714",
         "Alpha-Cash-CIC-Model",
         "v1",
         "Quick",
         "F",
         "android",
         "Quick",
         "Trench 1",
         "0.6425621908771012",
         "0.6040735881116797",
         null,
         null,
         "NH_Ii",
         "2_New Applicant",
         "Unknown",
         "Unknown",
         null,
         null,
         null,
         "0",
         null,
         "0",
         "1",
         "2025-09-25 07:45:47",
         null,
         "2025-09",
         "Train"
        ],
        [
         "4",
         "3730522",
         "9e86edd0-0393-4b66-a6af-89cfb9c270f1",
         "0.4556741334480477",
         "2025-10-07 17:15:42.679182",
         "2025-10-07 17:15:42.823178",
         "Alpha-Cash-CIC-Model",
         "v1",
         "Quick",
         "M",
         "android",
         "Quick",
         "Trench 1",
         "0.40850656817295",
         "0.4556741334480477",
         null,
         "0.0",
         "Gi",
         "2_New Applicant",
         "0.0",
         "0.0",
         "0.3333333333",
         "0.0",
         null,
         "0",
         "85100.0",
         "300000",
         "0",
         "2025-10-08 01:15:33",
         "2025-10-08 13:23:55",
         "2025-10",
         "Test"
        ]
       ],
       "shape": {
        "columns": 31,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>customerId</th>\n",
       "      <th>digitalLoanAccountId</th>\n",
       "      <th>prediction</th>\n",
       "      <th>start_time</th>\n",
       "      <th>end_time</th>\n",
       "      <th>modelDisplayName</th>\n",
       "      <th>modelVersionId</th>\n",
       "      <th>new_loan_type</th>\n",
       "      <th>gender</th>\n",
       "      <th>osType</th>\n",
       "      <th>loanType</th>\n",
       "      <th>trenchCategory</th>\n",
       "      <th>aStackScore</th>\n",
       "      <th>aCicScore</th>\n",
       "      <th>cic_max_age_all_contracts_snapshot</th>\n",
       "      <th>cic_ratio_overdue_contracts_to_granted_contracts</th>\n",
       "      <th>cic_ScoreRange</th>\n",
       "      <th>cic_ln_loan_level_user_type</th>\n",
       "      <th>cic_has_ever_been_overdue</th>\n",
       "      <th>cic_latest_granted_contract_overdue_flag</th>\n",
       "      <th>cic_ratio_closed_over_new_granted_cnt_24M</th>\n",
       "      <th>cic_ratio_risky_contracts_to_granted_contracts</th>\n",
       "      <th>cic_Short_and_Term_Loans_granted_contracts_cnt_24M</th>\n",
       "      <th>cic_flg_zero_non_granted_ever</th>\n",
       "      <th>cic_Personal_Loans_granted_contracts_amt_24M</th>\n",
       "      <th>cic_CreditAvgCreditLimit</th>\n",
       "      <th>cic_flg_zero_granted_ever</th>\n",
       "      <th>appln_submit_datetime</th>\n",
       "      <th>disbursementDateTime</th>\n",
       "      <th>Application_month</th>\n",
       "      <th>dataselection</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3711327</td>\n",
       "      <td>e3b782e3-0ddd-487b-99ac-877479e0b9f4</td>\n",
       "      <td>0.7408754443271659</td>\n",
       "      <td>2025-10-01 07:32:57.111020</td>\n",
       "      <td>2025-10-01 07:32:57.246689</td>\n",
       "      <td>Alpha-Cash-CIC-Model</td>\n",
       "      <td>v1</td>\n",
       "      <td>Quick</td>\n",
       "      <td>M</td>\n",
       "      <td>android</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>0.718002</td>\n",
       "      <td>0.740875</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>Ai</td>\n",
       "      <td>2_New Applicant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0.3333333333</td>\n",
       "      <td>None</td>\n",
       "      <td>1</td>\n",
       "      <td>None</td>\n",
       "      <td>40000</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-01 15:32:47</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-10</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3734296</td>\n",
       "      <td>b91a7ba7-352a-4965-8527-87a21ecefcf7</td>\n",
       "      <td>0.7506778180294458</td>\n",
       "      <td>2025-10-09 15:29:42.308355</td>\n",
       "      <td>2025-10-09 15:29:42.434107</td>\n",
       "      <td>Alpha-Cash-CIC-Model</td>\n",
       "      <td>v1</td>\n",
       "      <td>Quick</td>\n",
       "      <td>F</td>\n",
       "      <td>android</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>0.665141</td>\n",
       "      <td>0.750678</td>\n",
       "      <td>7</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>Ai</td>\n",
       "      <td>2_New Applicant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-09 23:29:32</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-10</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3731772</td>\n",
       "      <td>3d50eb87-0075-4e6c-968a-3fc303b0238c</td>\n",
       "      <td>0.3228720506435742</td>\n",
       "      <td>2025-10-08 08:39:21.660847</td>\n",
       "      <td>2025-10-08 08:39:21.860095</td>\n",
       "      <td>Alpha-Cash-CIC-Model</td>\n",
       "      <td>v1</td>\n",
       "      <td>Quick</td>\n",
       "      <td>F</td>\n",
       "      <td>android</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>0.198943</td>\n",
       "      <td>0.322872</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>Hi</td>\n",
       "      <td>2_New Applicant</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0</td>\n",
       "      <td>18610.0</td>\n",
       "      <td>92500</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-08 16:39:05</td>\n",
       "      <td>2025-10-09 08:25:52</td>\n",
       "      <td>2025-10</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3703708</td>\n",
       "      <td>c1d96cd9-0410-45bd-9c2d-b3ca7f4be45d</td>\n",
       "      <td>0.6040735881116797</td>\n",
       "      <td>2025-09-24 23:45:56.812265</td>\n",
       "      <td>2025-09-24 23:45:56.890714</td>\n",
       "      <td>Alpha-Cash-CIC-Model</td>\n",
       "      <td>v1</td>\n",
       "      <td>Quick</td>\n",
       "      <td>F</td>\n",
       "      <td>android</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>0.642562</td>\n",
       "      <td>0.604074</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NH_Ii</td>\n",
       "      <td>2_New Applicant</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2025-09-25 07:45:47</td>\n",
       "      <td>NaT</td>\n",
       "      <td>2025-09</td>\n",
       "      <td>Train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3730522</td>\n",
       "      <td>9e86edd0-0393-4b66-a6af-89cfb9c270f1</td>\n",
       "      <td>0.4556741334480477</td>\n",
       "      <td>2025-10-07 17:15:42.679182</td>\n",
       "      <td>2025-10-07 17:15:42.823178</td>\n",
       "      <td>Alpha-Cash-CIC-Model</td>\n",
       "      <td>v1</td>\n",
       "      <td>Quick</td>\n",
       "      <td>M</td>\n",
       "      <td>android</td>\n",
       "      <td>Quick</td>\n",
       "      <td>Trench 1</td>\n",
       "      <td>0.408507</td>\n",
       "      <td>0.455674</td>\n",
       "      <td>&lt;NA&gt;</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>Gi</td>\n",
       "      <td>2_New Applicant</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.3333333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>None</td>\n",
       "      <td>0</td>\n",
       "      <td>85100.0</td>\n",
       "      <td>300000</td>\n",
       "      <td>0</td>\n",
       "      <td>2025-10-08 01:15:33</td>\n",
       "      <td>2025-10-08 13:23:55</td>\n",
       "      <td>2025-10</td>\n",
       "      <td>Test</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  customerId                  digitalLoanAccountId          prediction  \\\n",
       "0    3711327  e3b782e3-0ddd-487b-99ac-877479e0b9f4  0.7408754443271659   \n",
       "1    3734296  b91a7ba7-352a-4965-8527-87a21ecefcf7  0.7506778180294458   \n",
       "2    3731772  3d50eb87-0075-4e6c-968a-3fc303b0238c  0.3228720506435742   \n",
       "3    3703708  c1d96cd9-0410-45bd-9c2d-b3ca7f4be45d  0.6040735881116797   \n",
       "4    3730522  9e86edd0-0393-4b66-a6af-89cfb9c270f1  0.4556741334480477   \n",
       "\n",
       "                  start_time                   end_time      modelDisplayName  \\\n",
       "0 2025-10-01 07:32:57.111020 2025-10-01 07:32:57.246689  Alpha-Cash-CIC-Model   \n",
       "1 2025-10-09 15:29:42.308355 2025-10-09 15:29:42.434107  Alpha-Cash-CIC-Model   \n",
       "2 2025-10-08 08:39:21.660847 2025-10-08 08:39:21.860095  Alpha-Cash-CIC-Model   \n",
       "3 2025-09-24 23:45:56.812265 2025-09-24 23:45:56.890714  Alpha-Cash-CIC-Model   \n",
       "4 2025-10-07 17:15:42.679182 2025-10-07 17:15:42.823178  Alpha-Cash-CIC-Model   \n",
       "\n",
       "  modelVersionId new_loan_type gender   osType loanType trenchCategory  \\\n",
       "0             v1         Quick      M  android    Quick       Trench 1   \n",
       "1             v1         Quick      F  android    Quick       Trench 1   \n",
       "2             v1         Quick      F  android    Quick       Trench 1   \n",
       "3             v1         Quick      F  android    Quick       Trench 1   \n",
       "4             v1         Quick      M  android    Quick       Trench 1   \n",
       "\n",
       "   aStackScore  aCicScore  cic_max_age_all_contracts_snapshot  \\\n",
       "0     0.718002   0.740875                                <NA>   \n",
       "1     0.665141   0.750678                                   7   \n",
       "2     0.198943   0.322872                                <NA>   \n",
       "3     0.642562   0.604074                                <NA>   \n",
       "4     0.408507   0.455674                                <NA>   \n",
       "\n",
       "   cic_ratio_overdue_contracts_to_granted_contracts cic_ScoreRange  \\\n",
       "0                                          0.666667             Ai   \n",
       "1                                          1.000000             Ai   \n",
       "2                                          0.035714             Hi   \n",
       "3                                               NaN          NH_Ii   \n",
       "4                                          0.000000             Gi   \n",
       "\n",
       "  cic_ln_loan_level_user_type cic_has_ever_been_overdue  \\\n",
       "0             2_New Applicant                       1.0   \n",
       "1             2_New Applicant                       1.0   \n",
       "2             2_New Applicant                       1.0   \n",
       "3             2_New Applicant                   Unknown   \n",
       "4             2_New Applicant                       0.0   \n",
       "\n",
       "  cic_latest_granted_contract_overdue_flag  \\\n",
       "0                                      1.0   \n",
       "1                                      1.0   \n",
       "2                                      0.0   \n",
       "3                                  Unknown   \n",
       "4                                      0.0   \n",
       "\n",
       "  cic_ratio_closed_over_new_granted_cnt_24M  \\\n",
       "0                                      None   \n",
       "1                                       1.0   \n",
       "2                                       1.0   \n",
       "3                                      None   \n",
       "4                              0.3333333333   \n",
       "\n",
       "  cic_ratio_risky_contracts_to_granted_contracts  \\\n",
       "0                                   0.3333333333   \n",
       "1                                            0.0   \n",
       "2                                            0.0   \n",
       "3                                           None   \n",
       "4                                            0.0   \n",
       "\n",
       "  cic_Short_and_Term_Loans_granted_contracts_cnt_24M  \\\n",
       "0                                               None   \n",
       "1                                               None   \n",
       "2                                                4.0   \n",
       "3                                               None   \n",
       "4                                               None   \n",
       "\n",
       "  cic_flg_zero_non_granted_ever cic_Personal_Loans_granted_contracts_amt_24M  \\\n",
       "0                             1                                         None   \n",
       "1                             0                                       2000.0   \n",
       "2                             0                                      18610.0   \n",
       "3                             0                                         None   \n",
       "4                             0                                      85100.0   \n",
       "\n",
       "  cic_CreditAvgCreditLimit cic_flg_zero_granted_ever appln_submit_datetime  \\\n",
       "0                    40000                         0   2025-10-01 15:32:47   \n",
       "1                        0                         0   2025-10-09 23:29:32   \n",
       "2                    92500                         0   2025-10-08 16:39:05   \n",
       "3                        0                         1   2025-09-25 07:45:47   \n",
       "4                   300000                         0   2025-10-08 01:15:33   \n",
       "\n",
       "  disbursementDateTime Application_month dataselection  \n",
       "0                  NaT           2025-10          Test  \n",
       "1                  NaT           2025-10          Test  \n",
       "2  2025-10-09 08:25:52           2025-10          Test  \n",
       "3                  NaT           2025-09         Train  \n",
       "4  2025-10-08 13:23:55           2025-10          Test  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sq = r\"\"\"  \n",
    "WITH parsed as (\n",
    "  select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "--REPLACE(REPLACE(prediction, \"'\", '\"'), \"None\", \"null\") AS prediction_clean\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName = 'Alpha-Cash-CIC-Model'\n",
    "),\n",
    "\n",
    "latest_request as (\n",
    "select * from parsed\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY customerId, digitalLoanAccountId,modelDisplayName ORDER BY start_time DESC ) = 1),\n",
    "\n",
    "model_run as (\n",
    "select customerId,digitalLoanAccountId,modelName, publish_time,requestPayload as requestPayload_clean\n",
    "--REPLACE(REPLACE(requestPayload, \"'\", '\"'), \"None\", \"null\") AS requestPayload_clean\n",
    "from `prj-prod-dataplatform.audit_balance.ml_request_details` \n",
    "WHERE modelName = 'Alpha-Cash-Model-response'\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY customerId, digitalLoanAccountId,modelName ORDER BY publish_time DESC ) = 1),\n",
    "base as (\n",
    "select * from (\n",
    "  select \n",
    " r.customerId,\n",
    " r.digitalLoanAccountId,\n",
    " r.prediction,\n",
    " r.start_time,\n",
    " r.end_time,\n",
    " r.modelDisplayName,\n",
    " r.modelVersionId,\n",
    " loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "  REGEXP_EXTRACT(m.requestPayload_clean, r\"osType[:=]['\\\"]?([^'\\\"]+)['\\\"]?\") AS osType,\n",
    "  REGEXP_EXTRACT(m.requestPayload_clean, r\"loanType[:=]['\\\"]?([^'\\\"]+)['\\\"]?\") AS loanType,\n",
    "  REGEXP_EXTRACT(m.requestPayload_clean, r\"trenchCategory[:=]['\\\"]?([^'\\\"]+)['\\\"]?\") AS trenchCategory,\n",
    "   SAFE_CAST(REGEXP_EXTRACT(m.requestPayload_clean, r\"aStackScore[:= ]([0-9\\.]+)\") AS FLOAT64) AS aStackScore,\n",
    "  SAFE_CAST(REGEXP_EXTRACT(m.requestPayload_clean, r\"aCicScore[:= ]([0-9\\.]+)\") AS FLOAT64) AS aCicScore,\n",
    "  --  Alpha CIC Score Model Features for Trench 1\n",
    "  SAFE_CAST(JSON_VALUE(r.calcFeatures, \"$.cic_max_age_all_contracts_snapshot\") AS INT64) AS cic_max_age_all_contracts_snapshot,\n",
    "  SAFE_CAST(JSON_VALUE(r.calcFeatures, \"$.cic_ratio_overdue_contracts_to_granted_contracts\") AS FLOAT64) AS cic_ratio_overdue_contracts_to_granted_contracts,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_ScoreRange\") AS cic_ScoreRange,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_ln_loan_level_user_type\") AS cic_ln_loan_level_user_type,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_has_ever_been_overdue\") AS cic_has_ever_been_overdue,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_latest_granted_contract_overdue_flag\") AS cic_latest_granted_contract_overdue_flag,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_ratio_closed_over_new_granted_cnt_24M\") AS cic_ratio_closed_over_new_granted_cnt_24M,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_ratio_risky_contracts_to_granted_contracts\") AS cic_ratio_risky_contracts_to_granted_contracts,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_Short_and_Term_Loans_granted_contracts_cnt_24M\") AS cic_Short_and_Term_Loans_granted_contracts_cnt_24M,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_flg_zero_non_granted_ever\") AS cic_flg_zero_non_granted_ever,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_Personal_Loans_granted_contracts_amt_24M\") AS cic_Personal_Loans_granted_contracts_amt_24M,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_CreditAvgCreditLimit\") AS cic_CreditAvgCreditLimit,\n",
    "  JSON_VALUE(r.calcFeatures, \"$.cic_flg_zero_granted_ever\") AS cic_flg_zero_granted_ever,\n",
    "  coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "FROM latest_request r\n",
    "left join model_run m\n",
    "on r.digitalLoanAccountId = m.digitalLoanAccountId \n",
    "left join risk_credit_mis.loan_master_table loanmaster \n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    " )\n",
    "where trenchCategory = 'Trench 1'\n",
    ")\n",
    "select *, case when appln_submit_datetime <= '2025-09-30' then 'Train' else 'Test' end dataselection from base\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(sq).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854d547d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(100).to_json(r'D:\\OneDrive - Tonik Financial Pte Ltd\\MyStuff\\Data Engineering\\Model_Monitoring\\New_Model_Monitoring\\Notebook\\CSI_testing_data.json', orient='records', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5427c5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cic_flg_zero_granted_ever'].value_counts(dropna=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2a6007",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the object column to numeric for correct population stability index calculation\n",
    "columns_to_convert = [\n",
    "    'cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "    'cic_ratio_risky_contracts_to_granted_contracts',\n",
    "    'cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "    'cic_Personal_Loans_granted_contracts_amt_24M',\n",
    "    'cic_CreditAvgCreditLimit'\n",
    "]\n",
    "\n",
    "for col in columns_to_convert:\n",
    "    df[col] = pd.to_numeric(df[col], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62ef21c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18584598",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r\"sample.csv\", index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45774bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with your data:\n",
    "def analyze_sample_data(df):\n",
    "    # Define features and segments\n",
    "    feature_list = [\n",
    "        'aCicScore',\n",
    "        'cic_max_age_all_contracts_snapshot',\n",
    "        'cic_ratio_overdue_contracts_to_granted_contracts', \n",
    "        'cic_ScoreRange',\n",
    "        'cic_ln_loan_level_user_type', \n",
    "        'cic_has_ever_been_overdue',\n",
    "        'cic_latest_granted_contract_overdue_flag',\n",
    "        'cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "        'cic_ratio_risky_contracts_to_granted_contracts',\n",
    "        'cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "        'cic_flg_zero_non_granted_ever',\n",
    "        'cic_Personal_Loans_granted_contracts_amt_24M',\n",
    "        'cic_CreditAvgCreditLimit', \n",
    "        'cic_flg_zero_granted_ever',\n",
    "    ]\n",
    "    \n",
    "    segment_columns = ['new_loan_type', 'gender', 'osType', 'loanType', 'trenchCategory']\n",
    "    \n",
    "    # Calculate PSI\n",
    "    results = compute_population_stability_index(df, feature_list, segment_columns, 'Application_month')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the analysis\n",
    "results = analyze_sample_data(df)\n",
    "psi_results = results['feature_psi'].copy()\n",
    "psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "\n",
    "bin_psi_results = results['bin_psi'].copy()\n",
    "\n",
    "bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5917cfec",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results[psi_results['segment_name'] == 'gender'].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda4d96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display results\n",
    "print(\"\\n=== FEATURE-LEVEL PSI RESULTS ===\")\n",
    "results['feature_psi'].head(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce860ece",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n=== BIN-LEVEL PSI RESULTS ===\")\n",
    "results['bin_psi'].head(100).sort_values(by=['feature', 'month', 'segment_name', 'segment_value', 'bin'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db8e233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics\n",
    "if not results['feature_psi'].empty:\n",
    "    print(f\"\\n=== SUMMARY ===\")\n",
    "    print(f\"Total feature-month-segment combinations: {len(results['feature_psi'])}\")\n",
    "    print(f\"Features analyzed: {results['feature_psi']['feature'].nunique()}\")\n",
    "    print(f\"Segments analyzed: {results['feature_psi']['segment_name'].nunique()}\")\n",
    "    print(f\"PSI range: [{results['feature_psi']['psi'].min():.4f}, {results['feature_psi']['psi'].max():.4f}]\")\n",
    "    \n",
    "    # Count by PSI interpretation\n",
    "    psi_interpretation = []\n",
    "    for psi in results['feature_psi']['psi'].dropna():\n",
    "        if psi < 0.1:\n",
    "            psi_interpretation.append('Stable (PSI < 0.1)')\n",
    "        elif psi < 0.25:\n",
    "            psi_interpretation.append('Moderate change (0.1  PSI < 0.25)')\n",
    "        else:\n",
    "            psi_interpretation.append('Significant change (PSI  0.25)')\n",
    "    \n",
    "    if psi_interpretation:\n",
    "        interpretation_counts = pd.Series(psi_interpretation).value_counts()\n",
    "        print(\"\\nPSI Interpretation Distribution:\")\n",
    "        for interpretation, count in interpretation_counts.items():\n",
    "            print(f\"  {interpretation}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4319f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage with your data:\n",
    "def analyze_sample_data(df):\n",
    "    # Load your data\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Define features and segments\n",
    "    feature_list = ['aCicScore',\n",
    "        'cic_max_age_all_contracts_snapshot',\n",
    "        'cic_ratio_overdue_contracts_to_granted_contracts', \n",
    "        'cic_ScoreRange',\n",
    "        'cic_ln_loan_level_user_type', \n",
    "        'cic_has_ever_been_overdue',\n",
    "        'cic_latest_granted_contract_overdue_flag',\n",
    "        'cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "        'cic_ratio_risky_contracts_to_granted_contracts',\n",
    "        'cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "        'cic_flg_zero_non_granted_ever',\n",
    "        'cic_Personal_Loans_granted_contracts_amt_24M',\n",
    "        'cic_CreditAvgCreditLimit', \n",
    "        'cic_flg_zero_granted_ever',\n",
    "    ]\n",
    "    \n",
    "    segment_columns = ['new_loan_type', 'gender', 'osType', 'loanType', 'trenchCategory']\n",
    "    \n",
    "    # Calculate PSI\n",
    "    results = compute_population_stability_index(df, feature_list, segment_columns, 'Application_month')\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Run the analysis\n",
    "if __name__ == \"__main__\":\n",
    "    results = analyze_sample_data(df)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n=== FEATURE-LEVEL PSI RESULTS ===\")\n",
    "    results['feature_psi'].head(10)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7506915",
   "metadata": {},
   "outputs": [],
   "source": [
    "    print(\"\\n=== BIN-LEVEL PSI RESULTS ===\")\n",
    "    print(results['bin_psi'].head(10))\n",
    "    \n",
    "    # Save results to files\n",
    "    results['feature_psi'].to_csv('feature_psi_results.csv', index=False)\n",
    "    results['bin_psi'].to_csv('bin_psi_results.csv', index=False)\n",
    "    \n",
    "    print(\"\\nResults saved to feature_psi_results.csv and bin_psi_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc51e803",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def usage(df):\n",
    "    \"\"\"\n",
    "    Example of how to use both functions\n",
    "    \"\"\"\n",
    "    # Load your data\n",
    "    df = df.copy()\n",
    "    \n",
    "    feature_list = ['aCicScore',\n",
    "        'cic_max_age_all_contracts_snapshot',\n",
    "        'cic_ratio_overdue_contracts_to_granted_contracts', \n",
    "        'cic_ScoreRange',\n",
    "        'cic_ln_loan_level_user_type', \n",
    "        'cic_has_ever_been_overdue',\n",
    "        'cic_latest_granted_contract_overdue_flag',\n",
    "        'cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "        'cic_ratio_risky_contracts_to_granted_contracts',\n",
    "        'cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "        'cic_flg_zero_non_granted_ever',\n",
    "        'cic_Personal_Loans_granted_contracts_amt_24M',\n",
    "        'cic_CreditAvgCreditLimit', \n",
    "        'cic_flg_zero_granted_ever',\n",
    "    ]\n",
    "    \n",
    "    segment_columns = ['new_loan_type', 'gender', 'osType', 'loanType', 'trenchCategory']\n",
    "    \n",
    "    # Calculate overall PSI\n",
    "    print(\"Calculating overall PSI...\")\n",
    "    psi_results = calculate_population_stability_index(df, feature_list, segment_columns)\n",
    "    analyze_psi_results(psi_results)\n",
    "    psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "    psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "    psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "    \n",
    "    \n",
    "    # Calculate bin-level PSI\n",
    "    print(\"\\nCalculating bin-level PSI...\")\n",
    "    bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "    analyze_bin_level_results(bin_psi_results)\n",
    "    bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "    bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "    bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "    \n",
    "    psi_results = psi_results[[ 'modelDisplayName',\n",
    "       'modelVersionId', 'trenchCategory', 'feature', 'feature_type', 'segment_column', 'segment_value',\n",
    "       'baseline_month', 'current_month', 'psi', 'num_baseline_records',\n",
    "       'num_current_records', 'psi_interpretation']].copy()\n",
    "    \n",
    "    bin_psi_results = bin_psi_results[[ 'modelDisplayName', 'modelVersionId',\n",
    "       'trenchCategory','feature', 'feature_type', 'segment_column', 'segment_value',\n",
    "       'baseline_month', 'current_month', 'bin', 'baseline_percentage',\n",
    "       'current_percentage', 'psi_component', 'baseline_count',\n",
    "       'current_count', 'percentage_change', 'change_interpretation',\n",
    "       'abs_percentage_change',]].copy()\n",
    "    \n",
    "    \n",
    "\n",
    "    return psi_results, bin_psi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e00f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "psi_results, bin_psi_results = usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0a466d",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6ba2b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_psi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f9f44e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.temp_csi_new_monitoring_data\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.temp_csi_new_monitoring_data_feature_bin_level\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_TRUNCATE\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98ddc5b",
   "metadata": {},
   "source": [
    "## Alpha-Cash-Stack-Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895b3367",
   "metadata": {},
   "outputs": [],
   "source": [
    "sq = r\"\"\"WITH parsed as (\n",
    "  select customerId, digitalLoanAccountId,modelDisplayName,modelVersionId,start_time,end_time,prediction,\n",
    "REPLACE(REPLACE(calcFeature, \"'\", '\"'), \"None\", \"null\") AS calcFeatures,\n",
    "FROM `prj-prod-dataplatform.audit_balance.ml_model_run_details`\n",
    "where modelDisplayName = 'Alpha-Cash-Stack-Model'),\n",
    "latest_request as (\n",
    "select * from parsed\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY customerId, digitalLoanAccountId,modelDisplayName ORDER BY start_time DESC ) = 1),\n",
    "model_run as (\n",
    "select customerId,digitalLoanAccountId,modelName, publish_time,requestPayload as requestPayload_clean\n",
    "from `prj-prod-dataplatform.audit_balance.ml_request_details` \n",
    "WHERE modelName = 'Alpha-Cash-Model-response'\n",
    "QUALIFY ROW_NUMBER() OVER (PARTITION BY customerId, digitalLoanAccountId,modelName ORDER BY publish_time DESC ) = 1)\n",
    "select * from (\n",
    "  select \n",
    " r.customerId,\n",
    " r.digitalLoanAccountId,\n",
    " r.prediction Alpha_Cash_Stack_Score,\n",
    " r.start_time,\n",
    " r.end_time,\n",
    " r.modelDisplayName,\n",
    " r.modelVersionId,\n",
    "  loanmaster.new_loan_type,\n",
    " loanmaster.gender,\n",
    "\n",
    "  REGEXP_EXTRACT(m.requestPayload_clean, r\"osType[:=]['\\\"]?([^'\\\"]+)['\\\"]?\") AS osType,\n",
    "  REGEXP_EXTRACT(m.requestPayload_clean, r\"loanType[:=]['\\\"]?([^'\\\"]+)['\\\"]?\") AS loanType,\n",
    "  REGEXP_EXTRACT(m.requestPayload_clean, r\"trenchCategory[:=]['\\\"]?([^'\\\"]+)['\\\"]?\") AS trenchCategory,\n",
    "\n",
    " SAFE_CAST(JSON_VALUE(r.calcFeatures, \"$.apps_score\") AS FLOAT64) AS  apps_score,\n",
    " SAFE_CAST(JSON_VALUE(r.calcFeatures, \"$.c_demo_score\") AS FLOAT64) AS  c_demo_score,\n",
    " SAFE_CAST(JSON_VALUE(r.calcFeatures, \"$.c_credo_score\") AS FLOAT64) AS  c_credo_score,\n",
    " SAFE_CAST(JSON_VALUE(r.calcFeatures, \"$.c_tx_score\") AS FLOAT64) AS  c_tx_score,\n",
    " SAFE_CAST(JSON_VALUE(r.calcFeatures, \"$.ca_cic_score\") AS FLOAT64) AS  ca_cic_score,\n",
    "coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time) AS appln_submit_datetime,\n",
    "  loanmaster.disbursementDateTime,\n",
    "  format_date('%Y-%m', coalesce(IF(loanmaster.new_loan_type = 'Flex-up', loanmaster.startApplyDateTime, loanmaster.termsAndConditionsSubmitDateTime),  r.start_time)) as Application_month, \n",
    "FROM latest_request r\n",
    "left join model_run m\n",
    "on r.digitalLoanAccountId = m.digitalLoanAccountId \n",
    "left join risk_credit_mis.loan_master_table loanmaster \n",
    "  ON loanmaster.digitalLoanAccountId = r.digitalLoanAccountId\n",
    ") where trenchCategory = 'Trench 1'\n",
    ";\n",
    "\"\"\"\n",
    "\n",
    "df = client.query(sq).to_dataframe()\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e54a8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f30b11bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae575064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage\n",
    "def usage(df):\n",
    "    \"\"\"\n",
    "    Example of how to use both functions\n",
    "    \"\"\"\n",
    "    # Load your data\n",
    "    df = df.copy()\n",
    "    \n",
    "    feature_list = ['Alpha_Cash_Stack_Score',  \n",
    "                    'apps_score', \n",
    "                    'c_demo_score', \n",
    "                    'c_credo_score', \n",
    "                    'c_tx_score', \n",
    "                    'ca_cic_score'\n",
    "    ]\n",
    "    \n",
    "    segment_columns = ['new_loan_type', 'gender', 'osType', 'loanType', 'trenchCategory']\n",
    "    \n",
    "    # Calculate overall PSI\n",
    "    print(\"Calculating overall PSI...\")\n",
    "    psi_results = calculate_population_stability_index(df, feature_list, segment_columns)\n",
    "    analyze_psi_results(psi_results)\n",
    "    psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "    psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "    psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "    \n",
    "    \n",
    "    # Calculate bin-level PSI\n",
    "    print(\"\\nCalculating bin-level PSI...\")\n",
    "    bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "    analyze_bin_level_results(bin_psi_results)\n",
    "    bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "    bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "    bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "    \n",
    "    psi_results = psi_results[[ 'modelDisplayName',\n",
    "       'modelVersionId', 'trenchCategory', 'feature', 'feature_type', 'segment_column', 'segment_value',\n",
    "       'baseline_month', 'current_month', 'psi', 'num_baseline_records',\n",
    "       'num_current_records', 'psi_interpretation']].copy()\n",
    "    \n",
    "    bin_psi_results = bin_psi_results[[ 'modelDisplayName', 'modelVersionId',\n",
    "       'trenchCategory','feature', 'feature_type', 'segment_column', 'segment_value',\n",
    "       'baseline_month', 'current_month', 'bin', 'baseline_percentage',\n",
    "       'current_percentage', 'psi_component', 'baseline_count',\n",
    "       'current_count', 'percentage_change', 'change_interpretation',\n",
    "       'abs_percentage_change',]].copy()\n",
    "    \n",
    "    \n",
    "\n",
    "    return psi_results, bin_psi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af5278f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the analysis\n",
    "psi_results, bin_psi_results = usage(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f0610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aab0bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "bin_psi_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20547d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.temp_csi_new_monitoring_data\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete\n",
    "\n",
    "\n",
    "# Upload to BigQuery\n",
    "table_id = \"prj-prod-dataplatform.dap_ds_poweruser_playground.temp_csi_new_monitoring_data_feature_bin_level\"\n",
    "job_config = bigquery.LoadJobConfig(\n",
    "    write_disposition=\"WRITE_APPEND\",  # or \"WRITE_APPEND\"\n",
    ")\n",
    "job = client.load_table_from_dataframe(bin_psi_results, table_id, job_config=job_config)\n",
    "job.result()  # Wait for the job to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82dcc64b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "21909c49",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff11312",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "# from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "# def calculate_population_stability_index(\n",
    "#     df: pd.DataFrame, \n",
    "#     feature_list: List[str], \n",
    "#     segment_columns: List[str], \n",
    "#     month_column: str = 'Application_month',\n",
    "#     baseline_month: Optional[str] = None\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Calculate Population Stability Index (PSI) for features overall and by segments.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     df : pd.DataFrame\n",
    "#         Input dataframe with features and segments\n",
    "#     feature_list : List[str]\n",
    "#         List of feature names to calculate PSI for\n",
    "#     segment_columns : List[str]\n",
    "#         List of segment column names\n",
    "#     month_column : str\n",
    "#         Name of the month column (default: 'Application_month')\n",
    "#     baseline_month : str, optional\n",
    "#         Specific baseline month to use. If None, uses minimum month\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     pd.DataFrame\n",
    "#         DataFrame with PSI values for each feature, segment, and month\n",
    "#     \"\"\"\n",
    "    \n",
    "#     # Identify feature types\n",
    "#     def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, str]:\n",
    "#         \"\"\"Identify categorical vs numerical features\"\"\"\n",
    "#         feature_types = {}\n",
    "        \n",
    "#         for feature in feature_list:\n",
    "#             # Check if feature exists in dataframe\n",
    "#             if feature not in df.columns:\n",
    "#                 print(f\"Warning: Feature '{feature}' not found in dataframe\")\n",
    "#                 continue\n",
    "                \n",
    "#             # Check data type and unique values\n",
    "#             unique_vals = df[feature].nunique()\n",
    "#             dtype = df[feature].dtype\n",
    "            \n",
    "#             # Rules for categorical features\n",
    "#             if (dtype == 'object' or \n",
    "#                 unique_vals <= 10 or \n",
    "#                 feature in ['cic_ScoreRange', 'cic_ln_loan_level_user_type', \n",
    "#                           'cic_has_ever_been_overdue', 'cic_latest_granted_contract_overdue_flag',\n",
    "#                           'cic_flg_zero_non_granted_ever', 'cic_flg_zero_granted_ever']):\n",
    "#                 feature_types[feature] = 'categorical'\n",
    "#             else:\n",
    "#                 feature_types[feature] = 'numerical'\n",
    "                \n",
    "#         return feature_types\n",
    "    \n",
    "#     # Create bins for numerical features\n",
    "#     def create_numerical_bins(baseline_data: pd.Series, n_bins: int = 10) -> pd.IntervalIndex:\n",
    "#         \"\"\"Create bins for numerical features using deciles\"\"\"\n",
    "#         # Remove null values\n",
    "#         clean_data = baseline_data.dropna()\n",
    "        \n",
    "#         if len(clean_data) == 0:\n",
    "#             return None\n",
    "            \n",
    "#         # Create decile bins\n",
    "#         try:\n",
    "#             bins = pd.qcut(clean_data, n_bins, duplicates='drop', retbins=True)[1]\n",
    "#             # Ensure bins are unique and in order\n",
    "#             bins = sorted(set(bins))\n",
    "#             return pd.IntervalIndex.from_breaks(bins)\n",
    "#         except:\n",
    "#             # Fallback to equal width bins if deciles fail\n",
    "#             min_val = clean_data.min()\n",
    "#             max_val = clean_data.max()\n",
    "#             bins = np.linspace(min_val, max_val, n_bins + 1)\n",
    "#             return pd.IntervalIndex.from_breaks(bins)\n",
    "    \n",
    "#     # Process categorical features\n",
    "#     def process_categorical_feature(baseline_data: pd.Series, top_n: int = 6) -> List:\n",
    "#         \"\"\"Get top N categories and group rest as 'Other'\"\"\"\n",
    "#         value_counts = baseline_data.value_counts()\n",
    "#         top_categories = value_counts.head(top_n).index.tolist()\n",
    "#         return top_categories\n",
    "    \n",
    "#     # Calculate distribution\n",
    "#     def calculate_distribution(data: pd.Series, feature_type: str, \n",
    "#                             bins: pd.IntervalIndex = None, \n",
    "#                             top_categories: List = None) -> Dict:\n",
    "#         \"\"\"Calculate distribution of data\"\"\"\n",
    "#         if feature_type == 'numerical':\n",
    "#             return calculate_numerical_distribution(data, bins)\n",
    "#         else:\n",
    "#             return calculate_categorical_distribution(data, top_categories)\n",
    "    \n",
    "#     def calculate_numerical_distribution(data: pd.Series, bins: pd.IntervalIndex) -> Dict:\n",
    "#         \"\"\"Calculate distribution for numerical data\"\"\"\n",
    "#         if bins is None:\n",
    "#             return {}\n",
    "            \n",
    "#         clean_data = data.dropna()\n",
    "#         if len(clean_data) == 0:\n",
    "#             return {str(bin_): 0 for bin_ in bins}\n",
    "        \n",
    "#         binned = pd.cut(clean_data, bins, include_lowest=True)\n",
    "#         distribution = binned.value_counts().sort_index()\n",
    "#         total = len(clean_data)\n",
    "        \n",
    "#         return {str(interval): count/total if total > 0 else 0 \n",
    "#                 for interval, count in distribution.items()}\n",
    "    \n",
    "#     def calculate_categorical_distribution(data: pd.Series, top_categories: List) -> Dict:\n",
    "#         \"\"\"Calculate distribution for categorical data\"\"\"\n",
    "#         if top_categories is None:\n",
    "#             return {}\n",
    "            \n",
    "#         value_counts = data.value_counts()\n",
    "#         total = len(data.dropna())\n",
    "        \n",
    "#         distribution = {}\n",
    "#         for category in top_categories:\n",
    "#             if category in value_counts:\n",
    "#                 distribution[category] = value_counts[category] / total if total > 0 else 0\n",
    "#             else:\n",
    "#                 distribution[category] = 0\n",
    "                \n",
    "#         # Calculate \"Other\" category\n",
    "#         other_categories = [cat for cat in value_counts.index if cat not in top_categories]\n",
    "#         other_count = value_counts[other_categories].sum() if other_categories else 0\n",
    "#         distribution['Other'] = other_count / total if total > 0 else 0\n",
    "        \n",
    "#         return distribution\n",
    "    \n",
    "#     # Calculate PSI\n",
    "#     def calculate_psi(expected_dist: Dict, actual_dist: Dict) -> float:\n",
    "#         \"\"\"Calculate Population Stability Index\"\"\"\n",
    "#         psi = 0\n",
    "#         all_categories = set(expected_dist.keys()) | set(actual_dist.keys())\n",
    "        \n",
    "#         for category in all_categories:\n",
    "#             expected_pct = expected_dist.get(category, 1e-6)  # Avoid division by zero\n",
    "#             actual_pct = actual_dist.get(category, 1e-6)\n",
    "            \n",
    "#             # Avoid log(0) by using small epsilon\n",
    "#             if expected_pct == 0:\n",
    "#                 expected_pct = 1e-6\n",
    "#             if actual_pct == 0:\n",
    "#                 actual_pct = 1e-6\n",
    "                \n",
    "#             psi_component = (actual_pct - expected_pct) * np.log(actual_pct / expected_pct)\n",
    "#             psi += psi_component\n",
    "            \n",
    "#         return psi\n",
    "    \n",
    "#     # Main PSI calculation logic\n",
    "#     def calculate_psi_for_feature(df: pd.DataFrame, feature: str, feature_type: str, \n",
    "#                                 month_column: str, segment_info: Tuple[str, str] = None) -> pd.DataFrame:\n",
    "#         \"\"\"Calculate PSI for a specific feature\"\"\"\n",
    "        \n",
    "#         # Filter data if segment is specified\n",
    "#         if segment_info:\n",
    "#             segment_col, segment_val = segment_info\n",
    "#             segment_data = df[df[segment_col] == segment_val].copy()\n",
    "#         else:\n",
    "#             segment_data = df.copy()\n",
    "#             segment_info = ('Overall', 'Overall')\n",
    "        \n",
    "#         # Get months and baseline\n",
    "#         months = sorted(segment_data[month_column].unique())\n",
    "#         if len(months) < 2:\n",
    "#             return pd.DataFrame()  # Need at least 2 months for comparison\n",
    "            \n",
    "#         if baseline_month:\n",
    "#             if baseline_month not in months:\n",
    "#                 print(f\"Warning: Baseline month {baseline_month} not found in segment {segment_info}\")\n",
    "#                 return pd.DataFrame()\n",
    "#             baseline_month_used = baseline_month\n",
    "#         else:\n",
    "#             baseline_month_used = months[0]\n",
    "        \n",
    "#         # Get baseline data\n",
    "#         baseline_data = segment_data[segment_data[month_column] == baseline_month_used][feature]\n",
    "        \n",
    "#         # Skip if baseline data is empty\n",
    "#         if len(baseline_data.dropna()) == 0:\n",
    "#             return pd.DataFrame()\n",
    "        \n",
    "#         # Prepare bins or categories based on baseline\n",
    "#         if feature_type == 'numerical':\n",
    "#             bins = create_numerical_bins(baseline_data)\n",
    "#             top_categories = None\n",
    "#         else:\n",
    "#             bins = None\n",
    "#             top_categories = process_categorical_feature(baseline_data)\n",
    "        \n",
    "#         # Calculate baseline distribution\n",
    "#         baseline_dist = calculate_distribution(baseline_data, feature_type, bins, top_categories)\n",
    "        \n",
    "#         # Calculate PSI for each month compared to baseline\n",
    "#         results = []\n",
    "#         for current_month in months:\n",
    "#             if current_month == baseline_month_used:\n",
    "#                 # For baseline month, PSI = 0 (comparison with itself)\n",
    "#                 psi_value = 0.0\n",
    "#                 current_data = baseline_data\n",
    "#                 current_dist = baseline_dist\n",
    "#             else:\n",
    "#                 current_data = segment_data[segment_data[month_column] == current_month][feature]\n",
    "                \n",
    "#                 if len(current_data.dropna()) == 0:\n",
    "#                     continue\n",
    "                    \n",
    "#                 current_dist = calculate_distribution(current_data, feature_type, bins, top_categories)\n",
    "#                 psi_value = calculate_psi(baseline_dist, current_dist)\n",
    "            \n",
    "#             results.append({\n",
    "#                 'feature': feature,\n",
    "#                 'feature_type': feature_type,\n",
    "#                 'segment_column': segment_info[0],\n",
    "#                 'segment_value': segment_info[1],\n",
    "#                 'baseline_month': baseline_month_used,\n",
    "#                 'current_month': current_month,\n",
    "#                 'psi': psi_value,\n",
    "#                 'num_baseline_records': len(baseline_data),\n",
    "#                 'num_current_records': len(current_data)\n",
    "#             })\n",
    "        \n",
    "#         return pd.DataFrame(results)\n",
    "    \n",
    "#     # Main execution\n",
    "#     print(\"Identifying feature types...\")\n",
    "#     feature_types = identify_feature_types(df, feature_list)\n",
    "    \n",
    "#     # Prepare results dataframe\n",
    "#     all_results = []\n",
    "    \n",
    "#     # Calculate PSI overall (no segmentation)\n",
    "#     print(\"Calculating overall PSI...\")\n",
    "#     for feature, ftype in feature_types.items():\n",
    "#         print(f\"  Processing {feature} ({ftype})\")\n",
    "#         result_df = calculate_psi_for_feature(df, feature, ftype, month_column)\n",
    "#         if not result_df.empty:\n",
    "#             all_results.append(result_df)\n",
    "    \n",
    "#     # Calculate PSI for each segment\n",
    "#     print(\"Calculating segment-wise PSI...\")\n",
    "#     for segment_col in segment_columns:\n",
    "#         if segment_col not in df.columns:\n",
    "#             print(f\"Warning: Segment column '{segment_col}' not found in dataframe\")\n",
    "#             continue\n",
    "            \n",
    "#         segment_values = df[segment_col].dropna().unique()\n",
    "#         print(f\"  Processing segment: {segment_col} ({len(segment_values)} values)\")\n",
    "        \n",
    "#         for segment_val in segment_values:\n",
    "#             for feature, ftype in feature_types.items():\n",
    "#                 result_df = calculate_psi_for_feature(\n",
    "#                     df, feature, ftype, month_column, (segment_col, segment_val)\n",
    "#                 )\n",
    "#                 if not result_df.empty:\n",
    "#                     all_results.append(result_df)\n",
    "    \n",
    "#     # Combine all results\n",
    "#     if all_results:\n",
    "#         final_results = pd.concat(all_results, ignore_index=True)\n",
    "        \n",
    "#         # Add PSI interpretation\n",
    "#         def interpret_psi(psi_value):\n",
    "#             if psi_value < 0.1:\n",
    "#                 return 'No significant change'\n",
    "#             elif psi_value < 0.2:\n",
    "#                 return 'Minor change'\n",
    "#             elif psi_value < 0.5:\n",
    "#                 return 'Moderate change'\n",
    "#             else:\n",
    "#                 return 'Significant change'\n",
    "        \n",
    "#         final_results['psi_interpretation'] = final_results['psi'].apply(interpret_psi)\n",
    "        \n",
    "#         # Create combined month column for plotting\n",
    "#         # For baseline month, use baseline_month, for others use current_month\n",
    "#         final_results['plot_month'] = final_results['current_month']\n",
    "        \n",
    "#         # Add month sequence for proper ordering in plots\n",
    "#         all_months = sorted(final_results['plot_month'].unique())\n",
    "#         month_sequence = {month: i for i, month in enumerate(all_months)}\n",
    "#         final_results['month_sequence'] = final_results['plot_month'].map(month_sequence)\n",
    "        \n",
    "#         return final_results.sort_values(['feature', 'segment_column', 'segment_value', 'month_sequence'])\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# def calculate_bin_level_psi(\n",
    "#     df: pd.DataFrame,\n",
    "#     feature_list: List[str],\n",
    "#     segment_columns: List[str],\n",
    "#     month_column: str = 'Application_month',\n",
    "#     baseline_month: Optional[str] = None,\n",
    "#     top_n_categories: int = 6\n",
    "# ) -> pd.DataFrame:\n",
    "#     \"\"\"\n",
    "#     Calculate bin-level PSI details showing distribution changes for each bin/category.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     df : pd.DataFrame\n",
    "#         Input dataframe with features and segments\n",
    "#     feature_list : List[str]\n",
    "#         List of feature names to calculate PSI for\n",
    "#     segment_columns : List[str]\n",
    "#         List of segment column names\n",
    "#     month_column : str\n",
    "#         Name of the month column\n",
    "#     baseline_month : str, optional\n",
    "#         Specific baseline month to use\n",
    "#     top_n_categories : int\n",
    "#         Number of top categories to keep for categorical features\n",
    "    \n",
    "#     Returns:\n",
    "#     --------\n",
    "#     pd.DataFrame\n",
    "#         DataFrame with bin-level PSI details\n",
    "#     \"\"\"\n",
    "    \n",
    "#     def identify_feature_types(df: pd.DataFrame, feature_list: List[str]) -> Dict[str, str]:\n",
    "#         \"\"\"Identify categorical vs numerical features\"\"\"\n",
    "#         feature_types = {}\n",
    "#         for feature in feature_list:\n",
    "#             if feature not in df.columns:\n",
    "#                 continue\n",
    "#             unique_vals = df[feature].nunique()\n",
    "#             dtype = df[feature].dtype\n",
    "            \n",
    "#             if (dtype == 'object' or unique_vals <= 10 or \n",
    "#                 feature in ['cic_ScoreRange', 'cic_ln_loan_level_user_type', \n",
    "#                           'cic_has_ever_been_overdue', 'cic_latest_granted_contract_overdue_flag',\n",
    "#                           'cic_flg_zero_non_granted_ever', 'cic_flg_zero_granted_ever']):\n",
    "#                 feature_types[feature] = 'categorical'\n",
    "#             else:\n",
    "#                 feature_types[feature] = 'numerical'\n",
    "#         return feature_types\n",
    "    \n",
    "#     def create_numerical_bins(baseline_data: pd.Series, n_bins: int = 10) -> pd.IntervalIndex:\n",
    "#         \"\"\"Create bins for numerical features\"\"\"\n",
    "#         clean_data = baseline_data.dropna()\n",
    "#         if len(clean_data) == 0:\n",
    "#             return None\n",
    "#         try:\n",
    "#             bins = pd.qcut(clean_data, n_bins, duplicates='drop', retbins=True)[1]\n",
    "#             bins = sorted(set(bins))\n",
    "#             return pd.IntervalIndex.from_breaks(bins)\n",
    "#         except:\n",
    "#             min_val = clean_data.min()\n",
    "#             max_val = clean_data.max()\n",
    "#             bins = np.linspace(min_val, max_val, n_bins + 1)\n",
    "#             return pd.IntervalIndex.from_breaks(bins)\n",
    "    \n",
    "#     def get_bin_level_distribution(data: pd.Series, feature_type: str, \n",
    "#                                  bins: pd.IntervalIndex = None, \n",
    "#                                  top_categories: List = None) -> pd.DataFrame:\n",
    "#         \"\"\"Get distribution at bin/category level\"\"\"\n",
    "#         if feature_type == 'numerical':\n",
    "#             return get_numerical_bin_distribution(data, bins)\n",
    "#         else:\n",
    "#             return get_categorical_bin_distribution(data, top_categories)\n",
    "    \n",
    "#     def get_numerical_bin_distribution(data: pd.Series, bins: pd.IntervalIndex) -> pd.DataFrame:\n",
    "#         \"\"\"Get numerical distribution by bin\"\"\"\n",
    "#         if bins is None:\n",
    "#             return pd.DataFrame()\n",
    "            \n",
    "#         clean_data = data.dropna()\n",
    "#         if len(clean_data) == 0:\n",
    "#             return pd.DataFrame({'bin': [str(bin_) for bin_ in bins], 'count': 0, 'percentage': 0})\n",
    "        \n",
    "#         binned = pd.cut(clean_data, bins, include_lowest=True)\n",
    "#         distribution = binned.value_counts().sort_index()\n",
    "#         total = len(clean_data)\n",
    "        \n",
    "#         results = []\n",
    "#         for interval, count in distribution.items():\n",
    "#             results.append({\n",
    "#                 'bin': str(interval),\n",
    "#                 'count': count,\n",
    "#                 'percentage': count / total if total > 0 else 0\n",
    "#             })\n",
    "        \n",
    "#         # Add missing bins with zero count\n",
    "#         existing_bins = {str(interval) for interval in distribution.index}\n",
    "#         for bin_interval in bins:\n",
    "#             bin_str = str(bin_interval)\n",
    "#             if bin_str not in existing_bins:\n",
    "#                 results.append({\n",
    "#                     'bin': bin_str,\n",
    "#                     'count': 0,\n",
    "#                     'percentage': 0\n",
    "#                 })\n",
    "        \n",
    "#         return pd.DataFrame(results)\n",
    "    \n",
    "#     def get_categorical_bin_distribution(data: pd.Series, top_categories: List) -> pd.DataFrame:\n",
    "#         \"\"\"Get categorical distribution by category\"\"\"\n",
    "#         if top_categories is None:\n",
    "#             return pd.DataFrame()\n",
    "            \n",
    "#         value_counts = data.value_counts()\n",
    "#         total = len(data.dropna())\n",
    "        \n",
    "#         results = []\n",
    "#         # Top categories\n",
    "#         for category in top_categories:\n",
    "#             count = value_counts.get(category, 0)\n",
    "#             results.append({\n",
    "#                 'bin': str(category),\n",
    "#                 'count': count,\n",
    "#                 'percentage': count / total if total > 0 else 0\n",
    "#             })\n",
    "        \n",
    "#         # Other categories\n",
    "#         other_categories = [cat for cat in value_counts.index if cat not in top_categories]\n",
    "#         other_count = value_counts[other_categories].sum() if other_categories else 0\n",
    "#         results.append({\n",
    "#             'bin': 'Other',\n",
    "#             'count': other_count,\n",
    "#             'percentage': other_count / total if total > 0 else 0\n",
    "#         })\n",
    "        \n",
    "#         return pd.DataFrame(results)\n",
    "    \n",
    "#     # Main execution for bin-level analysis\n",
    "#     print(\"Calculating bin-level PSI details...\")\n",
    "#     feature_types = identify_feature_types(df, feature_list)\n",
    "#     all_bin_results = []\n",
    "    \n",
    "#     # Process overall and segments\n",
    "#     segments_to_process = [('Overall', 'Overall')]  # Overall first\n",
    "#     for segment_col in segment_columns:\n",
    "#         if segment_col in df.columns:\n",
    "#             for segment_val in df[segment_col].dropna().unique():\n",
    "#                 segments_to_process.append((segment_col, segment_val))\n",
    "    \n",
    "#     for segment_info in segments_to_process:\n",
    "#         segment_col, segment_val = segment_info\n",
    "        \n",
    "#         # Filter data for segment\n",
    "#         if segment_col == 'Overall':\n",
    "#             segment_data = df.copy()\n",
    "#         else:\n",
    "#             segment_data = df[df[segment_col] == segment_val].copy()\n",
    "        \n",
    "#         # Get months and baseline\n",
    "#         months = sorted(segment_data[month_column].unique())\n",
    "#         if len(months) < 2:\n",
    "#             continue\n",
    "            \n",
    "#         baseline_month_used = baseline_month if baseline_month else months[0]\n",
    "        \n",
    "#         if baseline_month_used not in months:\n",
    "#             continue\n",
    "        \n",
    "#         print(f\"Processing segment: {segment_col}={segment_val}\")\n",
    "        \n",
    "#         for feature, ftype in feature_types.items():\n",
    "#             print(f\"  Feature: {feature}\")\n",
    "            \n",
    "#             # Get baseline data and setup\n",
    "#             baseline_data = segment_data[segment_data[month_column] == baseline_month_used][feature]\n",
    "            \n",
    "#             if len(baseline_data.dropna()) == 0:\n",
    "#                 continue\n",
    "            \n",
    "#             # Prepare bins/categories\n",
    "#             if ftype == 'numerical':\n",
    "#                 bins = create_numerical_bins(baseline_data)\n",
    "#                 top_categories = None\n",
    "#             else:\n",
    "#                 bins = None\n",
    "#                 top_categories = segment_data[segment_data[month_column] == baseline_month_used][feature].value_counts().head(top_n_categories).index.tolist()\n",
    "            \n",
    "#             # Get baseline distribution\n",
    "#             baseline_dist_df = get_bin_level_distribution(baseline_data, ftype, bins, top_categories)\n",
    "#             baseline_dist_dict = dict(zip(baseline_dist_df['bin'], baseline_dist_df['percentage']))\n",
    "            \n",
    "#             # Process each current month\n",
    "#             for current_month in months:\n",
    "#                 if current_month == baseline_month_used:\n",
    "#                     # For baseline month, PSI components are 0\n",
    "#                     current_data = baseline_data\n",
    "#                     current_dist_df = baseline_dist_df\n",
    "#                     current_dist_dict = baseline_dist_dict\n",
    "#                 else:\n",
    "#                     current_data = segment_data[segment_data[month_column] == current_month][feature]\n",
    "                    \n",
    "#                     if len(current_data.dropna()) == 0:\n",
    "#                         continue\n",
    "                    \n",
    "#                     current_dist_df = get_bin_level_distribution(current_data, ftype, bins, top_categories)\n",
    "#                     current_dist_dict = dict(zip(current_dist_df['bin'], current_dist_df['percentage']))\n",
    "                \n",
    "#                 # Calculate PSI components for each bin\n",
    "#                 all_bins = set(baseline_dist_dict.keys()) | set(current_dist_dict.keys())\n",
    "                \n",
    "#                 for bin_name in all_bins:\n",
    "#                     expected_pct = baseline_dist_dict.get(bin_name, 1e-6)\n",
    "#                     actual_pct = current_dist_dict.get(bin_name, 1e-6)\n",
    "                    \n",
    "#                     # Calculate PSI component\n",
    "#                     if current_month == baseline_month_used:\n",
    "#                         psi_component = 0.0  # Baseline month has zero PSI component\n",
    "#                     else:\n",
    "#                         if expected_pct == 0:\n",
    "#                             expected_pct = 1e-6\n",
    "#                         if actual_pct == 0:\n",
    "#                             actual_pct = 1e-6\n",
    "#                         psi_component = (actual_pct - expected_pct) * np.log(actual_pct / expected_pct)\n",
    "                    \n",
    "#                     # Get counts\n",
    "#                     baseline_count = baseline_dist_df[baseline_dist_df['bin'] == bin_name]['count'].iloc[0] if bin_name in baseline_dist_df['bin'].values else 0\n",
    "#                     current_count = current_dist_df[current_dist_df['bin'] == bin_name]['count'].iloc[0] if bin_name in current_dist_df['bin'].values else 0\n",
    "                    \n",
    "#                     all_bin_results.append({\n",
    "#                         'feature': feature,\n",
    "#                         'feature_type': ftype,\n",
    "#                         'segment_column': segment_col,\n",
    "#                         'segment_value': segment_val,\n",
    "#                         'baseline_month': baseline_month_used,\n",
    "#                         'current_month': current_month,\n",
    "#                         'bin': bin_name,\n",
    "#                         'baseline_percentage': expected_pct,\n",
    "#                         'current_percentage': actual_pct,\n",
    "#                         'psi_component': psi_component,\n",
    "#                         'baseline_count': baseline_count,\n",
    "#                         'current_count': current_count,\n",
    "#                         'percentage_change': actual_pct - expected_pct\n",
    "#                     })\n",
    "    \n",
    "#     if all_bin_results:\n",
    "#         bin_results_df = pd.DataFrame(all_bin_results)\n",
    "        \n",
    "#         # Add interpretation for percentage changes\n",
    "#         def interpret_percentage_change(change):\n",
    "#             abs_change = abs(change)\n",
    "#             if abs_change < 0.01:\n",
    "#                 return 'Very Small'\n",
    "#             elif abs_change < 0.05:\n",
    "#                 return 'Small'\n",
    "#             elif abs_change < 0.1:\n",
    "#                 return 'Moderate'\n",
    "#             else:\n",
    "#                 return 'Large'\n",
    "        \n",
    "#         bin_results_df['change_interpretation'] = bin_results_df['percentage_change'].apply(interpret_percentage_change)\n",
    "        \n",
    "#         # Create combined month column for plotting\n",
    "#         bin_results_df['plot_month'] = bin_results_df['current_month']\n",
    "        \n",
    "#         # Add month sequence for proper ordering in plots\n",
    "#         all_months = sorted(bin_results_df['plot_month'].unique())\n",
    "#         month_sequence = {month: i for i, month in enumerate(all_months)}\n",
    "#         bin_results_df['month_sequence'] = bin_results_df['plot_month'].map(month_sequence)\n",
    "        \n",
    "#         return bin_results_df.sort_values(['feature', 'segment_column', 'segment_value', 'month_sequence', 'bin'])\n",
    "#     else:\n",
    "#         return pd.DataFrame()\n",
    "\n",
    "# # Enhanced plotting functions\n",
    "# import matplotlib.pyplot as plt\n",
    "# import seaborn as sns\n",
    "\n",
    "# def plot_psi_trends(psi_df: pd.DataFrame, features: List[str] = None, segments: List[Tuple[str, str]] = None):\n",
    "#     \"\"\"\n",
    "#     Plot PSI trends over time for specified features and segments.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     psi_df : pd.DataFrame\n",
    "#         PSI results from calculate_population_stability_index\n",
    "#     features : List[str], optional\n",
    "#         List of features to plot. If None, plots all features\n",
    "#     segments : List[Tuple[str, str]], optional\n",
    "#         List of segments to plot in format [(segment_column, segment_value), ...]\n",
    "#         If None, plots overall only\n",
    "#     \"\"\"\n",
    "#     if psi_df.empty:\n",
    "#         print(\"No PSI data to plot\")\n",
    "#         return\n",
    "    \n",
    "#     # Filter features if specified\n",
    "#     if features is None:\n",
    "#         features = psi_df['feature'].unique()\n",
    "    \n",
    "#     # Filter segments if specified\n",
    "#     if segments is None:\n",
    "#         plot_data = psi_df[psi_df['segment_column'] == 'Overall']\n",
    "#     else:\n",
    "#         segment_conditions = []\n",
    "#         for seg_col, seg_val in segments:\n",
    "#             condition = (psi_df['segment_column'] == seg_col) & (psi_df['segment_value'] == seg_val)\n",
    "#             segment_conditions.append(condition)\n",
    "        \n",
    "#         plot_data = psi_df[pd.concat(segment_conditions, axis=1).any(axis=1)]\n",
    "    \n",
    "#     # Create subplots\n",
    "#     n_features = len(features)\n",
    "#     n_cols = min(3, n_features)\n",
    "#     n_rows = (n_features + n_cols - 1) // n_cols\n",
    "    \n",
    "#     fig, axes = plt.subplots(n_rows, n_cols, figsize=(5*n_cols, 4*n_rows))\n",
    "#     if n_features == 1:\n",
    "#         axes = [axes]\n",
    "#     elif n_rows > 1 and n_cols > 1:\n",
    "#         axes = axes.flatten()\n",
    "    \n",
    "#     for idx, feature in enumerate(features):\n",
    "#         if idx >= len(axes):\n",
    "#             break\n",
    "            \n",
    "#         ax = axes[idx]\n",
    "#         feature_data = plot_data[plot_data['feature'] == feature]\n",
    "        \n",
    "#         # Plot each segment\n",
    "#         segments_in_data = feature_data[['segment_column', 'segment_value']].drop_duplicates()\n",
    "        \n",
    "#         for _, seg_row in segments_in_data.iterrows():\n",
    "#             seg_col, seg_val = seg_row['segment_column'], seg_row['segment_value']\n",
    "#             seg_data = feature_data[\n",
    "#                 (feature_data['segment_column'] == seg_col) & \n",
    "#                 (feature_data['segment_value'] == seg_val)\n",
    "#             ].sort_values('month_sequence')\n",
    "            \n",
    "#             label = f\"{seg_col}={seg_val}\" if seg_col != 'Overall' else 'Overall'\n",
    "#             ax.plot(seg_data['month_sequence'], seg_data['psi'], marker='o', label=label, linewidth=2)\n",
    "        \n",
    "#         ax.set_title(f'PSI Trend: {feature}', fontsize=14, fontweight='bold')\n",
    "#         ax.set_xlabel('Month Sequence')\n",
    "#         ax.set_ylabel('PSI Value')\n",
    "#         ax.grid(True, alpha=0.3)\n",
    "        \n",
    "#         # Add PSI interpretation guidelines\n",
    "#         ax.axhline(y=0.1, color='orange', linestyle='--', alpha=0.7, label='Minor Change Threshold')\n",
    "#         ax.axhline(y=0.2, color='red', linestyle='--', alpha=0.7, label='Moderate Change Threshold')\n",
    "        \n",
    "#         if idx == 0:  # Only show legend on first subplot\n",
    "#             ax.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    \n",
    "#     # Hide empty subplots\n",
    "#     for idx in range(len(features), len(axes)):\n",
    "#         axes[idx].set_visible(False)\n",
    "    \n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# def plot_feature_psi_heatmap(psi_df: pd.DataFrame, month: str = None):\n",
    "#     \"\"\"\n",
    "#     Create a heatmap of PSI values across features and segments for a specific month.\n",
    "    \n",
    "#     Parameters:\n",
    "#     -----------\n",
    "#     psi_df : pd.DataFrame\n",
    "#         PSI results from calculate_population_stability_index\n",
    "#     month : str, optional\n",
    "#         Specific month to plot. If None, uses latest month\n",
    "#     \"\"\"\n",
    "#     if psi_df.empty:\n",
    "#         print(\"No PSI data to plot\")\n",
    "#         return\n",
    "    \n",
    "#     # Use latest month if not specified\n",
    "#     if month is None:\n",
    "#         month = psi_df['current_month'].max()\n",
    "    \n",
    "#     # Filter data for the specific month\n",
    "#     heatmap_data = psi_df[psi_df['current_month'] == month].copy()\n",
    "    \n",
    "#     if heatmap_data.empty:\n",
    "#         print(f\"No data for month {month}\")\n",
    "#         return\n",
    "    \n",
    "#     # Create pivot table for heatmap\n",
    "#     pivot_data = heatmap_data.pivot_table(\n",
    "#         index=['segment_column', 'segment_value'],\n",
    "#         columns='feature',\n",
    "#         values='psi',\n",
    "#         aggfunc='first'\n",
    "#     ).fillna(0)\n",
    "    \n",
    "#     # Create the heatmap\n",
    "#     plt.figure(figsize=(max(12, len(pivot_data.columns) * 0.8), max(8, len(pivot_data) * 0.6)))\n",
    "    \n",
    "#     # Create custom colormap\n",
    "#     cmap = sns.diverging_palette(10, 130, as_cmap=True)\n",
    "    \n",
    "#     sns.heatmap(\n",
    "#         pivot_data,\n",
    "#         annot=True,\n",
    "#         fmt='.3f',\n",
    "#         cmap=cmap,\n",
    "#         center=0.1,\n",
    "#         cbar_kws={'label': 'PSI Value'}\n",
    "#     )\n",
    "    \n",
    "#     plt.title(f'PSI Values - {month}\\n(PSI < 0.1: No Change, 0.1-0.2: Minor, 0.2-0.5: Moderate, >0.5: Significant)',\n",
    "#               fontsize=14, fontweight='bold', pad=20)\n",
    "#     plt.xlabel('Features')\n",
    "#     plt.ylabel('Segments')\n",
    "#     plt.xticks(rotation=45, ha='right')\n",
    "#     plt.tight_layout()\n",
    "#     plt.show()\n",
    "\n",
    "\n",
    "# # Enhanced analysis functions\n",
    "# def analyze_psi_results(psi_df: pd.DataFrame):\n",
    "#     \"\"\"\n",
    "#     Analyze and summarize PSI results\n",
    "#     \"\"\"\n",
    "#     if psi_df.empty:\n",
    "#         print(\"No PSI results to analyze\")\n",
    "#         return\n",
    "    \n",
    "#     print(\"PSI Results Summary:\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     # Overall summary\n",
    "#     print(f\"\\nTotal PSI calculations: {len(psi_df)}\")\n",
    "#     print(f\"Features analyzed: {psi_df['feature'].nunique()}\")\n",
    "#     print(f\"Segments analyzed: {psi_df['segment_column'].nunique()}\")\n",
    "#     print(f\"Time periods analyzed: {psi_df['current_month'].nunique()}\")\n",
    "    \n",
    "#     # Features with highest average PSI (excluding baseline)\n",
    "#     non_baseline_psi = psi_df[psi_df['current_month'] != psi_df['baseline_month']]\n",
    "#     if not non_baseline_psi.empty:\n",
    "#         feature_psi_avg = non_baseline_psi.groupby('feature')['psi'].mean().sort_values(ascending=False)\n",
    "#         print(f\"\\nTop 5 features with highest average PSI (excluding baseline):\")\n",
    "#         for feature, avg_psi in feature_psi_avg.head().items():\n",
    "#             print(f\"  {feature}: {avg_psi:.4f}\")\n",
    "    \n",
    "#     # Segments with highest average PSI\n",
    "#     segment_psi_avg = psi_df.groupby('segment_column')['psi'].mean().sort_values(ascending=False)\n",
    "#     print(f\"\\nAverage PSI by segment:\")\n",
    "#     for segment, avg_psi in segment_psi_avg.items():\n",
    "#         print(f\"  {segment}: {avg_psi:.4f}\")\n",
    "    \n",
    "#     # PSI interpretation distribution\n",
    "#     interpretation_counts = psi_df['psi_interpretation'].value_counts()\n",
    "#     print(f\"\\nPSI Interpretation Distribution:\")\n",
    "#     for interpretation, count in interpretation_counts.items():\n",
    "#         percentage = (count / len(psi_df)) * 100\n",
    "#         print(f\"  {interpretation}: {count} ({percentage:.1f}%)\")\n",
    "    \n",
    "#     # Monthly trend\n",
    "#     monthly_psi = psi_df.groupby('current_month')['psi'].mean()\n",
    "#     print(f\"\\nAverage PSI by month:\")\n",
    "#     for month, avg_psi in monthly_psi.items():\n",
    "#         print(f\"  {month}: {avg_psi:.4f}\")\n",
    "\n",
    "\n",
    "# def analyze_bin_level_results(bin_psi_df: pd.DataFrame, top_n: int = 10):\n",
    "#     \"\"\"\n",
    "#     Analyze bin-level PSI results\n",
    "#     \"\"\"\n",
    "#     if bin_psi_df.empty:\n",
    "#         print(\"No bin-level results to analyze\")\n",
    "#         return\n",
    "    \n",
    "#     print(\"Bin-Level PSI Analysis:\")\n",
    "#     print(\"=\" * 80)\n",
    "    \n",
    "#     # Exclude baseline month for change analysis\n",
    "#     non_baseline_bins = bin_psi_df[bin_psi_df['current_month'] != bin_psi_df['baseline_month']]\n",
    "    \n",
    "#     if not non_baseline_bins.empty:\n",
    "#         # Bins with highest PSI components\n",
    "#         top_psi_components = non_baseline_bins.nlargest(top_n, 'psi_component')[[\n",
    "#             'feature', 'segment_value', 'current_month', 'bin', \n",
    "#             'psi_component', 'baseline_percentage', 'current_percentage'\n",
    "#         ]]\n",
    "        \n",
    "#         print(f\"\\nTop {top_n} bins with highest PSI components:\")\n",
    "#         for _, row in top_psi_components.iterrows():\n",
    "#             print(f\"  {row['feature']} | {row['segment_value']} | {row['current_month']}\")\n",
    "#             print(f\"    Bin: {row['bin']}\")\n",
    "#             print(f\"    PSI Component: {row['psi_component']:.4f}\")\n",
    "#             print(f\"    Baseline: {row['baseline_percentage']:.1%} -> Current: {row['current_percentage']:.1%}\")\n",
    "#             print()\n",
    "        \n",
    "#         # Largest percentage changes\n",
    "#         non_baseline_bins['abs_percentage_change'] = non_baseline_bins['percentage_change'].abs()\n",
    "#         top_changes = non_baseline_bins.nlargest(top_n, 'abs_percentage_change')[[\n",
    "#             'feature', 'segment_value', 'current_month', 'bin',\n",
    "#             'baseline_percentage', 'current_percentage', 'percentage_change'\n",
    "#         ]]\n",
    "        \n",
    "#         print(f\"\\nTop {top_n} largest percentage changes:\")\n",
    "#         for _, row in top_changes.iterrows():\n",
    "#             print(f\"  {row['feature']} | {row['segment_value']} | {row['current_month']}\")\n",
    "#             print(f\"    Bin: {row['bin']}\")\n",
    "#             print(f\"    Change: {row['baseline_percentage']:.1%} -> {row['current_percentage']:.1%}\")\n",
    "#             print(f\"    : {row['percentage_change']:+.1%}\")\n",
    "#             print()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b040149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example usage\n",
    "# def example_usage():\n",
    "#     \"\"\"\n",
    "#     Example of how to use both functions\n",
    "#     \"\"\"\n",
    "#     # Load your data\n",
    "#     # df = pd.read_csv('sample.csv')\n",
    "    \n",
    "#     feature_list = ['aCicScore',\n",
    "#         'cic_max_age_all_contracts_snapshot',\n",
    "#         'cic_ratio_overdue_contracts_to_granted_contracts', \n",
    "#         'cic_ScoreRange',\n",
    "#         'cic_ln_loan_level_user_type', \n",
    "#         'cic_has_ever_been_overdue',\n",
    "#         'cic_latest_granted_contract_overdue_flag',\n",
    "#         'cic_ratio_closed_over_new_granted_cnt_24M',\n",
    "#         'cic_ratio_risky_contracts_to_granted_contracts',\n",
    "#         'cic_Short_and_Term_Loans_granted_contracts_cnt_24M',\n",
    "#         'cic_flg_zero_non_granted_ever',\n",
    "#         'cic_Personal_Loans_granted_contracts_amt_24M',\n",
    "#         'cic_CreditAvgCreditLimit', \n",
    "#         'cic_flg_zero_granted_ever',\n",
    "#     ]\n",
    "    \n",
    "#     segment_columns = ['new_loan_type', 'gender', 'osType', 'loanType', 'trenchCategory']\n",
    "    \n",
    "#     # Calculate overall PSI\n",
    "#     print(\"Calculating overall PSI...\")\n",
    "#     psi_results = calculate_population_stability_index(df, feature_list, segment_columns)\n",
    "#     analyze_psi_results(psi_results)\n",
    "#     psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "#     psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "#     psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "    \n",
    "#     # Calculate bin-level PSI\n",
    "#     print(\"\\nCalculating bin-level PSI...\")\n",
    "#     bin_psi_results = calculate_bin_level_psi(df, feature_list, segment_columns)\n",
    "#     analyze_bin_level_results(bin_psi_results)\n",
    "#     bin_psi_results['modelDisplayName'] = df['modelDisplayName'].iloc[0]\n",
    "#     bin_psi_results['modelVersionId'] = df['modelVersionId'].iloc[0]\n",
    "#     bin_psi_results['trenchCategory'] = df['trenchCategory'].iloc[0]\n",
    "\n",
    "#     # psi_results = psi_results[[ 'modelDisplayName',\n",
    "#     # 'modelVersionId', 'trenchCategory', 'feature', 'feature_type', 'segment_column', 'segment_value',\n",
    "#     # 'baseline_month', 'current_month', 'psi', 'num_baseline_records',\n",
    "#     # 'num_current_records', 'psi_interpretation']].copy()\n",
    "    \n",
    "#     # bin_psi_results = bin_psi_results[[ 'modelDisplayName', 'modelVersionId',\n",
    "#     #    'trenchCategory','feature', 'feature_type', 'segment_column', 'segment_value',\n",
    "#     #    'baseline_month', 'current_month', 'bin', 'baseline_percentage',\n",
    "#     #    'current_percentage', 'psi_component', 'baseline_count',\n",
    "#     #    'current_count', 'percentage_change', 'change_interpretation',\n",
    "#     #    ]].copy()\n",
    "    \n",
    "#     # Create plots\n",
    "#     print(\"\\nCreating plots...\")\n",
    "#     plot_psi_trends(psi_results, features=feature_list[:6])  # Plot first 6 features\n",
    "#     plot_feature_psi_heatmap(psi_results)\n",
    "    \n",
    "#     return psi_results, bin_psi_results\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83598405",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Run the analysis\n",
    "# if __name__ == \"__main__\":\n",
    "#     psi_results, bin_psi_results = example_usage()\n",
    "#     pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f429ac2",
   "metadata": {},
   "outputs": [],
   "source": [
    "psi_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbc83f86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
