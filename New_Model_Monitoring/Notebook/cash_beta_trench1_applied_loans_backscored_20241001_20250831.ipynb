{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "87b0ccf7",
   "metadata": {},
   "source": [
    "# Define Library"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9893b0ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [markdown]\n",
    "# # Jupyter Notebook Loading Header\n",
    "#\n",
    "# This is a custom loading header for Jupyter Notebooks in Visual Studio Code.\n",
    "# It includes common imports and settings to get you started quickly.\n",
    "# %% [markdown]\n",
    "## Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from google.cloud import bigquery\n",
    "from google.cloud import storage\n",
    "import os\n",
    "import tempfile\n",
    "import time\n",
    "from datetime import datetime\n",
    "import uuid\n",
    "import joblib\n",
    "import uuid\n",
    "\n",
    "import gcsfs\n",
    "import duckdb as dd\n",
    "import pickle\n",
    "import joblib\n",
    "from typing import Union\n",
    "import io\n",
    "\n",
    "path = r'C:\\Users\\Dwaipayan\\AppData\\Roaming\\gcloud\\legacy_credentials\\dchakroborti@tonikbank.com\\adc.json'\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = path\n",
    "client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "os.environ[\"GOOGLE_CLOUD_PROJECT\"] = \"prj-prod-dataplatform\"\n",
    "# %% [markdown]\n",
    "## Configure Settings\n",
    "# Set options or configurations as needed\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option(\"Display.max_rows\", 100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c02c60e4",
   "metadata": {},
   "source": [
    "# Constant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3c0f9b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "CURRENT_DATE = datetime.now().strftime(\"%Y%m%d\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9b8be9",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c183b33b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The unique Id is: 92bf07a06a93\n"
     ]
    }
   ],
   "source": [
    "unique_id = str(uuid.uuid4()).replace('-', '')[-12:]\n",
    "print(f\"The unique Id is: {unique_id}\")\n",
    "BUCKETNAME = 'prod-asia-southeast1-tonik-aiml-workspace'\n",
    "CLOUDPATH = 'DC/Model_Monitoring/cash_beta_trench1_data'\n",
    "LOCALPATH = r'D:\\OneDrive - Tonik Financial Pte Ltd\\MyStuff\\Data Engineering\\Model_Monitoring\\New_Model_Monitoring\\Source_Data\\cash_beta_trench1_data'\n",
    "VERSION = 'V1'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc447b5",
   "metadata": {},
   "source": [
    "# <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Functions </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4e846f",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Save the data to google clound storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5422d609",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_df_to_gcs(df, bucket_name, destination_blob_name, file_format='csv'):\n",
    "    \"\"\"Saves a pandas DataFrame to Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        df: The pandas DataFrame to save.\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        destination_blob_name: The name of the blob to be created.\n",
    "        file_format: The file format to save the DataFrame in ('csv' or 'parquet').\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a temporary file\n",
    "    if file_format == 'csv':\n",
    "        temp_file = 'temp.csv'\n",
    "        df.to_csv(temp_file, index=False)\n",
    "    elif file_format == 'parquet':\n",
    "        temp_file = 'temp.parquet'\n",
    "        df.to_parquet(temp_file, index=False)\n",
    "    else:\n",
    "        raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "    # Upload the file to GCS\n",
    "    storage_client = storage.Client(project=\"prj-prod-dataplatform\")\n",
    "\n",
    "    storage_client = storage.Client()\n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "\n",
    "    blob.upload_from_filename(temp_file)\n",
    "\n",
    "    # Remove the temporary file\n",
    "    import os\n",
    "    os.remove(temp_file)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ceded77",
   "metadata": {},
   "source": [
    "## <div align=\"left\" style=\"color:rgb(51, 250, 250);\"> Read the Data from Google Cloud Storage </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b52766e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_df_from_gcs(bucket_name, source_blob_name, file_format='csv'):\n",
    "    \"\"\"Reads a DataFrame from Google Cloud Storage.\n",
    "\n",
    "    Args:\n",
    "        bucket_name: The name of the GCS bucket.\n",
    "        source_blob_name: The name of the blob to read.\n",
    "        file_format: The file format to read ('csv' or 'parquet').\n",
    "\n",
    "    Returns:\n",
    "        pandas.DataFrame: The data loaded from the GCS file.\n",
    "    \"\"\"\n",
    "    # Create a temporary file name\n",
    "    temp_file = f'temp.{file_format}'\n",
    "    \n",
    "    try:\n",
    "        # Initialize GCS client\n",
    "        storage_client = storage.Client()\n",
    "        bucket = storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(source_blob_name)\n",
    "\n",
    "        # Download the file to a temporary location\n",
    "        blob.download_to_filename(temp_file)\n",
    "\n",
    "        # Read the file into a DataFrame\n",
    "        if file_format == 'csv':\n",
    "            df = pd.read_csv(temp_file, low_memory=False)\n",
    "        elif file_format == 'parquet':\n",
    "            df = pd.read_parquet(temp_file)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid file format. Please choose 'csv' or 'parquet'.\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    finally:\n",
    "        # Clean up the temporary file\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a2f306",
   "metadata": {},
   "source": [
    "## <div align = \"left\" style=\"color:rgb(51, 250, 250);\"> Data Quality Report </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ba6eb646",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_quality_report(df, target_col='ln_fspd30_flag'):\n",
    "    # Initialize an empty list to store each row of data\n",
    "    report_data = []\n",
    "    # Iterate over each column in the DataFrame to compute metrics\n",
    "    for col in df.columns:\n",
    "        # Determine the data type of the column\n",
    "        data_type = df[col].dtype\n",
    "       \n",
    "        # Calculate the number of missing values in the column\n",
    "        missing_values = df[col].isnull().sum()\n",
    "       \n",
    "        # Calculate the percentage of missing values relative to the total number of rows\n",
    "        missing_percentage = (missing_values / len(df)) * 100\n",
    "       \n",
    "        # Calculate the number of unique values in the column\n",
    "        unique_values = df[col].nunique()\n",
    "       \n",
    "        # Calculate the percentage of non-missing values\n",
    "        non_missing_percentage = ((len(df) - missing_values) / len(df)) * 100\n",
    "       \n",
    "        # Check if the column is numeric to compute additional metrics\n",
    "        if pd.api.types.is_numeric_dtype(df[col]):\n",
    "            # Compute minimum, maximum, mean, median, mode, mode percentage, standard deviation, and quantiles\n",
    "            min_value = df[col].min()\n",
    "            max_value = df[col].max()\n",
    "            mean_value = df[col].mean()\n",
    "            median_value = df[col].median()\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = df[col].std()\n",
    "            quantile_25 = df[col].quantile(0.25)\n",
    "            quantile_50 = df[col].quantile(0.50)  # Same as median\n",
    "            quantile_75 = df[col].quantile(0.75)\n",
    "            \n",
    "            # Calculate the Interquartile Range (IQR)\n",
    "            iqr = quantile_75 - quantile_25\n",
    "            \n",
    "            # Calculate Skewness and Kurtosis\n",
    "            skewness = df[col].skew()\n",
    "            kurtosis = df[col].kurt()\n",
    "            \n",
    "            # Calculate Coefficient of Variation (CV) - standardized measure of dispersion\n",
    "            cv = (std_dev / mean_value) * 100 if mean_value != 0 else None\n",
    "            \n",
    "            # Calculate correlation with target variable if target exists in dataframe\n",
    "            if target_col in df.columns and col != target_col and pd.api.types.is_numeric_dtype(df[target_col]):\n",
    "                # Calculate correlation only using rows where both columns have non-null values\n",
    "                correlation = df[[col, target_col]].dropna().corr().iloc[0, 1]\n",
    "            else:\n",
    "                correlation = None\n",
    "        else:\n",
    "            # Assign None for non-numeric columns where appropriate\n",
    "            min_value = None\n",
    "            max_value = None\n",
    "            mean_value = None\n",
    "            median_value = None\n",
    "            mode_value = df[col].mode().iloc[0] if not df[col].mode().empty else None\n",
    "            mode_percentage = (df[col] == mode_value).sum() / len(df) * 100 if mode_value is not None else None\n",
    "            std_dev = None\n",
    "            quantile_25 = None\n",
    "            quantile_50 = None\n",
    "            quantile_75 = None\n",
    "            iqr = None\n",
    "            skewness = None\n",
    "            kurtosis = None\n",
    "            cv = None\n",
    "            correlation = None\n",
    "       \n",
    "        # Append the computed metrics for the current column to the list\n",
    "        report_data.append({\n",
    "            'Column': col,\n",
    "            'Data Type': data_type,\n",
    "            'Missing Values': missing_values,\n",
    "            'Missing Percentage': missing_percentage,\n",
    "            'Unique Values': unique_values,\n",
    "            'Min': min_value,\n",
    "            'Max': max_value,\n",
    "            'Mean': mean_value,\n",
    "            'Median': median_value,\n",
    "            'Mode': mode_value,\n",
    "            'Mode Percentage': mode_percentage,\n",
    "            'Std Dev': std_dev,\n",
    "            'Non-missing Percentage': non_missing_percentage,\n",
    "            '25% Quantile': quantile_25,\n",
    "            '50% Quantile': quantile_50,\n",
    "            '75% Quantile': quantile_75,\n",
    "            'IQR': iqr,\n",
    "            'Skewness': skewness,\n",
    "            'Kurtosis': kurtosis,\n",
    "            'CV (%)': cv,\n",
    "            f'Correlation with {target_col}': correlation\n",
    "        })\n",
    "    # Create the DataFrame from the list of dictionaries\n",
    "    report = pd.DataFrame(report_data)\n",
    "   \n",
    "    # Return the complete data quality report DataFrame\n",
    "    return report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91c57c77",
   "metadata": {},
   "source": [
    "# <div align = \"left\" style=\"color:rgb(51,250,250);\"> Upload pickle file to Google Cloud Storage Bucke </div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3600b225",
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_to_gcs(bucket_name, source_file_path, destination_blob_name):\n",
    "    \"\"\"Uploads a file to Google Cloud Storage\"\"\"\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    blob.upload_from_filename(source_file_path)\n",
    "    print(f\"File {source_file_path} uploaded to {bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5acf1729",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import io\n",
    "from google.cloud import storage\n",
    "def save_pickle_to_gcs(data, bucket_name, destination_blob_name):\n",
    "    \"\"\"\n",
    "    Save any Python object as a pickle file to Google Cloud Storage\n",
    "    \n",
    "    Args:\n",
    "        data: The Python object to pickle (DataFrame, dict, list, etc.)\n",
    "        bucket_name: Name of the GCS bucket\n",
    "        destination_blob_name: Path/filename in the bucket\n",
    "    \"\"\"\n",
    "    # Initialize the GCS client\n",
    "    client = storage.Client()\n",
    "    bucket = client.bucket(bucket_name)\n",
    "    blob = bucket.blob(destination_blob_name)\n",
    "    \n",
    "    # Serialize the data to pickle format in memory\n",
    "    pickle_buffer = io.BytesIO()\n",
    "    pickle.dump(data, pickle_buffer)\n",
    "    pickle_buffer.seek(0)\n",
    "    \n",
    "    # Upload the pickle data to GCS\n",
    "    blob.upload_from_file(pickle_buffer, content_type='application/octet-stream')\n",
    "    print(f\"Pickle file uploaded to gs://{bucket_name}/{destination_blob_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64519c1d",
   "metadata": {},
   "source": [
    "# save_dataframe_multi_format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a7e6da83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_dataframe_multi_format(\n",
    "    dataframe: pd.DataFrame, \n",
    "    cloud_path: str, \n",
    "    filename: str, \n",
    "    client: bigquery.Client = None,\n",
    "    bucket_name: str = None\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Save a pandas DataFrame to Google Cloud Storage in multiple formats (CSV, Pickle, Parquet, Joblib).\n",
    "    \n",
    "    Args:\n",
    "        dataframe (pd.DataFrame): The DataFrame to save\n",
    "        cloud_path (str): The cloud path (e.g., 'DC/Model_Monitoring/cash_beta_trench1_data')\n",
    "        filename (str): The base filename without extension\n",
    "        client (bigquery.Client, optional): BigQuery client (for project reference)\n",
    "        bucket_name (str, optional): GCS bucket name. If None, will try to extract from client\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary with status of each file saved\n",
    "        \n",
    "    Example:\n",
    "        client = bigquery.Client(project='prj-prod-dataplatform')\n",
    "        CLOUDPATH = 'DC/Model_Monitoring/cash_beta_trench1_data'\n",
    "        \n",
    "        results = save_dataframe_multi_format(\n",
    "            dataframe=d1,\n",
    "            cloud_path=CLOUDPATH,\n",
    "            filename='my_data',\n",
    "            client=client,\n",
    "            bucket_name='your-bucket-name'  # Replace with your actual bucket name\n",
    "        )\n",
    "    \"\"\"\n",
    "    \n",
    "    # Initialize Google Cloud Storage client\n",
    "    storage_client = storage.Client(project=client.project if client else None)\n",
    "    \n",
    "    # You'll need to specify your bucket name here\n",
    "    # Common bucket names in GCP data platforms might be like:\n",
    "    # - 'prj-prod-dataplatform-storage'\n",
    "    # - 'dataplatform-storage'\n",
    "    # - or similar pattern\n",
    "    if bucket_name is None:\n",
    "        # You need to replace this with your actual bucket name\n",
    "        raise ValueError(\"Please provide the bucket_name parameter\")\n",
    "    \n",
    "    bucket = storage_client.bucket(bucket_name)\n",
    "    \n",
    "    # Results dictionary to track saves\n",
    "    results = {}\n",
    "    \n",
    "    # Ensure cloud_path doesn't start with '/'\n",
    "    cloud_path = cloud_path.lstrip('/')\n",
    "    \n",
    "    try:\n",
    "        # 1. Save as CSV\n",
    "        csv_buffer = io.StringIO()\n",
    "        dataframe.to_csv(csv_buffer, index=False)\n",
    "        csv_blob = bucket.blob(f\"{cloud_path}/{filename}.csv\")\n",
    "        csv_blob.upload_from_string(csv_buffer.getvalue(), content_type='text/csv')\n",
    "        results['csv'] = f\"gs://{bucket_name}/{cloud_path}/{filename}.csv\"\n",
    "        \n",
    "        # 2. Save as Pickle\n",
    "        pickle_buffer = io.BytesIO()\n",
    "        pickle.dump(dataframe, pickle_buffer)\n",
    "        pickle_blob = bucket.blob(f\"{cloud_path}/{filename}.pkl\")\n",
    "        pickle_blob.upload_from_string(pickle_buffer.getvalue(), content_type='application/octet-stream')\n",
    "        results['pickle'] = f\"gs://{bucket_name}/{cloud_path}/{filename}.pkl\"\n",
    "        \n",
    "        # 3. Save as Parquet\n",
    "        parquet_buffer = io.BytesIO()\n",
    "        dataframe.to_parquet(parquet_buffer, index=False)\n",
    "        parquet_blob = bucket.blob(f\"{cloud_path}/{filename}.parquet\")\n",
    "        parquet_blob.upload_from_string(parquet_buffer.getvalue(), content_type='application/octet-stream')\n",
    "        results['parquet'] = f\"gs://{bucket_name}/{cloud_path}/{filename}.parquet\"\n",
    "        \n",
    "        # 4. Save as Joblib\n",
    "        joblib_buffer = io.BytesIO()\n",
    "        joblib.dump(dataframe, joblib_buffer)\n",
    "        joblib_blob = bucket.blob(f\"{cloud_path}/{filename}.joblib\")\n",
    "        joblib_blob.upload_from_string(joblib_buffer.getvalue(), content_type='application/octet-stream')\n",
    "        results['joblib'] = f\"gs://{bucket_name}/{cloud_path}/{filename}.joblib\"\n",
    "        \n",
    "        print(\"All files saved successfully!\")\n",
    "        for format_type, path in results.items():\n",
    "            print(f\"{format_type.upper()}: {path}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred: {str(e)}\")\n",
    "        results['error'] = str(e)\n",
    "    \n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b2f88c",
   "metadata": {},
   "source": [
    "# Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "98b0246c",
   "metadata": {},
   "outputs": [],
   "source": [
    "schema1 = 'worktable_data_analysis'\n",
    "cash_beta_trench1 = f'cash_beta_trench1_applied_loans_backscored_20241001_20250831'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1cf3a5",
   "metadata": {},
   "source": [
    "# Query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99498bf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Job ID 1cada483-3682-4f12-9960-72b35d6c5460 successfully executed: 100%|\u001b[32m██████████\u001b[0m|"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Dwaipayan\\AppData\\Roaming\\Python\\Python312\\site-packages\\google\\cloud\\bigquery\\table.py:1900: UserWarning: BigQuery Storage module not found, fetch data with the REST endpoint instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Downloading: 100%|\u001b[32m██████████\u001b[0m|\n",
      "The shape of worktable_data_analysis.cash_beta_trench1_applied_loans_backscored_20241001_20250831 table is:\t (296480, 34)\n"
     ]
    }
   ],
   "source": [
    "sq = f\"\"\"\n",
    "select * from worktable_data_analysis.cash_beta_trench1_applied_loans_backscored_20241001_20250831;\n",
    "\"\"\"\n",
    "d1 = client.query(sq).to_dataframe(progress_bar_type='tqdm')\n",
    "print(f\"The shape of {schema1}.{cash_beta_trench1} table is:\\t {d1.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ad4b52",
   "metadata": {},
   "source": [
    "Found no duplicate digitalLoanAccountId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c70e1450",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files saved successfully!\n",
      "CSV: gs://prod-asia-southeast1-tonik-aiml-workspace/DC/Model_Monitoring/cash_beta_trench1_data/20250911_9e44a9a66db8_cash_beta_trench1_applied_loans_backscored_20241001_20250831.csv\n",
      "PICKLE: gs://prod-asia-southeast1-tonik-aiml-workspace/DC/Model_Monitoring/cash_beta_trench1_data/20250911_9e44a9a66db8_cash_beta_trench1_applied_loans_backscored_20241001_20250831.pkl\n",
      "PARQUET: gs://prod-asia-southeast1-tonik-aiml-workspace/DC/Model_Monitoring/cash_beta_trench1_data/20250911_9e44a9a66db8_cash_beta_trench1_applied_loans_backscored_20241001_20250831.parquet\n",
      "JOBLIB: gs://prod-asia-southeast1-tonik-aiml-workspace/DC/Model_Monitoring/cash_beta_trench1_data/20250911_9e44a9a66db8_cash_beta_trench1_applied_loans_backscored_20241001_20250831.joblib\n"
     ]
    }
   ],
   "source": [
    "filenames = f'{CURRENT_DATE}_{unique_id}_cash_beta_trench1_applied_loans_backscored_20241001_20250831'\n",
    "\n",
    "results = save_dataframe_multi_format(\n",
    "     dataframe=d1,\n",
    "     cloud_path=CLOUDPATH,\n",
    "     filename=filenames,\n",
    "     client=client,\n",
    "     bucket_name=f'{BUCKETNAME}'\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d44918f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:4: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<string>:4: SyntaxWarning: invalid escape sequence '\\{'\n",
      "<>:4: SyntaxWarning: invalid escape sequence '\\{'\n",
      "C:\\Users\\Dwaipayan\\AppData\\Local\\Temp\\ipykernel_4056\\3770834594.py:4: SyntaxWarning: invalid escape sequence '\\{'\n",
      "  joblib.dump(d1, f\"{LOCALPATH}\\{CURRENT_DATE}_{unique_id}_cash_beta_trench1_applied_loans_backscored_20241001_20250831.joblib\")\n",
      "<string>:4: SyntaxWarning: invalid escape sequence '\\{'\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['D:\\\\OneDrive - Tonik Financial Pte Ltd\\\\MyStuff\\\\Data Engineering\\\\Model_Monitoring\\\\New_Model_Monitoring\\\\Source_Data\\\\cash_beta_trench1_data\\\\20250911_92bf07a06a93_cash_beta_trench1_applied_loans_backscored_20241001_20250831.joblib']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d1.to_csv(fr\"{LOCALPATH}\\{CURRENT_DATE}_{unique_id}_cash_beta_trench1_applied_loans_backscored_20241001_20250831.csv\", index = False)\n",
    "d1.to_parquet(fr\"{LOCALPATH}\\{CURRENT_DATE}_{unique_id}_cash_beta_trench1_applied_loans_backscored_20241001_20250831.parquet\")\n",
    "d1.to_pickle(fr\"{LOCALPATH}\\{CURRENT_DATE}_{unique_id}_cash_beta_trench1_applied_loans_backscored_20241001_20250831.pkl\")\n",
    "joblib.dump(d1, f\"{LOCALPATH}\\{CURRENT_DATE}_{unique_id}_cash_beta_trench1_applied_loans_backscored_20241001_20250831.joblib\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8e8cb05",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
